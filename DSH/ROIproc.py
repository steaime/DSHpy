import sys
import os
import logging
import bisect
import collections
import math
import numpy as np
from scipy import ndimage as nd
import importlib.util

from DSH import Config as cf
from DSH import MIfile as MI
from DSH import MIstack as MIs
from DSH import SharedFunctions as sf
from DSH import IOfunctions as iof

def MaskCoordsFromBoundaries(c0_list, c1_list=None, flatten_res=True):
    """Generate a list of polar masks coordinates, each mask of the form [c0, c1, dc0, dc1]
    
    Parameters
    ----------
    c0_list    : list of first coordinate points (for adjacent masks) or list of couples [c0_min, c0_max]
    c1_list    : list of second coordinate points or list of couples [c1_min, c1_max].
    flatten_res: if True, flatten the (c0, c1) dimensions into a single list.
                otherwise, return a 3D array with separate c0 and c1 axes
    
    Returns
    -------
    mSpecs: 2D or 3D array, depending on flatten_res. Last dimension is [c0, c1, dc0, dc1]
    """
    c_list = [c0_list, c1_list]
    for j in range(2):
        if (not sf.IsIterable(c_list[j][0])):
            tmp_list = []
            for i in range(len(c_list[j])-1):
                tmp_list.append([c_list[j][i], c_list[j][i+1]])
            c_list[j] = tmp_list
    mSpecs = np.empty((len(c_list[0]), len(c_list[1]), 4), dtype=float)
    for i in range(mSpecs.shape[0]):
        for j in range(mSpecs.shape[1]):
            mSpecs[i, j] = [0.5*(c_list[0][i][0] + c_list[0][i][1]),\
                            0.5*(c_list[1][j][0] + c_list[1][j][1]),\
                            c_list[0][i][1] - c_list[0][i][0],\
                            c_list[1][j][1] - c_list[1][j][0]]
    logging.debug('MaskCoords created with\n\t- {0} first coords from {1:.2f} (+- {2:.2f}) to {3:.2f} (+- {4:.2f}) '.format(len(c_list[0]), mSpecs[0,0,0], 0.5*mSpecs[0,0,2], mSpecs[-1,0,0], 0.5*mSpecs[-1,0,2]) +
                  'and\n\t- {0} second coords from {1} (+- {2:.2f}) to {3:.2f} (+- {4:.2f})'.format(len(c_list[1]), mSpecs[0,0,1], 0.5*mSpecs[0,0,3], mSpecs[0,-1,1], 0.5*mSpecs[0,-1,3]))
    if flatten_res:
        return mSpecs.reshape(-1, mSpecs.shape[-1])
    else:
        return mSpecs

def GenerateMasks(coords, grid, common_mask=None):
    """Generate a list of regions of interest, labelled in the form of binary images, 
    each one with 0s everywhere and 1 inside the region of interest
    
    Parameters
    ----------
    coords       : list of mask coordinates in the form 
                    - [r, a, dr, da], if coordsystem=='polar'
                    - [x, y, dx, dy], if coordsystem=='cartesian'
    grid         : grid of coordinates, as generated by GenerateGrid2D
    common_mask  : eventually specify common mask to be multiplied to every mask
    
    Returns
    -------
    if binary_res: a 3D array, one page per binary images (mask)
    """
    if common_mask is None:
        common_mask = np.ones_like(grid[0], dtype=int)
        
    res = np.empty((len(coords), grid[0].shape[0], grid[0].shape[1]), dtype=np.dtype('b'))
    for m_idx in range(len(coords)):
        x0min, x0max = coords[m_idx][0]-0.5*coords[m_idx][2], coords[m_idx][0]+0.5*coords[m_idx][2]
        x1min, x1max = coords[m_idx][1]-0.5*coords[m_idx][3], coords[m_idx][1]+0.5*coords[m_idx][3]      
        res[m_idx] = np.where(np.logical_and(common_mask>0,
                                             np.logical_and(np.logical_and(grid[0]>=x0min, grid[0]<x0max),
                                                            np.logical_and(grid[1]>=x1min, grid[1]<x1max))),
                              1, 0)
    
    logging.debug('{0} binary masks created with shape {1}'.format(len(coords), grid[0].shape))
    return res

def GenerateROIgrid(px_coords, GridShape, coord_limits=None, common_mask=None):
    """
    Generate a list of ROIs tiling the entire space
    
    Parameters
    ----------
    px_coords :   pixel coordinates, as generated by SharedFunctions.PixelCoordGrid
    GridShape :   (M, N) int couple: number of ROIs across each coordinate axis
    coord_limits: [[min_x0, max_x0], [min_x1, max_x1]]
                  eventually restrict the tiling to a subset of the entire space
                  defined by its boundaries along each axis. If None, full axis span will be taken
    
    Returns:
    --------
    roi_masks :   3D binary array with ROI masks
    roi_coords:   list of coordinates in the form [c0, c1, dc0, dc1] as accepted by GenerateMasks
    """
    
    if common_mask is None:
        common_mask = np.ones_like(px_coords[0], dtype=int)
    if coord_limits is None:
        coord_limits = [None, None]
    ax_marks = [None, None]
    for i in range(2):
        if coord_limits[i] is None:
            coord_limits[i] = [np.min(px_coords[i]), np.max(px_coords[i])]
        ax_marks[i] = np.linspace(coord_limits[i][0], coord_limits[i][1], num=GridShape[i]+1, endpoint=True)
    
    roi_masks = np.zeros((np.prod(GridShape), px_coords[0].shape[0], px_coords[0].shape[1]), dtype=np.dtype('b'))
    roi_coords = []
    for i in range(GridShape[0]):
        for j in range(GridShape[1]):
            roi_masks[i*GridShape[1]+j] = np.where(np.logical_and(common_mask>0,
                                                       np.logical_and(np.logical_and(px_coords[0]>=ax_marks[0][i], px_coords[0]<ax_marks[0][i+1]),
                                                                      np.logical_and(px_coords[1]>=ax_marks[1][j], px_coords[1]<ax_marks[1][j+1]))),
                                        1, 0)
            roi_coords.append([0.5*(ax_marks[0][i]+ax_marks[0][i+1]), 0.5*(ax_marks[1][j]+ax_marks[1][j+1]), ax_marks[0][i+1]-ax_marks[0][i], ax_marks[1][j+1]-ax_marks[1][j]])
    
    logging.debug('{0} binary masks created with shape {1}'.format(len(roi_coords), px_coords[0].shape))
    return roi_masks, np.asarray(roi_coords)
    
def BinaryToIntegerMask(BinaryMasks):
    """
    Converts a list of binary ROI masks (1 inside the ROI, 0 elsewhere) to an integer mask with ROI index
    
    Parameters
    ----------
    BinaryMasks : list of 2D binary masks, one for every ROI
                  NOTE: no overlap between ROIs is assumed. 
                  In case of overlap, the last ROI will overwrite the first ones
                      
    Returns
    -------
    intMask :     2D integer mask, reporting the ROI index every pixel belongs to
                  pixels belonging to no ROI will be set -1
    """
    res = -1 * np.ones_like(BinaryMasks[0])
    for i in range(len(BinaryMasks)):
        res = np.where(BinaryMasks[i], i, res)
    return res

def FindBoundingBoxROI(ROImasks, margin=0):
    """
    Computes the smallest box enclosing all ROIs
    
    Parameters
    ----------
    ROImasks : 2D or 3D binary mask. if 3D, masks will be overlapped to find the global binding box
    margin : eventually add a margin, in pixel, to all sides of the bounding box

    Returns
    -------
    BBox : [min_row, min_col, max_row+1, max_col+1]
    """
    if ROImasks.ndim>2:
        use_mask = np.sum(ROImasks, axis=0)
    else:
        use_mask = ROImasks
    nonzero_cols = np.nonzero(np.sum(use_mask, axis=0))
    nonzero_rows = np.nonzero(np.sum(use_mask, axis=1))
    return np.asarray([max(0, np.min(nonzero_rows)-margin), 
                       max(0, np.min(nonzero_cols)-margin), 
                       min(use_mask.shape[0], np.max(nonzero_rows)+margin+1), 
                       min(use_mask.shape[1], np.max(nonzero_cols)+margin+1)])

def ROIAverage(image, ROImask, boolMask=False, weights=None, norm=None, masknans=False, BoundingBoxes=None, dtype=float, evalFunc=None, evalParams={}, debug=False):
    """
    Calculate the average value of an image on a series of ROIs.

    Parameters
    ----------
    image        - 2D image or 3D ndarray with list of images
    ROImask      - if boolMask : list of 2D bool arrays.
                          if BoundingBoxes is specified, ROImask[i] should have a size determined by BoundingBoxes[i] 
                          otherwise, each ROImask should have the same size as the input image(s)
                   else : 2D int array with same size as the input image(s), with each pixel labeled with the index of the
                          ROI it belongs to (0 based). Each pixel can only belong to one ROI.
                          Pixels not belonging to any ROI must be labeled with -1
                   It can be none if BoundingBoxes is specified. In this case, 
                   bounding boxes themselves will be used as ROIs
    weights      - can do a weighted average instead of a simple average if this keyword parameter
                   is set.  weights.shape must = image.shape.
    norm         - Normalization factors. Equals the number of pixel belonging to each ROI if average is not weighted
                   If None (default), it is computed from ROImask
    masknans     - assume the presence of NaNs in the array: mask them and don't count them in
                   the normalization by setting their weight to zero. Set it to False if you are
                   sure that there are no NaNs to improve calculation speed
    BoundingBoxes- None, or list of bounding boxes, as many as the number of ROIs
                   Each bounding box is a 4-element array of type [min_row, min_col, max_row+1, max_col+1]
    dtype        - Datatype of the accumulator in which the elements are summed. 
                   If the accumulator is too small, np.sum generates overflow
    evalFunc     - if None, simple average will be computed.
                   Otherwise, what will be averaged will be a function of the pixel values 
                   ex: to compute the variance, use SquareDistFromMean()
    evalParams   - eventually specify additional parameters for evalFunc

    Returns
    -------
    ROI_avg   : if image is 2D: 1D float array with ROI-averaged data
                if image is 3D: 2D float array, one image per row, one ROI per column.
                If a bin contains NO DATA, it will have a NAN value because of the
                divide-by-sum-of-weights component.
    norm      : sum of weights within the ROI. If weights==None, this reduces to the number of pixels in the ROI
    """
    
    
    if image.shape[0]==0:
        return None, None
    
    rbb = BoundingBoxes
    
    if ROImask is None:
        if BoundingBoxes is None:
            raise ValueError('either ROImask or BoundingBoxes must be specified!')
        else:
            nbins = len(BoundingBoxes)
            ROIboolMask = None
    else:
        if boolMask:
            nbins = len(ROImask)
            ROIboolMask = ROImask
        else:
            nbins = np.max(ROImask)+1
            if BoundingBoxes is None:
                ROIboolMask = [ROImask==b for b in range(nbins)]
            else:
                ROIboolMask = [ROImask[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]]==b for b in range(nbins)]
        
    use_weights = (weights is not None or masknans)
    if weights is None and use_weights:
        if (image.ndim > 2):
            weights = np.ones_like(image[0])
        else:
            weights = np.ones_like(image)
    if masknans:
        weights = np.multiply(weights, ~np.isnan(image))
    if use_weights:
        use_img = np.multiply(image, weights)
        if norm is None:
            # normalization factor for each bin
            if (weights.ndim > 2):
                if BoundingBoxes is None:
                    norm = np.array([[np.sum(np.multiply(weights[i], ROIboolMask[b])) for b in range(nbins)] for i in range(weights.shape[0])])
                else:
                    if ROIboolMask is None:
                        norm = np.array([[np.sum(weights[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]]) for b in range(nbins)] for i in range(weights.shape[0])])
                    else:
                        norm = np.array([[np.sum(np.multiply(weights[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b])) for b in range(nbins)] for i in range(weights.shape[0])])
            else:
                if BoundingBoxes is None:
                    norm = np.array([np.sum(np.multiply(weights, ROIboolMask[b])) for b in range(nbins)])
                else:
                    if ROIboolMask is None:
                        norm = np.array([np.sum(weights[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]]) for b in range(nbins)])
                    else:
                        norm = np.array([np.sum(np.multiply(weights[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b])) for b in range(nbins)])
                if (use_img.ndim > 2):
                    norm = [norm] * use_img.shape[0]
    else:
        use_img = image
        if norm is None:
            if BoundingBoxes is None:
                norm = np.array([np.sum(ROIboolMask[b]) for b in range(nbins)])
            else:
                if ROIboolMask is None:
                    norm = np.array([(rbb[b][2]-rbb[b][0])*(rbb[b][3]-rbb[b][1]) for b in range(nbins)])
                else:
                    norm = np.array([np.sum(ROIboolMask[b]) for b in range(nbins)])
        if (norm.ndim==1 and use_img.ndim > 2):
            norm = np.asarray([norm] * use_img.shape[0])
        
    if debug:
        logging.debug('  ROIAverage function called with {0}D input of shape {1}'.format(use_img.ndim, use_img.shape))
        logging.debug('  Normalization has shape {0}, factors range from {1} to {2}'.format(norm.shape, np.min(norm), np.max(norm)))
        if use_weights:
            if weights is None:
                logging.warn('  WARNING: use_weights is True but weights is None!')
            else:
                logging.debug('  Weight image. Weights has shape {0} and range from {1} to {2}'.format(weights.shape, np.min(weights), np.max(weights)))
            strprint = '  Weighted image'
        else:
            strprint = '  No weighting. Original image'
        logging.debug(strprint + ' has shape {0} and ranges from {1} to {2}'.format(use_img.shape, np.min(use_img), np.max(use_img)))
        if BoundingBoxes is not None:
            logging.debug('  Using bounding boxes (len: {0})'.format(len(rbb)))
            
    
    if (use_img.ndim > 2):
        if evalFunc==None:
            if BoundingBoxes is None:
                ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(use_img[i], ROIboolMask[b]), dtype=dtype), norm[i][b]) 
                                     for b in range(nbins)] for i in range(use_img.shape[0])])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([[np.true_divide(np.sum(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])
                else:
                    ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b]), 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])
        else:
            if BoundingBoxes is None:
                ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(evalFunc(use_img[i], **evalParams), ROIboolMask[b]), dtype=dtype), norm[i][b]) 
                                     for b in range(nbins)] for i in range(use_img.shape[0])])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([[np.true_divide(np.sum(evalFunc(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])                    
                else:
                    ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(evalFunc(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), ROIboolMask[b]), 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])
                
    else:
        if evalFunc==None:
            if BoundingBoxes is None:
                ROI_avg = np.array([np.true_divide(np.sum(np.multiply(use_img, ROIboolMask[b]), dtype=dtype), norm[b]) for b in range(nbins)])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([np.true_divide(np.sum(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])                    
                else:
                    ROI_avg = np.array([np.true_divide(np.sum(np.multiply(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b]), 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])
        else:
            if BoundingBoxes is None:
                ROI_avg = np.array([np.true_divide(np.sum(np.multiply(evalFunc(use_img, **evalParams), ROIboolMask[b]), dtype=dtype),
                                                   norm[b]) for b in range(nbins)])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([np.true_divide(np.sum(evalFunc(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])                    
                else:
                    ROI_avg = np.array([np.true_divide(np.sum(np.multiply(evalFunc(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), ROIboolMask[b]), 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])
                
    
    return ROI_avg, norm

def ROIEval(image, ROImask, evalFuncs, evalParams={}):
    """
    Evaluate a function or a list of functions on image values restricted to ROIs.

    Parameters
    ----------
    image        - The 2D image
    ROImask      - 2D int array with same size as image, with each pixel labeled with the index of the
                   ROI it belongs to (0 based). Each pixel can only belong to one ROI.
                   Pixels not belonging to any ROI must be labeled with -1
    evalFuncs    - Function or list of functions to be evaluated. First argument should be array-like (it will be pixel values)
                   return value can be anything (does not need to be numeric)
    evalParams   - Dict or list of dicts, eventually specify additional parameters for evalFunc

    Returns
    -------
    ROIres   :  if single function is given: list of results, one item per ROI
                if multiple functions : list of lists of results
                                        ROIres[i][j] will be evaluation of i-th function on j-th ROI
    """
    if sf.IsIterable(evalFuncs):
        flatten_res = False
    else:
        evalFuncs = [evalFuncs]
        flatten_res = True
    evalParams = sf.CheckIterableVariable(evalParams, len(evalFuncs), force_length=True)
    ROIres = [[f(image[ROImask==b], **evalParams[i]) for b in range(np.max(ROImask))] for i, f in enumerate(evalFuncs)]
    if flatten_res:
        return ROIres[0]
    else:
        return ROIres
    
def LoadImageTimes(img_times_source, usecols=0, skiprows=1, root_folder=None, default_value=None, return_unique=False):
    '''
    Load image times from file or list of files
    '''
    return iof.LoadImageTimes(img_times_source, usecols=usecols, skiprows=skiprows, root_folder=root_folder, return_unique=return_unique)
    
def AverageG2M1(cI_file, avg_interval=None, save_fname=None, save_prefix='g2m1', cut_prefix_len=2, delimiter='\t', comment='#', save_stderr=False, sharp_bound=False, lag_tolerance=1e-2, lag_tolerance_isrelative=True):
    cur_cI, cur_times, cur_lagidx_list = iof.ReadCIfile(cI_file)
    int_bounds = sf.ValidateAverageInterval(avg_interval, num_datapoints=cur_cI.shape[0])
    
    g2m1_avg = AverageCorrTimetrace(cur_cI, cur_times, cur_lagidx_list, avg_interval, return_stderr=True, sharp_bound=sharp_bound, lag_tolerance=lag_tolerance, lag_tolerance_isrelative=lag_tolerance_isrelative)
    g2m1, g2m1_lags = g2m1_avg[0], g2m1_avg[1]
    
    if sharp_bound:
        int_suffix = '*'
    else:
        int_suffix = ''
    
    if save_stderr:
        g2m1_err = g2m1_avg[2]
        str_hdr_g = str(delimiter).join(['dt'+delimiter+'t{0:.2f}[{1}:{2}]'.format(cur_times[int((int_bounds[tavgidx][0]+int_bounds[tavgidx][1])/2)], int_bounds[tavgidx][0], int_bounds[tavgidx][1])+int_suffix+'_avg'+
                                              delimiter+'t{0:.2f}[{1}:{2}]'.format(cur_times[int((int_bounds[tavgidx][0]+int_bounds[tavgidx][1])/2)], int_bounds[tavgidx][0], int_bounds[tavgidx][1])+int_suffix+'_err'
                                         for tavgidx in range(g2m1.shape[0])])
        g2m1_out = np.empty((g2m1.shape[1], 3*g2m1.shape[0]), dtype=float)
        g2m1_out[:,0::3] = g2m1_lags.T
        g2m1_out[:,1::3] = g2m1.T
        g2m1_out[:,2::3] = g2m1_err.T
    else:
        str_hdr_g = str(delimiter).join(['dt'+delimiter+'t{0:.2f}[{1}:{2}]'.format(cur_times[int((int_bounds[tavgidx][0]+int_bounds[tavgidx][1])/2)], int_bounds[tavgidx][0], int_bounds[tavgidx][1])+int_suffix for tavgidx in range(g2m1.shape[0])])
        g2m1_out = np.empty((g2m1.shape[1], 2*g2m1.shape[0]), dtype=float)
        g2m1_out[:,0::2] = g2m1_lags.T
        g2m1_out[:,1::2] = g2m1.T
    
    if save_fname is None:
        save_fname = save_prefix + sf.GetFilenameFromCompletePath(cI_file)[cut_prefix_len:]
    
    np.savetxt(os.path.join(os.path.dirname(cI_file), save_fname), 
           g2m1_out, header=str_hdr_g, delimiter=delimiter, comments=comment)

def AverageCorrTimetrace(CorrData, ImageTimes, Lagtimes_idxlist, avg_interval=None, return_stderr=False, sharp_bound=False, lag_tolerance=1e-2, lag_tolerance_isrelative=True, verbose=0):
    '''
    Average correlation timetraces
    
    Parameters
    ----------
    - CorrData: 2D array. Element [i,j] is correlation between t[i] and t[i]+tau[j]
    - ImageTimes: 1D array, float. i-th element is the physical time at which i-th image was taken
    - Lagtimes_idxlist: 1D array, int. i-th element is the lagtime, in image units
                 NOTE: they must be positive. Not yet compatible with negative time lags
    - avg_interval: None, int, couple interval=[min_idx, max_idx] or list of couples [interval1, ..., intervalN]. 
                 if None or int<=0, result will be averaged on the entire stack
                 if int>0, resolve the average on consecutive chunks of avg_interval images each
                 if interval=[min_idx, max_idx], average will be done in the specified interval
                 if [interval1, ..., intervalN], a list of intervals will be computed as above
    - return_stderr: bool. If True, return standard deviation of the correlation points averaged to obtain the g2-1
    - sharp_bound: bool. 
                 if False, use interval bounds to associate any reference time to the corresponding interval, 
                           but average on all time lags, including those for which ref_time + time_lag >= max_idx
                 if True, for each reference time, restrict average such that both correlated timepoints are in the specified interval
    - lag_tolerance : average together time lags closer than lag_tolerance
    - lag_tolerance_isrelative : if False, time lags lag[i] and lag[j] will be averaged together if |lag[i] - lag[j]| < lag_tolerance
                                 if True, the criterion will be that |lag[i] - lag[j]| < lag_tolerance * |lag[i] + lag[j]| / 2
    
    Returns
    -------
    - g2m1: 2D array. Element [i,j] represents j-th lag time and i-th time-resolved chunk
    - g2m1_lags: 2D array. It contains the time delays, in physical units, of the respective correlation data
    - g2m1_stderr: 2D array. Element [i,j] represents the standard deviation of the data averaged to obtain g2m1[i,j]
                    Only returned if return_stderr==True
    '''
    int_bounds = sf.ValidateAverageInterval(avg_interval, num_datapoints=CorrData.shape[0])
            
    g2m1_alllags, g2m1_laglist = sf.FindLags(series=ImageTimes, lags_index=Lagtimes_idxlist, subset_intervals=int_bounds, tolerance=lag_tolerance, tolerance_isrelative=lag_tolerance_isrelative, verbose=verbose)
    g2m1 = np.zeros((len(int_bounds), np.max([len(l) for l in g2m1_laglist])), dtype=float)
    g2m1_lags = np.nan * np.ones_like(g2m1, dtype=float)
    g2m1_avgnum = np.zeros_like(g2m1, dtype=int)
    if verbose:
        logging.debug('AverageCorrTimetrace: cI time averages will be performed by dividing the {0} time points into {1} windows of sizes {2}'.format(CorrData.shape[0], len(int_bounds), [intb[1]-intb[0] for intb in int_bounds]))
        logging.debug('Detailed interval bounds: {0}'.format(int_bounds))
        logging.debug('original cI has shape ' + str(CorrData.shape) + '. Averaged g2m1 has shape ' + str(g2m1.shape) + ' (check: ' + str(g2m1_avgnum.shape) + ')')
    
    for cur_tavg_idx in range(len(int_bounds)):
        g2m1_lags[cur_tavg_idx,:len(g2m1_laglist[cur_tavg_idx])] = g2m1_laglist[cur_tavg_idx]
        for tidx in range(*int_bounds[cur_tavg_idx]): # all timepoints in current interval
            for lidx in range(CorrData.shape[1]): # all available lagtimes in input correlation matrix
                if (tidx < len(g2m1_alllags[lidx])): # g2m1_alllags[lidx] is the list of time delays in physical units associated to lidx-th integer lagtime
                                                     # don't consider reference times tidx >= len(g2m1_alllags[lidx]), as the second time points is beyond maximum timepoint
                    if (sharp_bound == False) or (tidx + Lagtimes_idxlist[lidx] >= int_bounds[cur_tavg_idx][0] and \
                                                  tidx + Lagtimes_idxlist[lidx] < int_bounds[cur_tavg_idx][1]):
                        # find which lagtime in physical units is closest to the current lagtime
                        cur_lagidx = np.argmin(np.abs(np.subtract(g2m1_laglist[cur_tavg_idx], g2m1_alllags[lidx][tidx])))
                        if (~np.isnan(CorrData[tidx,lidx])):
                            g2m1_avgnum[cur_tavg_idx,cur_lagidx] += 1
                            g2m1[cur_tavg_idx,cur_lagidx] += CorrData[tidx,lidx]
                    elif verbose:
                        logging.debug('Time index {0} and lag index {1} (d{2}) skipped because of sharp bound {3}'.format(tidx, lidx, Lagtimes_idxlist[lidx], int_bounds[cur_tavg_idx]))
    g2m1 = np.divide(g2m1, g2m1_avgnum)
    
    if return_stderr:
        g2m1_err = np.zeros_like(g2m1)
        for cur_tavg_idx in range(len(int_bounds)):
            for tidx in range(*int_bounds[cur_tavg_idx]):
                for lidx in range(CorrData.shape[1]):
                    if (tidx < len(g2m1_alllags[lidx])):
                        if (sharp_bound == False) or (tidx + Lagtimes_idxlist[lidx] >= int_bounds[cur_tavg_idx][0] and \
                                                      tidx + Lagtimes_idxlist[lidx] < int_bounds[cur_tavg_idx][1]):
                            cur_lagidx = np.argmin(np.abs(np.subtract(g2m1_laglist[cur_tavg_idx], g2m1_alllags[lidx][tidx])))
                            if (~np.isnan(CorrData[tidx,lidx])):
                                g2m1_err[cur_tavg_idx,cur_lagidx] += np.square(CorrData[tidx,lidx]-g2m1[cur_tavg_idx,cur_lagidx])
        g2m1_err = np.sqrt(np.divide(g2m1_err, g2m1_avgnum))
        return g2m1, g2m1_lags, g2m1_err
    else:
        return g2m1, g2m1_lags

def ValidateShiftRange(ShiftRange, ImageShape=None, useROI=None):
    '''
    Validates the range of drifts that can be detected for a given image size and a given ROI size
    
    Parameters
    ----------
    - SearchRange : integer, couple of integers or 4D vector with boundaries for search range, in the form [min_x, max_x, min_y, max_y]
                    where min_x, max_x are minimum and maximum x lags (along columns)
                          min_y, max_y are minimum and maximum y lags (along rows)
                    if integer, search range is [-margin, margin, -margin, margin]
                    if couple of integers [margin_x, margin_y] search range is [-margin_x, margin_x, -margin_y, margin_y]
    - ImageShape :  shape of the input images [num_row, num_col], or None
                    if None, the program won't check that the correctly-formatted SearchRange is compatible with image size
    - useROI :      ROI coordinates, in the form [min_row, min_col, max_row, max_col], or None
                    
    Returns
    -------
    - res:          validated range, in the form [min_x, max_x, min_y, max_y]
    '''
    if not isinstance(ShiftRange, collections.abc.Iterable):
        res = [ShiftRange, ShiftRange]
    else:
        res = ShiftRange
    if len(res) == 2:
        res = [-res[0], res[0], -res[1], res[1]]
    if ImageShape is not None and useROI is not None:
        max_range = [-useROI[1], ImageShape[1]-useROI[3], -useROI[0], ImageShape[0]-useROI[2]]
        res = [max(res[0], max_range[0]), min(res[1], max_range[1]), max(res[2], max_range[2]), min(res[3], max_range[3])]
    return res

def ValidateShiftROI(SearchROI, ShiftRange, ImageShape, ValidateRange=True, debugMode=False):
    if ValidateRange:
        ShiftRange = ValidateShiftRange(ShiftRange)
    if debugMode:
        if SearchROI is None:
            logging.debug('Searching for the largest ROI able to be shifted in range {0} within image of shape {1}'.format(ShiftRange, ImageShape))
        else:
            logging.debug('Validating compatibility of ROI with boundaries {0} to be shifted in range {1} within image of shape {2}'.format(SearchROI, ShiftRange, ImageShape))
    min_x, min_y = max(0, -ShiftRange[0]), max(0, -ShiftRange[2])
    max_x, max_y = min(ImageShape[1], ImageShape[1]-ShiftRange[1]), min(ImageShape[0], ImageShape[0]-ShiftRange[3])
    largestROI = [min_y, min_x, max_y, max_x]
    if (largestROI[2] <= largestROI[0] or largestROI[3] <= largestROI[1]):
        logging.error('No ROI compatible with image shape {0} and search range {1}: ROI {2} has negative edge sizes'.format(ImageShape, ShiftRange, largestROI))
        return None
    if SearchROI is None:
        resROI = largestROI
    else:
        resROI = [min(largestROI[2]-1, max(SearchROI[0], largestROI[0])), min(largestROI[3]-1, max(SearchROI[1], largestROI[1])), 
                  max(largestROI[0]+1, min(SearchROI[2], largestROI[2])), max(largestROI[1]+1, min(SearchROI[3], largestROI[3]))]
    if debugMode:
        logging.debug('Largest ROI able to be shifted: {0}. Final result: {1}'.format(largestROI, resROI))
    return resROI
    

def CalcCrosscorrMatrix(Image, Reference, SearchRange=1, SearchROI=None, ValidateInput=True, debugMode=False):
    '''
    Calculates the spatial crosscorrelation matrix between two images
    
    Parameters
    ----------
    - Image, Reference: 2D vectors (input images). They must have the same shape.
    - SearchRange : integer, couple of integers or 4D vector with boundaries for search range, in the form [min_x, max_x, min_y, max_y]
                    where min_x, max_x are minimum and maximum x lags (along columns)
                          min_y, max_y are minimum and maximum y lags (along rows)
                    if integer, search range is [-margin, margin, -margin, margin]
                    if couple of integers [margin_x, margin_y] search range is [-margin_x, margin_x, -margin_y, margin_y]
    - SearchROI :   4D vector with ROI definition, in the form [min_row, min_col, max_row, max_col], 
                    compatible with ROIproc.ROIboundingBoxes
                    If None, it is set to the largest subset allowing to explore the whole SearchRange
                    NOTE: ROI should be far enough from the boundaries to allow searching in the expected range.
                          If this is not the case, SearchRange will be adapted
    - ValidateInput : if True, validate SearchRange and SearchROI. Else, assume that it will be valid (to increase computation speed)

    Returns
    -------
    - Xcorr:        cross-correlation matrix.
    '''
    if (ValidateInput):
        im_shape = Image.shape
        SearchRange = ValidateShiftRange(SearchRange, im_shape, SearchROI)
        SearchROI = ValidateShiftROI(SearchROI, SearchRange, im_shape)
        if SearchROI is None:
            return None
    Xcorr = np.empty((SearchRange[3]-SearchRange[2]+1, SearchRange[1]-SearchRange[0]+1), dtype=float)
    Ref_crop = Reference[SearchROI[0]:SearchROI[2], SearchROI[1]:SearchROI[3]]
    Ref_crop_meansquare = np.mean(np.square(Ref_crop))
    if debugMode:
        logging.debug('CalcCrosscorrMatrix: Image has shape {0} (check: {1}), cropped ROI has shape {2}, search range is {3}, Xcorr has shape {4}'.format(Image.shape, 
                                                                                                     Reference.shape, SearchROI, SearchRange, Xcorr.shape))
        logging.debug(str(Reference.shape) + '[' + str(SearchROI[0]) + ':' + str(SearchROI[2]) + ', ' 
                      + str(SearchROI[1]) + ':' + str(SearchROI[3]) + '] -> ' + str(Ref_crop.shape))
    for iRow in range(Xcorr.shape[0]):
        lagy = iRow + SearchRange[2]
        for iCol in range(Xcorr.shape[1]):
            lagx = iCol + SearchRange[0]
            Image_crop = Image[SearchROI[0]+lagy:SearchROI[2]+lagy, SearchROI[1]+lagx:SearchROI[3]+lagx]
            Xcorr[iRow, iCol] = np.mean(Image_crop * Ref_crop) / np.sqrt(np.mean(np.square(Image_crop)) * Ref_crop_meansquare)
    return Xcorr

def FindParabolaExt(_corr, _x0, _y0, _x3, _y3, _a0, _a1, _a2, _a4, _a5):
    '''
    Function called by CalcDisplacement() to find subpixel parabola coordinates given 5 fixed points, 5 computed coefficients and a sixth new point
    '''
    _a3 = (_corr[_y3,_x3] - _a0 - _a1*(_x3-_x0) - _a2*(_x3-_x0)*(_x3-_x0+1) - _a4*(_y3-_y0) - _a5*(_y3-_y0)*(_y3-_y0+1))/((_x3-_x0)*(_y3-_y0))
    #here is the extremum
    ooden = 1./(4*_a2*_a5 - _a3**2)
    _f1 = (_a5*(_y0-1) - _a4)
    _f2 = (2*_a2*_a5 - _a3**2)
    xp = (_a5*(2*_a2*(_x0-1) + _a3*_y0 - 2*_a1) - _f1*_a3 + _f2*_x0)*ooden
    yp = (_a3*(_a2 + _a1) + 2*_f1*_a2 + _f2*_y0)*ooden
    zp = _a0 + _a1*(xp-_x0) + _a2*(xp-_x0)*(xp-_x0+1) + _a3*(xp-_x0)*(yp-_y0) + _a4*(yp-_y0) + _a5*(yp-_y0)*(yp-_y0+1)
    return xp, yp, zp

def FindSubpixelPeak(Matrix, TopLeftCoord=(0,0)):
    '''
    Finds the coordinates of the local maximum with subpixel resolution
    
    Parameters
    ----------
    - Matrix: 2D vector
    - TopLeftCoord : couple of float representing the coordinates (x, y) of top-left pixel of the Matrix

    Returns
    -------
    - xpeak, ypeak: subpixel-resolved coordinates of the crosscorrelation peak 
    - peak_height:  height of the subpixel-resolved crosscorrelation peak
                    If the pixel-resolved max falls on the edge of the search area, no subpixel search is performed and 
                    the routines returns the pixel-resolved position of the (apparent) max. peak_height is then set to -1
    '''
    imax = np.argmax(Matrix)
    cmax = imax % Matrix.shape[1]
    rmax = int((imax-cmax)/Matrix.shape[1])
    #check whether the max of corr lies on the edge of the search interval. If so, do not perform subpixel search
    if (rmax<=0 or rmax>=(Matrix.shape[0]-1) or cmax<=0 or cmax>=(Matrix.shape[1]-1)):
        return cmax + TopLeftCoord[0], rmax + TopLeftCoord[1], -1
    
    # Now we find the 2D parabola through 6 points around the maximum of corr
    # to get the maximum of crosscorrelation with subpixel resolution using Newton polynomial-like method
    # we define the paraboloid as z = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1) + a3*(x-x0)*(y-y0) + a4*(y-y0) + a5*(y-y0)*(y-y1).
    # Then, one can easily find by hand the coefficients a0...a5

    #here we impose the first 5 conditions, based on 5 points forming a cross centered on rmax,cmax
    a0 = Matrix[rmax,cmax]
    a1 = a0 - Matrix[rmax,cmax-1]
    a2 = (Matrix[rmax,cmax+1] - a0 - a1)*0.5
    a4 = a0 - Matrix[rmax-1,cmax]
    a5 = (Matrix[rmax+1,cmax] - a0 - a4)*0.5

    #we choose the 6th point in 4 different ways (to insure symmetry) and average over the corresponding results. Note that
    #the paraboloid equation was written in such a way that the choice of the 6th point only affects a3.    
    peaks = np.empty((4, 3), dtype=float)
    p3s = [[cmax-1, rmax-1], [cmax+1, rmax+1], [cmax-1, rmax+1], [cmax+1, rmax-1]]
    for i in range(len(p3s)):
        peaks[i] = FindParabolaExt(Matrix, cmax, rmax, p3s[i][0], p3s[i][1], a0, a1, a2, a4, a5)
    peak_avg = np.mean(peaks, axis=0)
    
    return peak_avg[0] + TopLeftCoord[0], peak_avg[1] + TopLeftCoord[1], peak_avg[2]
    
def FindCrosscorrPeak(Image, Reference, SearchRange, SearchROI=None, SubgridShape=None, ValidateInput=True, debugMode=False):
    '''
    Calculates the position and height of the crosscorrelation between two images with subpixel resolution
    Peak position is identified as the max of a paraboloid that goes through 6 points about the pixel-resolved peak
    
    Parameters
    ----------
    - Image, Reference: 2D vectors (input images)
    - SearchRange : integer, couple of integers or 4D vector with boundaries for search range, in the form [min_x, max_x, min_y, max_y]
                    where min_x, max_x are minimum and maximum x lags (along columns)
                          min_y, max_y are minimum and maximum y lags (along rows)
                    if integer, search range is [-margin, margin, -margin, margin]
                    if couple of integers [margin_x, margin_y] search range is [-margin_x, margin_x, -margin_y, margin_y]
    - SearchROI :   None, 4D vector with ROI definition, in the form [min_row, min_col, max_row, max_col], 
                    compatible with ROIproc.ROIboundingBoxes
    - SubgridShape: None or couple of int (M,N). Eventually divide the validated SearchROI into a grid of MxN ROIs
                    (M ROIs along x axis, N along y axis), and process drifts independently in each subROI, 
                    to obtain statistics (average and standard deviation).
                    None corresponds to (1,1)

    Returns
    -------
    - xpeak, ypeak: subpixel-resolved coordinates of the crosscorrelation peak 
    - peak_height:  height of the subpixel-resolved crosscorrelation peak
                    If the pixel-resolved max falls on the edge of the search area, no subpixel search is performed and 
                    the routines returns the pixel-resolved position of the (apparent) max. peak_height is then set to -1
    - xperr, yperr, zperr: standard errors on above quantities, only provided if SubgridShape is defined
    '''
    assert Image.shape == Reference.shape, 'Input images in FindCrosscorrPeak should have the same shape. Instead, here we have {0} and {1}'.format(Image.shape, Reference.shape)
    if (ValidateInput):
        SearchRange = ValidateShiftRange(SearchRange, Image.shape, SearchROI)
        SearchROI = ValidateShiftROI(SearchROI, SearchRange, Image.shape)
    if SubgridShape is None:
        SubgridShape = (1,1)
    if debugMode:
        logging.debug('ROIproc.FindCrosscorrPeak started. Search range: {0}, SearchROI: {1}, SubgridShape: {2}'.format(SearchRange, SearchROI, SubgridShape))
    if np.prod(SubgridShape) == 1:
        Xcorr = CalcCrosscorrMatrix(Image, Reference, SearchRange, SearchROI, ValidateInput=False, debugMode=debugMode)
        return FindSubpixelPeak(Xcorr, TopLeftCoord=(SearchRange[0],SearchRange[2]))
    else:
        grid_x = np.linspace(SearchROI[1], SearchROI[3], num=SubgridShape[0]+1, endpoint=True, dtype=int)
        grid_y = np.linspace(SearchROI[0], SearchROI[2], num=SubgridShape[1]+1, endpoint=True, dtype=int)
        if debugMode:
            logging.debug('SubROI array generated: x={0}; y={1}'.format(grid_x, grid_y))
        xp_all, yp_all, zp_all = [], [], []
        count_bad = 0
        for ix in range(SubgridShape[0]):
            for iy in range(SubgridShape[1]):
                cur_xp, cur_yp, cur_zp = FindCrosscorrPeak(Image=Image, Reference=Reference, SearchRange=SearchRange, 
                                                           SearchROI=[grid_y[iy], grid_x[ix], grid_y[iy+1], grid_x[ix+1]], 
                                                           SubgridShape=None, ValidateInput=False, debugMode=debugMode)
                if cur_zp>0:
                    xp_all.append(cur_xp)
                    yp_all.append(cur_yp)
                    zp_all.append(cur_zp)
                else:
                    if debugMode:
                        logging.warn('SubROI [{0},{1}] with coordinates {2} returned analysis error - peak likely outside search range'.format(ix, iy, [grid_y[iy], grid_x[ix], grid_y[iy+1], grid_x[ix+1]]))
                    count_bad += 1
        if count_bad > 0:
            logging.warn('Spatial crosscorrelation on ROI {0} divided in subroi grid of shape {1} '.format(SearchROI, SubgridShape) +\
                         'returned analysis error for {0}/{1} ROIs (presumably peak outside search range)'.format(count_bad, np.prod(SubgridShape)))
        if len(xp_all)>0:
            return np.mean(xp_all), np.mean(yp_all), np.mean(zp_all), np.std(xp_all), np.std(yp_all), np.std(zp_all)
        else:
            return cur_xp, cur_yp, cur_zp, np.nan, np.nan, np.nan

def LoadFromConfig(ConfigParams, runAnalysis=True, outputSubfolder='reproc'):
    """Loads a ROIproc object from a config file like the one exported in ROIproc.ExportConfiguration
    
    Parameters
    ----------
    ConfigParams : full path of the config file to read or dict or Config object
    runAnalysis  : if the config file has an Analysis section, 
                   set runAnalysis=True to run the analysis after initializing the object
    outputSubfolder : save analysis output in a subfolder of Analysis.out_folder from configuration
                    if None, directly save output in Analysis.out_folder
                
    Returns
    -------
    the ROIproc object
    """
    
    config = cf.LoadConfig(ConfigParams)
    folder_root = config.Get('General', 'folder', None, str)
    
    strlog = 'ROIproc.LoadFromConfig reading configuration '
    if (type(ConfigParams) in [str]):
        strlog += 'from filename ' + str(ConfigParams)
        config_folder = sf.GetFolderFromCompletePath(ConfigParams)
        if folder_root is None:
            folder_root = config_folder
            logging.warn('ROIproc.LoadFromConfig: root folder inferred from configuration filename: ' + str(folder_root))
        elif folder_root != config_folder:
            logging.warn('ROIproc.LoadFromConfig: root folder from configuration file ({0}) is different from that inferred from configuration filename ({1})'.format(folder_root, config_folder))
    elif (type(ConfigParams) in [dict, collections.OrderedDict]):
        strlog += 'from dictionnary ({0} sections)'.format(len(ConfigParams.keys()))
    else:
        strlog += 'from Config object ({0} sections)'.format(ConfigParams.CountSections())
    if folder_root is None:
        strlog += ' -- No root folder specified!'
    else:
        strlog += ' -- Root folder: ' + str(folder_root)
    if config.HasSection('General'):
        strlog += ' -- version {0}, generated by {1} on {2}'.format(config.Get('General', 'version', 'NP', str),
                        config.Get('General', 'generated_by', 'NP', str), config.Get('General', 'generated_on', 'NP', str))
    logging.info(strlog)
    
    # MIfile
    if config.HasOption('MIfile', 'metadata_file'):
        miin_metadata = sf.GetAbsolutePath(config.Get('MIfile', 'metadata_file', None, str), root_path=folder_root)
    else:
        MIfile_keys = config.GetKeys('MIfile')
        logging.info('ROIproc.LoadFromConfig no "metadata_file" key found in section [MIfile] of config file: loading metadata directly from config file section (keys: {0}, expected keys: {1})'.format(MIfile_keys, MI.GetMetadataKeys()))
        if 'shape' not in MIfile_keys:
            logging.warn('ROIproc.LoadFromConfig no "shape" key in [MIfile] section of config file: resulting MIfile object will likely be corrupted')
        miin_metadata = config.ToDict(section='MIfile')
    miin_fname = config.Get('MIfile', 'filename', None, str)
    if config.HasOption('MIfile', 'is_stack'):
        miin_isstack = config.Get('MIfile', 'is_stack', False, bool)
        if miin_isstack and (type(miin_fname) in [str]):
            logging.warn('ROIproc.LoadFromConfig MIfile.is_stack={0} potentially inconsistent with MIfile.filename {1} of type {2}'.format(miin_isstack, miin_fname, type(miin_fname)))
    else:
        miin_isstack = (type(miin_fname) not in [str])
    if miin_isstack:
        mifile_info = 'MIstack ' + str(miin_fname)
        MIin = MIs.MIstack([sf.GetAbsolutePath(fname, root_path=folder_root) for fname in miin_fname], miin_metadata, Load=True, 
                           StackType=config.Get('MIfile', 'stack_type', 't', str))
    else:
        mifile_info = 'MIfile ' + str(miin_fname)
        MIin = MI.MIfile(sf.GetAbsolutePath(miin_fname, root_path=folder_root), miin_metadata)
    logging.debug('ROIproc.LoadFromConfig loading ' + str(mifile_info) + ' (metadata: ' + str(miin_metadata) + ')')

    # ROIs
    if config.HasSection('ROIs'):
        roi_num = config.Get('ROIs', 'number', None, int)
        if roi_num is not None:
            logging.debug('ROIproc.LoadFromConfig loading {0} ROIs'.format(roi_num))
        roi_maskfile = sf.GetAbsolutePath(config.Get('ROIs', 'mask_file', None, str), root_path=folder_root)
        logging.debug('ROIproc.LoadFromConfig reading integer mask {0} (shape {1} expected)'.format(roi_maskfile, MIin.ImageShape()))
        roi_int_mask = MI.ReadBinary(roi_maskfile, MIin.ImageShape(), 'i')
        if roi_int_mask is None:
            ROImasks = None
            logging.warn('ROIproc.LoadFromConfig error reading binary ROI masks from integer mask {0}'.format(roi_maskfile))
        else:
            ROImasks = [roi_int_mask==b for b in range(np.max(roi_int_mask)+1)]
            logging.debug('ROIproc.LoadFromConfig: {0} binary ROI masks loaded from integer mask {1} of shape {2}'.format(len(ROImasks), roi_maskfile, roi_int_mask.shape))
        ROImetadata = config.ToDict(section='ROIs')
        if config.HasOption('ROIs', 'coord_file'):
            roi_coord_file = sf.GetAbsolutePath(config.Get('ROIs', 'coord_file', '', str), root_path=folder_root)
            ROIcoords, ROInames, NormFact, ROIbb = iof.LoadROIcoords(roi_coord_file)
            ROImetadata['coords'] = np.array2string(ROIcoords, separator=',')
            ROImetadata['coord_names'] = ROInames
            logging.debug('ROIproc.LoadFromConfig: ROI coordinates has shape {0} (coordinate names: {1})'.format(ROIcoords.shape, ROInames))
        else:
            ROIcoords, ROInames, NormFact, ROIbb = None, None, None, None
            logging.info('ROIproc.LoadFromConfig: No ROIcoord file: ROI coordinates will be automatically generated')
    else:
        ROImasks = None
        ROImetadata = None
        if config.HasSection('SALS'):
            logging.info('ROIproc.LoadFromConfig: No ROIs section in configuration file. SALS section has been detected: ROIs will probably be inherited')
        else:
            logging.warn('ROIproc.LoadFromConfig: No ROIs section in configuration file. ROIproc will be initialized with default ROI')
    
    # Times
    if config.HasOption('ImgTimes', 'values'):
        img_times = config.Get('ImgTimes', 'values', [1.], float)
        logging.debug('ROIproc.LoadFromConfig: {0} image times loaded from config file: {1}'.format(len(img_times), img_times))
    else:
        num_times = config.Get('ImgTimes', 'number', 0, str)
        imgtimes_fname = config.Get('ImgTimes', 'file', None, str)
        if miin_isstack:
            imgtimes_fname = [sf.GetAbsolutePath(fname, root_path=folder_root) for fname in imgtimes_fname]
        else:
            imgtimes_fname = sf.GetAbsolutePath(imgtimes_fname, root_path=folder_root)
        imgtimes_usecol = config.Get('ImgTimes', 'usecol', 0, int)
        imgtimes_skiprow = config.Get('ImgTimes', 'skiprow', 0, int)
        img_times = iof.LoadImageTimes(imgtimes_fname, usecols=imgtimes_usecol, skiprows=imgtimes_skiprow, root_folder=None, return_unique=False)
        logging.debug('ROIproc.LoadFromConfig: {0} image times loaded from file {1}, column {2} (expected: {3})'.format(len(img_times), imgtimes_fname, imgtimes_usecol, num_times))
    if config.HasOption('ExpTimes', 'values'):
        exp_times = config.Get('ExpTimes', 'values', [1.], float)
        logging.debug('ROIproc.LoadFromConfig: {0} exposure times loaded from config file: {1}'.format(len(exp_times), exp_times))
    else:
        num_exptimes = config.Get('ExpTimes', 'number', 0, str)
        exptimes_fname = sf.GetAbsolutePath(config.Get('ExpTimes', 'file', None, str), root_path=folder_root)
        if miin_isstack:
            exptimes_fname = [sf.GetAbsolutePath(fname, root_path=folder_root) for fname in exptimes_fname]
        else:
            exptimes_fname = sf.GetAbsolutePath(exptimes_fname, root_path=folder_root)
        exptimes_usecol = config.Get('ExpTimes', 'usecol', 0, int)
        exptimes_skiprow = config.Get('ExpTimes', 'skiprow', 0, int)
        exp_times = iof.LoadImageTimes(exptimes_fname, usecols=exptimes_usecol, skiprows=exptimes_skiprow, root_folder=None, return_unique=True)
        logging.debug('ROIproc.LoadFromConfig: {0} exposure times loaded from file {1}, column {2} (expected: {3})'.format(len(exp_times), exptimes_fname, exptimes_usecol, num_exptimes))

    ROI_proc = ROIproc(MIin, ROImasks, ROImetadata=ROImetadata, imgTimes=img_times, expTimes=exp_times)
    logging.info('ROIproc object loaded!')
        
    if runAnalysis:
        ROI_proc.RunFromConfig(config, AnalysisSection='Analysis', OutputSubfolder=outputSubfolder)
            
    return ROI_proc




class ROIproc():
    """ Class to process MIfile computing averages and time correlations on Regions Of Interest (ROIs) """
    
    def __init__(self, MIin, ROImasks, ROImetadata=None, imgTimes=None, expTimes=[1]):
        """
        Initialize SALS

        Parameters
        ----------
        MIin :      input MIfile or MIstack. It can be empty (i.e. initialized with metadata only)
        ROImasks :  list of binary masks. Each mask is a 2D binary array with same shape as MIin.ImageShape()
                    True values (nonzero) denote pixels that will be included in the analysis,
                    False values (zeroes) will be excluded
                    If None, all pixels will be included.
        ROImetadata : None or dict with additional ROI metadata (see SetROIs function)
                    
        imgTimes :  Float array of length Nimgs. i-th element will be the time of the image.
                    If none, images will be 
        """
        
        self.MIinput   = MIin
        self.SetROIs(ROImasks, ROImetadata)
        self._loadTimes(imgTimes)
        self.SetExptimes(expTimes)
        self._initConstants()
        
    def _loadTimes(self, imgTimes=None):
        if imgTimes is None:
            self.imgTimes = np.arange(0, self.MIinput.ImageNumber() * 1./self.MIinput.GetFPS(), 1./self.MIinput.GetFPS())
            logging.debug('{0} image times automatically generated from MI metadata (fps={1}Hz)'.format(len(self.imgTimes), self.MIinput.GetFPS()))
        else:
            if (len(imgTimes) < self.MIinput.ImageNumber()):
                raise ValueError('Image times ({0}) should at least be as many as the image number ({1}).'.format(len(imgTimes), self.MIinput.ImageNumber()))
            elif (len(imgTimes) > self.MIinput.ImageNumber()):
                logging.warn('More image times ({0}) than images ({1}). Only first {1} image times will be considered'.format(len(imgTimes), self.MIinput.ImageNumber()))
                self.imgTimes  = imgTimes[:self.MIinput.ImageNumber()]
            else:
                self.imgTimes  = imgTimes
                logging.debug('{0} image times loaded (Image number: {1})'.format(len(self.imgTimes), self.MIinput.ImageNumber()))

     
    def _initConstants(self):
        #Constants
        self.MaxSafeAvgIntensity = 40
        self.LoopMaxInfoN = 100
        self.dt_tolerance = 1e-2
        self.dt_tolerance_isrelative = True
        self.DebugMode = False
        self.txt_delim = '\t'
        self.txt_comm = '#'
        self.savetxt_kwargs = {'delimiter': self.txt_delim, 'comments': self.txt_comm}
        
    def __repr__(self):
        if (self.MIinput.IsStack()):
            return '<ROIproc object: MIstack (' + str(self.MIinput.Count()) + ' MIfiles) + ' + str(self.CountROIs()) + ' ROIs>'
        else:
            return '<ROIproc object: MIfile (' + str(self.MIinput.FileName) + ') + ' + str(self.CountROIs()) + ' ROIs>'

    def __str__(self):
        str_res  = '\n|-----------------|'
        str_res += '\n|  ROIproc class: |'
        str_res += '\n|-----------------+---------------'
        str_res += '\n| Input           : '
        if (self.MIinput.IsStack()):
            str_res += 'MIstack (' + str(self.MIinput.Count()) + ' MIfiles)'
        else:
            str_res += 'MIfile (' + self.MIinput.FileName + ')'
        str_res += ', ' + str(self.MIinput.ImageNumber()) + ' images'
        str_res += '\n| ROIs            : ' + str(self.CountROIs()) + ' (' + str(self.CountValidROIs()) + ' valid, ' + str(self.CountEmptyROIs()) + ' empty)'
        str_res += '\n| Exposure times  : ' + str(self.NumExpTimes()) + ', from ' + str(self.expTimes[0]) + ' to ' + str(self.expTimes[-1])
        str_res += '\n|-----------------+---------------'
        return str_res
        
    def SetROIs(self, ROImasks, ROImetadata=None):
        '''
        Sets ROIs from list of binary masks
        
        Parameters
        ----------
        ROImasks :  list of binary masks
        ROImetadata : None or dict with ROI metadata. Can contain one or more of the following keys:
                    'coords' : ndarray with ROI coordinates. If not set, ROIcoords=np.arange(self.CountROIs())
                    'coord_names' : list of str: coordinate names. If not set, ROIcoord_names = [coord0, coord1, ...]
                    'box_margin' : int with margin left for drift measurement. Default: 0

        '''
        
        if ROImasks is None:
            ROImasks = [np.ones(self.MIinput.ImageShape(), dtype=np.dtype('b'))]
            
        if ROImetadata is not None:
            roi_meta = cf.LoadConfig(ROImetadata, SectionName='ROIs')
            ROIcoords = roi_meta.Get('ROIs', 'coords', None, float)
            ROIcoord_names = roi_meta.Get('ROIs', 'coord_names', None, str)
            BoundingBoxMargin = roi_meta.Get('ROIs', 'box_margin', 0, int)
        else:
            ROIcoords = None
            ROIcoord_names = None
            BoundingBoxMargin = 0
        
        self.ROIs = BinaryToIntegerMask(ROImasks)
        self.ROI_masks = np.asarray(ROImasks)
        self.ROI_maskSizes = np.array([np.sum(self.ROI_masks[b]) for b in range(self.CountROIs())])
        
        self.UpdateBBmargin(BoundingBoxMargin)
        
        if self.CountEmptyROIs() > 0:
            if self.CountValidROIs() > 0:
                logging.warning('There are {0} out of {1} empty masks'.format(self.CountEmptyROIs(), self.CountROIs()))
            else:
                logging.error('ROI mask is empty (no valid ROIs found)')
        else:
            logging.info('Set {0} valid ROIs'.format(self.CountROIs()))
            
        if ROIcoords is None:
            ROIcoords = np.arange(self.CountROIs())
        else:
            ROIcoords = np.asarray(ROIcoords)
        if ROIcoords.ndim == 1:
            ROIcoords = np.expand_dims(ROIcoords, axis=1)
        self.ROIcoords = ROIcoords
        if ROIcoord_names is None:
            ROIcoord_names = ['coord' + str(i) for i in range(ROIcoords.shape[1])]
        self.ROIcoord_names = ROIcoord_names
        
    def UpdateBBmargin(self, BoundingBoxMargin):
        if BoundingBoxMargin<0:
            logging.error('Bounding box margin cannot be negative. Set to 0')
            BoundingBoxMargin = 0
        self.BoundingBoxMargin = BoundingBoxMargin
        self.BoundingBox = FindBoundingBoxROI(self.ROI_masks, BoundingBoxMargin)
        self.CropROIbb = [self.BoundingBox[1], self.BoundingBox[0], self.BoundingBox[3]-self.BoundingBox[1], self.BoundingBox[2]-self.BoundingBox[0]]
        self.ROIboundingBoxes = []
        self.ROI_masks_crop = []
        trivial_cropbb = True
        for ridx in range(self.CountROIs()):
            if self.IsROIvalid(ridx):
                cur_bb = FindBoundingBoxROI(self.ROI_masks[ridx], margin=0)
                self.ROIboundingBoxes.append([cur_bb[0]-self.BoundingBox[0], cur_bb[1]-self.BoundingBox[1], cur_bb[2]-self.BoundingBox[0], cur_bb[3]-self.BoundingBox[1]])
                cur_maskcrop = self.ROI_masks[ridx,cur_bb[0]:cur_bb[2],cur_bb[1]:cur_bb[3]]
                if np.any(cur_maskcrop==0):
                    self.ROI_masks_crop.append(cur_maskcrop)
                    trivial_cropbb = False
                else:
                    self.ROI_masks_crop.append(np.ones((cur_bb[2]-cur_bb[0],cur_bb[3]-cur_bb[1])))
            else:
                logging.debug('ROI {0} is empty'.format(ridx))
                self.ROIboundingBoxes.append([0, 0, self.BoundingBox[2]-self.BoundingBox[0], self.BoundingBox[3]-self.BoundingBox[1]])
                self.ROI_masks_crop.append(np.zeros((self.BoundingBox[2]-self.BoundingBox[0], self.BoundingBox[3]-self.BoundingBox[1]), dtype=np.dtype('b')))
                trivial_cropbb = False
        self.ROIboundingBoxes = np.asarray(self.ROIboundingBoxes)
        if trivial_cropbb:
            self.ROI_masks_crop = None

    def SetExptimes(self, expTimes):
        if not hasattr(expTimes, "__len__"):
            try:
                float_expt = float(expTimes)
                expTimes = [float_expt]  # if expTimes is a scalar, turn it into a 1-element list
            except:
                logging.error('ROIproc.SetExptimes() called with non-numeric argument: ' + str(expTimes))
                expTimes = []
        expTimes = np.asarray(expTimes)
        if expTimes.size > 0:
            _exps = np.unique(expTimes)
            # check that expTimes is sorted:
            #assert np.all(np.diff(expTimes) >= 0), 'Exposure times ' + str(expTimes) + ' must be sorted!'
            self.expTimes = np.asarray(sorted(_exps))
            if len(self.expTimes) > 1:
                logging.debug('Set {0} exptimes, sorted from {1} to {2}'.format(len(self.expTimes), self.expTimes[0], self.expTimes[-1]))
            else:
                logging.debug('Set one single exposure time: {0}'.format(self.expTimes[0]))
        else:
            logging.error('ROIproc.SetExptimes() called with empty expTimes list: ' + str(expTimes))

    def IsROIvalid(self, ROIidx):
        return self.ROI_maskSizes[ROIidx]>0
    def CountROIs(self):
        return len(self.ROI_masks)
    def CountEmptyROIs(self):
        return self.CountROIs() - self.CountValidROIs()
    def CountValidROIs(self):
        return np.count_nonzero(self.ROI_maskSizes)
    def ImageNumber(self):
        return self.MIinput.ImageNumber()
    def NumTimes(self):
        return self.MIinput.ImageNumber() // len(self.expTimes)
    def NumExpTimes(self):
        return len(self.expTimes)
    def StackInput(self):
        return self.MIinput.IsStack()
    def ReadCIfile(self, fpath):
        return iof.ReadCIfile(fpath)
    
    def GetImage(self, image_idx, buffer=None):
        '''
        Assumes that buffer contains images already cropped to size CropROIbb
        '''
        return self.MIinput.GetImage(image_idx, cropROI=self.CropROIbb, buffer=buffer, buffer_crop=False)
    def GetCroppedShape(self):
        return (self.CropROIbb[3], self.CropROIbb[2])
        
    def ReadMI(self):
        return self.MIinput.Read(cropROI=self.CropROIbb)

    def SaveIavg(self, SaveFolder, Iavg, AllExpData=None):
        """ Saves output of SLS analysis

        Parameters
        ----------
        Iavg :          2D array of shape (NumTimes(), NumROIs())
        AllExpData :    None or [Iav_allexp, best_exptime_idx], data with all exposure times
        
        Saves
        -----
        - Iavg.dat :     file with normalized average intensity computed using best exposure times
        - exptimes.dat:  file with index of best exposure times chosen (only if AllExpData is not None)
        - Iavg_raw.dat:  file with all average intensities for all exposure times (only if AllExpData is not None)
        - ROIcoords.dat: file with ROI coordinates and normalization factors
        - ROI_mask.raw:  raw file with pixel mask
        """
        
        sf.CheckCreateFolder(SaveFolder)
        ROIhdr_str = self.txt_delim.join(self.ROIcoord_names)
        flat_times = self.imgTimes.reshape(-1)
        
        str_hdr_avg = ROIhdr_str + ''.join([self.txt_delim+'t{0:.3f}'.format(flat_times[i])
                                            for i in range(0, self.ImageNumber(), self.NumExpTimes())])
        np.savetxt(os.path.join(SaveFolder, 'Iavg.dat'), np.append(self.ROIcoords[:,:2], Iavg.T, axis=1), 
                   header=str_hdr_avg, **self.savetxt_kwargs)
        
        if AllExpData is not None:
            ROIavgs_allExp, BestExptime_Idx = AllExpData
            np.savetxt(os.path.join(SaveFolder, 'exptimes.dat'), np.append(self.ROIcoords[:,:2], BestExptime_Idx.T, axis=1), 
                       header=str_hdr_avg, **self.savetxt_kwargs)
            str_hdr_raw = ROIhdr_str + ''.join([self.txt_delim+'t{0:.3f}_e{1:.3f}'.format(flat_times[i], self.expTimes[i%len(self.expTimes)])
                                                for i in range(len(flat_times))])
            np.savetxt(os.path.join(SaveFolder, 'Iavg_raw.dat'), np.append(self.ROIcoords[:,:2], ROIavgs_allExp.reshape((-1, ROIavgs_allExp.shape[-1])).T, axis=1), 
                       header=str_hdr_raw, **self.savetxt_kwargs)

    def LoadIavg(self, outFolder, skipcols=2):
        Ir_allexp, Ir, best_exptimes = None, None, None
        Iavg_fpath = os.path.join(outFolder, 'Iavg.dat')
        if os.path.isfile(Iavg_fpath):
            Iavg = np.loadtxt(Iavg_fpath, **self.savetxt_kwargs)
            if (Iavg.shape[1] > skipcols):
                Iavg = Iavg[:,skipcols:].T
                logging.debug('Loading average intensity data from {0}: output has shape {1}'.format(Iavg_fpath, Iavg.shape))
            else:
                Iavg = None
                logging.warn('Loading average intensity data from {0} failed (column number {1} not exceeding columns to be skipped, {2}): None returned'.format(Iavg_fpath, Iavg.shape[1], skipcols))
        else:
            Iavg = None
        if Iavg is None:
            Iav_allexp = None
        else:
            Iavraw_fpath = os.path.join(outFolder, 'Iavg_raw.dat')
            if os.path.isfile(Iavraw_fpath):
                Iav_allexp = np.loadtxt(Iavraw_fpath, **self.savetxt_kwargs)
                if (Iav_allexp.shape[1] > skipcols):
                    Iav_allexp = Iav_allexp[:,skipcols:].T
                    Iav_allexp = np.expand_dims(Iav_allexp, axis=1)
                    logging.debug('Loading raw average intensity data from {0}: output has shape {1}'.format(Iavraw_fpath, Iav_allexp.shape))
                else:
                    Iav_allexp = None
                    logging.warn('Loading raw average intensity data from {0} failed (column number {1} not exceeding columns to be skipped, {2}): None returned'.format(Iavraw_fpath, Iav_allexp.shape[1], skipcols))
        Iexp_fpath = os.path.join(outFolder, 'exptimes.dat')
        if os.path.isfile(Iexp_fpath):
            best_exptimes = np.loadtxt(Iexp_fpath, **self.savetxt_kwargs)
            if (best_exptimes.shape[1] > skipcols):
                best_exptimes = best_exptimes[:,skipcols:].T
                logging.debug('Loading best exposure times from {0}: output has shape {1}'.format(Iexp_fpath, best_exptimes.shape))
            else:
                best_exptimes = None
                logging.warn('Loading best exposure times from {0} failed (column number {1} not exceeding columns to be skipped, {2}): None returned'.format(Iexp_fpath, best_exptimes.shape[1], skipcols))
        else:
            best_exptimes = None
                
        return Iav_allexp, Iavg, best_exptimes

    def ROIaverageIntensity(self, stack1=None, no_buffer=False, imgs=None, no_boundingbox=False):
        return self.ROIaverageProduct(stack1, stack2=None, no_buffer=no_buffer, imgs=imgs, no_boundingbox=no_boundingbox)
    
    def ROIaverageProduct(self, stack1=None, stack2=None, no_buffer=False, imgs=None, no_boundingbox=False):
        """ ROI average product of images

        Parameters
        ----------
        stack1 : list of indexes. Images will be either read by MIinput or retrieved from img_buffer
                 if None, all images will be included in stack1
        stack2 : None, or list of indexes
                 - if None: function will return averages of single images (in stack1)
                 - if list: length should be the same as stack1
        no_buffer : if True, avoid reading all images to a buffer, but read images one by one
                    (dumping them afterwards)
        imgs : None or 3D array with buffered images. If None, images will be read from MIinput

        Returns
        -------
        AvgRes : 2D array. Element [i,j] is the average of i-th image (or the product of i-th images in the two stacks) on j-th ROI
        """
        if stack1 is None:
            stack1 = np.arange(self.ImageNumber())
        num_ROI = self.CountROIs()
        if no_boundingbox:
            use_bb = None
        else:
            use_bb = self.ROIboundingBoxes
            
        if (self.StackInput() or no_buffer):
            AvgRes = np.nan*np.ones((len(stack1), num_ROI), dtype=float)
            for i in range(AvgRes.shape[0]):
                if stack2 is None:
                    AvgRes[i], NormList = ROIAverage(self.GetImage(stack1[i], buffer=imgs), self.ROI_masks_crop, boolMask=True, 
                                                     norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
                else:
                    if (stack1[i]==stack2[i]):
                        AvgRes[i], NormList = ROIAverage(np.square(self.GetImage(stack1[i], buffer=imgs)), self.ROI_masks_crop, boolMask=True, 
                                                         norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
                    else:
                        AvgRes[i], NormList = ROIAverage(np.multiply(self.GetImage(stack1[i], buffer=imgs), self.GetImage(stack2[i], buffer=imgs)), 
                                                         self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
                    if (self.DebugMode):
                        if (np.any(AvgRes[i]<0)):
                            min_idx = np.argmin(AvgRes[i])
                            logging.warn('Negative cross product value (image1: {0}, image2: {1}, ROI{2} avg: {3})'.format(stack1[i], stack2[i], min_idx, 
                                                                                                                           AvgRes[i][min_idx]))
                            logging.debug('   >>> Debug output for ROIAverage function:\n'
                                          + str(ROIAverage(np.multiply(self.GetImage(stack1[i], buffer=imgs), self.GetImage(stack2[i], buffer=imgs)), 
                                                           self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb, debug=True)))
        else:
            if imgs is None:
                imgs = self.ReadMI()
            if self.DebugMode:
                if len(stack1) < 100:
                    logging.debug('ROIproc.ROIaverageProduct processing buffer with {0} images. First stack ({1} indexes): {2}'.format(len(imgs), len(stack1), stack1))
                else:
                    logging.debug('ROIproc.ROIaverageProduct processing buffer with {0} images. First stack ({1} indexes): [{2},{3},...,{4},{5}]'.format(len(imgs), 
                                                                                                    len(stack1), stack1[0], stack1[1], stack1[-2], stack1[-1]))
            if stack2 is None:
                if self.DebugMode:
                    logging.debug('No second stack: averaging single images')
                cur_stack = imgs[stack1]
            elif np.array_equal(stack1, stack2):
                if self.DebugMode:
                    logging.debug('Equal stacks provided: averaging squared single images')
                cur_stack = np.square(imgs[stack1])
            else:
                if self.DebugMode:
                    if len(stack2) < 100:
                        logging.debug('Second stack ({0} indexes): {1}'.format(len(stack2), stack2))
                    else:
                        logging.debug('Second stack ({0} indexes): [{1},{2},...,{3},{4}]'.format(len(stack2), stack2[0], stack2[1], stack2[-2], stack2[-1]))
                cur_stack = np.multiply(imgs[stack1], imgs[stack2])
            if self.DebugMode:
                logging.debug('ROIproc.ROIaverageProduct averaging stack1=' + str(stack1) + ' and stack2=' + str(stack2) + ' in debug mode...')
                AvgRes, NormList = ROIAverage(cur_stack, self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb, debug=True)
            else:
                AvgRes, NormList = ROIAverage(cur_stack, self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
            
        return AvgRes
    
    def CalcCorrelation(self, t1, t2, d0corr=True):
        '''
        Calculate sparse correlations between times t1 and t2
        
        Parameters
        ----------
        - t1, t2: int or list of int: image times to be correlated, in image units
                  if t1 and t2 are int, output is 1D
                  if one between t1 and t2 is int or 1-element list, it will be replicated to match the legth of the other list
                  if both t1 and t2 are lists (they should be equal size, or either one should have just one element),
                  correlation will be computed between each t1[i], t2[i] couple.
        - d0corr: bool. True to apply d0 ('contrast') normalization
                  If False, result will be <I1*I2>/(<I1><I2>)
                  If True, result will be further normalized by the average contrast:
                        [(<I1*I1>/(<I1><I1)) + (<I2*I2>/(<I2><I2))] / 2
        
        Returns
        -------
        - corr: 1D or 2D array. 
                  If multiple times are given output is 2D: corr[i,j] is the correlation of i-th couple (t1[i], t2[i]) on j-th ROI
                  If a single t1 and t2 is given, output is 1D: corr[i] is the correlation between the two times computed on i-th ROI
        '''
        if not isinstance(t1, collections.abc.Iterable):
            t1 = [t1]
        if not isinstance(t2, collections.abc.Iterable):
            t2 = [t2]
        if len(t1) != len(t2):
            if len(t1)==1:
                t1 = t1*len(t2)
                logging.info('ROIproc.CalcCorrelation(): single first time ({0}) replicated {1} times to match length of second time ({2}).'.format(t1[0], len(t1), len(t2)))
            elif len(t2)==1:
                t2 = t2*len(t1)
                logging.info('ROIproc.CalcCorrelation(): single second time ({0}) replicated {1} times to match length of second time ({2}).'.format(t2[0], len(t2), len(t1)))
            else:
                logging.warning('ROIproc.CalcCorrelation(): number of first times ({0}) must match that of second times ({1}). Longer list will be cropped'.format(len(t1), len(t2)))
                common_len = min(len(t1), len(t2))
                t1 = t1[:common_len]
                t2 = t2[:common_len]
        avg_t1 = self.ROIaverageIntensity(t1)
        avg_t2 = self.ROIaverageIntensity(t2)
        corr = np.divide(self.ROIaverageProduct(t1, t2), np.multiply(avg_t1, avg_t2)) - 1
        if d0corr:
            contrast_t1 = np.divide(self.ROIaverageProduct(t1, t1), np.square(avg_t1)) - 1
            contrast_t2 = np.divide(self.ROIaverageProduct(t2, t2), np.square(avg_t2)) - 1
            corr = 2 * corr / (contrast_t1 + contrast_t2)
        return np.squeeze(corr)
            
    def FindBestExptimes(self, AverageIntensities):
        '''
        Find best exposure times based on ROI-averaged intensities
        
        Parameters
        ----------
        - AverageIntensities: 3D array. Element [i,j,k] is the average intensity of j-th ROI measured at k-th exposure time during i-th exposure time ramp
        
        Returns
        -------
        - ROIavgs_best: 2D array. Element [i,j] is the intensity of the best exposure time of j-th ROI during i-th time, normalized by the exposure time itself
        - BestExptime_Idx: 2D array, containing the index of the best exposure time selected for ROIavgs_best
        '''
        if AverageIntensities.ndim < 3:
            AverageIntensities = AverageIntensities.reshape((self.NumTimes(), self.NumExpTimes(), -1))
        ROIavgs_best = np.zeros((self.NumTimes(), AverageIntensities.shape[-1]), dtype=float)
        BestExptime_Idx = -1 * np.ones_like(ROIavgs_best, dtype=int)
        for idx, val in np.ndenumerate(ROIavgs_best):
            BestExptime_Idx[idx] = min(bisect.bisect(AverageIntensities[idx[0], :, idx[1]], self.MaxSafeAvgIntensity), len(self.expTimes)-1)
            ROIavgs_best[idx] = AverageIntensities[idx[0], BestExptime_Idx[idx], idx[1]] / self.expTimes[BestExptime_Idx[idx]]
        return ROIavgs_best, BestExptime_Idx
                             
    def doSLS(self, saveFolder, buf_images=None, no_buffer=False, force_calc=True):
        """ Run SLS analysis: compute average intensity, eventually choosing best exposure time for each ROI
        
        Parameters
        ----------
        - saveFolder: folder path, to save analysis output. 
                      If None, no output will be saved
        - buf_images: 3D array, buffer with images to be processed. If None, images will be loaded
        - no_buffer: bool, used only if buf_images is None. If False, the entire image stack will be read in a buffer at once.
                     otherwise, images will be loaded one by one
        - force_calc: bool. If False, program will search for previously computed SLS results and load those.
        
        Returns
        -------
        - ROIavgs_allExp : 3D array. Element [i,j,k] is the average of j-th exposure time in i-th exposure time sweep, averaged on k-th ROI
        - ROIavgs_best : 2D array. Element [i,j] is the best average intensity taken from i-th exposure time sweep, averaged on j-th ROI
        - BestExptime_Idx : 2D array (int). Element [i,j] is the index of the optimum exposure time for j-th ROI
        - buf_images : 3D array. Buffer of images eventually read during the analysis
        """
        
        if saveFolder is not None:
            sf.CheckCreateFolder(saveFolder)
            self.LastSaveFolder = saveFolder
        
        ROIavgs_allExp, ROIavgs_best, BestExptime_Idx = None, None, None
        if not force_calc:
             ROIavgs_allExp, ROIavgs_best, BestExptime_Idx = self.LoadIavg(saveFolder)
        
        if ROIavgs_allExp is None or ROIavgs_best is None or BestExptime_Idx is None:

            if buf_images is None:
                if no_buffer or self.StackInput()==True:
                    self.MIinput.OpenForReading()
                    buf_images = None
                else:
                    buf_images = self.ReadMI()

            # Compute average intensity for all images
            all_avg = self.ROIaverageIntensity(stack1=list(range(self.ImageNumber())), no_buffer=no_buffer, imgs=buf_images)
            if all_avg.shape[0] % (self.NumTimes() * self.NumExpTimes()) != 0:
                limit_len = self.NumTimes() * self.NumExpTimes()
                all_avg = all_avg[:limit_len]
                logging.warning('Number of images ({0}) is not a multiple of exposure times ({1}). '.format(self.ImageNumber(), self.NumExpTimes()) + 
                                'Average intensity output of shape {0} cannot be reshaped using number of times ({1}) '.format(all_avg.shape, self.NumTimes()) +
                                'and exposure times ({1}). Restricting SLS analysis to first {0} images'.format(self.NumExpTimes(), limit_len))
            ROIavgs_allExp = all_avg.reshape((self.NumTimes(), self.NumExpTimes(), -1))
            ROIavgs_best, BestExptime_Idx = self.FindBestExptimes(ROIavgs_allExp)
            if saveFolder is not None:
                self.SaveIavg(saveFolder, ROIavgs_best, [ROIavgs_allExp, BestExptime_Idx])
            
            logging.debug('ROIproc.doSLS: output saved')

            # TODO: time average SLS
        
        strlog = 'ROIproc.doSLS analysis returned: raw data (shape: {0})'.format(ROIavgs_allExp.shape) 
        strlog += ', Iavg data (shape: {0}), exptime data (shape: {1})'.format(ROIavgs_best.shape, BestExptime_Idx.shape)
        if buf_images is None:
            strlog += ', no buffer images'
        else:
            strlog += ', buffer images (shape {0})'.format(buf_images.shape)
        logging.debug(strlog)
        
        return ROIavgs_allExp, ROIavgs_best, BestExptime_Idx, buf_images

    def doDLS(self, saveFolder, lagtimes, reftimes='all', no_buffer=False, drift_corr=0, 
              force_SLS=True, save_transposed=False, export_configparams=None, include_negative_lags=False,
              g2m1_averageN=None, g2m1_reterr=False):
        """ Run SLS/DLS analysis

        Parameters
        ----------
        lagtimes :              'all' or list of int. 
                                - If 'all', all available lagtimes will be processed
                                - Otherwise, only specified lagtimes will be processed
        reftimes :              'all' or list of int. 
                                - If 'all', all reference times will be used
                                - Otherwise, specialize the analysis to a subset of reference times
        drift_corr:             int. if > 0, track peak of spatial crosscorrelations to find and correct for speckle drift
                                up to a maximum drift of drift_corr pixels. If 0, do not perform this extra step
                                NOTE: this analysis uses rectangular ROIs without masks.
                                ROI coordinates are set by ROIproc.ROIboundingBoxes
        no_buffer :             bool. If True, avoid reading full MIfile to RAM
        force_SLS :             bool. If False, program will load previously computed SLS results if available.
        save_transposed:        bool. Format of correlation timetrace output
                                - if False, classic cI output: one line per reference time, one column per time delay
                                - if True, transposed output: one line per time delay, one column per reference time
                                  NOTE: transposed output is incompatible with drift correction
        include_negative_lags:  Used if lagtimes=='all'. if False (default), only process prositive lagtimes. 
                                If reftimes=='all', negative lagtimes are redundant and include_negative_lags will be set to False.
                                If sparse reftimes and all lagtimes are processed, set include_negative_lags==True to include negative lagtimes
        export_configparams:    None or dict with additional configuration parameters to be exported to the output configuration file
        g2m1_averageN :         int or None. If None, average correlation timetraces on the whole time window to obtain one g2-1 curve per ROI
                                if N>0, average correlation timetraces on windows of N datapoints, 
                                to obtain (self.NumTimes() / N) g2-1 curves per ROI
        g2m1_reterr :           bool. True to return standard deviation of correlation curves averaged to produce g2-1
        """
        
        sf.CheckCreateFolder(saveFolder)
        self.LastSaveFolder = saveFolder
        fout = open(os.path.join(saveFolder, 'analysis_log.txt'), 'w')
            
        if reftimes=='all':
            DLS_reftimes = np.arange(self.NumTimes())
            if include_negative_lags:
                sf.LogWrite('ROIproc.doDLS(): No need of processing negative timelags with reftimes==all: include_negative_lags changed to False', 
                            fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
                include_negative_lags = False
        else:
            DLS_reftimes = np.asarray(reftimes)

        if lagtimes=='all':
            if include_negative_lags:
                DLS_lags = np.arange(-self.NumTimes()+1, self.NumTimes())
            else:
                DLS_lags = np.arange(self.NumTimes())
        else:
            DLS_lags = np.asarray(lagtimes)
            if np.min(DLS_lags) < 0 and not include_negative_lags:
                DLS_lags = DLS_lags[DLS_lags>=0]
                sf.LogWrite('ROIproc.doDLS(): Negative lagtimes specified with include_negative_lags==False. Negative lag times will be removed from the list', 
                            fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
        if DLS_lags[0] != 0:
            sf.LogWrite('ROIproc.doDLS(): 0 lagtime prepended to DLS_lags', 
                        fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
            DLS_lags = np.insert(DLS_lags, 0, 0)
        DLS_lagnum = len(DLS_lags)
        if self.DebugMode:
            sf.LogWrite('Complete list of lagtimes: ' + str(DLS_lags), 
                        fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
            
        if drift_corr>0:
            if self.ROI_masks_crop is not None:
                sf.LogWrite('ROIproc.doDLS(): using drift correction with non-rectangular ROIs or with a pixel mask excluding some pixels. ' +\
                            'WARNING: drift correction will take into account all pixels in ROI bounding boxes, disregarding masks', 
                            fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
            ValidROI = np.ones((len(self.ROIboundingBoxes)), dtype=bool)
            search_range = ValidateShiftRange(drift_corr, self.GetCroppedShape())
            search_ROIs = []
            count_ROImodif = 0
            for ridx in range(len(self.ROIboundingBoxes)):
                cur_valid_ROI = ValidateShiftROI(self.ROIboundingBoxes[ridx], search_range, self.GetCroppedShape(), debugMode=self.DebugMode)
                if cur_valid_ROI is None:
                    ValidROI[ridx] = False
                    sf.LogWrite('ROIproc.doDLS() error: ROI {0} (bounding box: {1}) incompatible with search range {2} '.format(ridx, self.ROIboundingBoxes[ridx], search_range) +\
                                'in image of cropped shape {0} (Original shape: {1}, bounding box margin: {2}). Did you forget to call ROIproc.UpdateBBmargin()?'.format(self.GetCroppedShape(), self.MIinput.ImageShape(), self.BoundingBoxMargin), 
                                fLog=fout, logLevel=logging.ERROR, add_prefix='\n'+sf.TimeStr()+' | ERROR: ')
                    search_ROIs.append(self.ROIboundingBoxes[ridx])
                else:
                    if not np.array_equal(cur_valid_ROI, self.ROIboundingBoxes[ridx]):
                        count_ROImodif += 1
                        sf.LogWrite('ROIproc.doDLS() error: ROI {0} (bounding box: {1}) has to be shrunk to {2} to accommodate search range {3} '.format(ridx, self.ROIboundingBoxes[ridx], cur_valid_ROI, search_range) +\
                                     'in image of cropped shape {0} (Original shape: {1}, bounding box margin: {2})'.format(self.GetCroppedShape(), self.MIinput.ImageShape(), self.BoundingBoxMargin), 
                                    fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
                    search_ROIs.append(cur_valid_ROI)
            sf.LogWrite('ROIproc.doDLS() configured spatial crosscorrelation analysis on {0} ROIs ({1} valid, {2} modified)'.format(len(self.ROIboundingBoxes), np.sum(ValidROI), count_ROImodif), 
                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        

        analysis_params = {'General' : {},
                           'Analysis': {
                                        'type' : 'DLS',
                                        'out_folder' : os.path.abspath(saveFolder),
                                        'lagtimes' : lagtimes,
                                        'reftimes' : reftimes,
                                        'no_buffer' : no_buffer,
                                        'force_SLS' : force_SLS,
                                        'save_transposed' : save_transposed,
                                        'include_negative_lags' : include_negative_lags,
                                        'drift_corr' : drift_corr,
                                        'g2m1_averageN' : g2m1_averageN,
                                        'g2m1_reterr' : g2m1_reterr,
                            },}
        if drift_corr>0:
            analysis_params['Analysis']['drift_search_range'] = search_range
        analysis_params['General']['generated_by'] = 'ROIproc.doDLS'
        config_fname = 'ROIprocConfig.ini'
        if export_configparams is not None:
            analysis_params = sf.UpdateDict(analysis_params, export_configparams)
            if 'SALS' in analysis_params['General']['generated_by']:
                config_fname = 'SALSconfig.ini'
        self.ExportConfiguration(saveFolder, other_params=analysis_params, out_fname=config_fname)
        
        sf.LogWrite('ROIproc.doDLS Analysis started! Input data is {0} images ({1} times, {2} exposure times)'.format(self.ImageNumber(), self.NumTimes(), self.NumExpTimes()), 
                    fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        if (self.ImageNumber() != (self.NumTimes() * self.NumExpTimes())):
            sf.LogWrite('WARNING: Number of images ({0}) should correspond to the number of times ({1}) times the number of exposure times ({2})'.format(self.ImageNumber(), 
                            self.NumTimes(), self.NumExpTimes()), fLog=fout, logLevel=logging.WARN, add_prefix='\n'+sf.TimeStr()+' | WARN: ')
        sf.LogWrite('Analysis will resolve {0} ROIs and DLS will be performed on {1} reference times and {2} lagtimes. Output will be saved in folder {3}'.format(self.CountROIs(), 
                        len(DLS_reftimes), DLS_lagnum, saveFolder), fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        sf.LogWrite('Now starting with SLS...', fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
                
        if len(DLS_reftimes)<1:
            sf.LogWrite('ERROR: at least 1 reference time needed for DLS (' + str(len(DLS_reftimes)) + ' given). DLS aborted', 
                        fLog=fout, logLevel=logging.ERROR, add_prefix='\n'+sf.TimeStr()+' | ERROR: ')
            fout.close()
            return None
        
            
        ROIavgs_allExp, ROIavgs_best, BestExptime_Idx, buf_images = self.doSLS(saveFolder, buf_images=None, no_buffer=no_buffer, force_calc=force_SLS)
        
        if buf_images is None:
            if no_buffer:
                self.MIinput.OpenForReading()
                buf_images = None
            elif self.StackInput()==False:
                buf_images = self.ReadMI()

        sf.LogWrite('SLS analysis completed. Now doing DLS ({0} exposure times, {1} time points, {2} lagtimes)'.format(self.NumExpTimes(), len(DLS_reftimes), DLS_lagnum), 
                    fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        
        log_period = 1
        for e in range(self.NumExpTimes()):
            readrange = self.MIinput.Validate_zRange([e, -1, self.NumExpTimes()])
            idx_list = np.arange(*readrange, dtype=int)
            sf.LogWrite('Now performing DLS on {0}-th exposure time. Using image range {1} ({2} images)'.format(e, readrange, len(idx_list)), 
                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
            ISQavg = self.ROIaverageProduct(stack1=idx_list, stack2=idx_list, no_buffer=no_buffer, imgs=buf_images)

            if reftimes=='all' and drift_corr==0:
                cI = np.nan * np.ones((ISQavg.shape[1], ISQavg.shape[0], DLS_lagnum), dtype=float)
                cI[:,:,0] = np.subtract(np.divide(ISQavg, np.square(ROIavgs_allExp[:,e,:])), 1).T
                sf.LogWrite('Contrast (d0) processed', fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
                if (self.LoopMaxInfoN < DLS_lagnum):
                    log_period = int(math.ceil(DLS_lagnum*1.0/self.LoopMaxInfoN))
                    sf.LogWrite('Number of lagtimes ({0}) exceeding {1}: logging every {2}-th lagtime processed'.format(DLS_lagnum, self.LoopMaxInfoN, log_period), 
                                fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                for lidx in range(1, DLS_lagnum):
                    if (DLS_lags[lidx]<ISQavg.shape[0]):

                        IXavg = self.ROIaverageProduct(stack1=idx_list[:-DLS_lags[lidx]], stack2=idx_list[DLS_lags[lidx]:], 
                                                                 no_buffer=no_buffer, imgs=buf_images)
                        # 'classic' cI formula
                        cI[:,:-DLS_lags[lidx],lidx] = np.subtract(np.divide(IXavg, np.multiply(ROIavgs_allExp[:-DLS_lags[lidx],e,:],
                                                                                                   ROIavgs_allExp[DLS_lags[lidx]:,e,:])), 1).T
                        # d0 normalization
                        cI[:,:-DLS_lags[lidx],lidx] = np.divide(cI[:,:-DLS_lags[lidx],lidx], 0.5 * np.add(cI[:,:-DLS_lags[lidx],0], cI[:,DLS_lags[lidx]:,0]))
                        
                        if (lidx % log_period == 0 or lidx == DLS_lagnum-1):
                            sf.LogWrite('Lagtime {0}/{1} (d{2}) completed'.format(lidx, DLS_lagnum-1, DLS_lags[lidx]), 
                                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')


            else:
                # Shape of output cIs: [num_ROIs, num_reftimes, num_lagtimes]
                cI = np.nan * np.ones((ISQavg.shape[1], len(DLS_reftimes), DLS_lagnum), dtype=float)
                if drift_corr>0:
                    cIcr = np.nan * np.ones_like(cI, dtype=float)
                    dxdy = np.nan * np.ones((cI.shape[0], cI.shape[1], cI.shape[2], 2), dtype=float)
                sf.LogWrite('Computing cI with custom-defined set of reference time and/or lag times: result has shape {0} ({1} ROIs, {2} reference times, {3} lag times)'.format(cI.shape, 
                                self.CountROIs(), len(DLS_reftimes), DLS_lagnum), fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')

                if (self.LoopMaxInfoN < len(DLS_reftimes)):
                    log_period = int(math.ceil(len(DLS_reftimes)*1.0/self.LoopMaxInfoN))
                    sf.LogWrite('Number of reference times ({0}) exceeding {1}: logging every {2}-th reference time processed'.format(len(DLS_reftimes), self.LoopMaxInfoN, log_period), 
                                fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')

                # compute all d0s (even if it is not in the list of lagtimes)
                all_d0 = np.subtract(np.divide(ISQavg, np.square(ROIavgs_allExp[:,e,:])), 1).T
                if self.DebugMode:
                    sf.LogWrite('SALS.doDLS - cI.shape : ' + str(cI.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                    sf.LogWrite('SALS.doDLS - ISQavg.shape : ' + str(ISQavg.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                    sf.LogWrite('SALS.doDLS - ROIavgs_allExp.shape : ' + str(ROIavgs_allExp.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')

                for ref_tidx in range(len(DLS_reftimes)):

                    cur_stack2 = []
                    cur_lagtimes = []
                    cur_lagidx = []
                    for lidx in range(len(DLS_lags)):
                        if DLS_lags[lidx]+DLS_reftimes[ref_tidx] >= 0 and DLS_lags[lidx]+DLS_reftimes[ref_tidx] < len(idx_list):
                            cur_stack2.append(DLS_lags[lidx]+DLS_reftimes[ref_tidx])
                            cur_lagtimes.append(DLS_lags[lidx])
                            cur_lagidx.append(lidx)
                    if self.DebugMode:
                        strLog = 'Images to correlate with reference time #{0} (t={1}): '.format(ref_tidx, DLS_reftimes[ref_tidx]) +\
                                str([str(cur_stack2[i]) + ' (d' + str(cur_lagtimes[i]) + ')' for i in range(min(10, len(cur_lagtimes)))])
                        if len(cur_lagtimes) > 10:
                            strLog += ' (' + str(len(cur_lagtimes)-10) + ' more)'
                        sf.LogWrite(strLog, fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                            
                    # 
                    if drift_corr>0:
                        ref_img = self.GetImage(DLS_reftimes[ref_tidx], buffer=buf_images)
                        if self.DebugMode:
                            sf.LogWrite('Reference image shape for drift correction: ' + str(ref_img.shape), 
                                            fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')

                    if len(cur_stack2)>0:

                        IXavg = self.ROIaverageProduct(stack1=[idx_list[DLS_reftimes[ref_tidx]]]*len(cur_stack2), stack2=cur_stack2, 
                                                                 no_buffer=no_buffer, imgs=buf_images)

                        if self.DebugMode:
                            sf.LogWrite('IXavg.shape [num_images={0}, num_ROIs={1}] = {2}'.format(len(cur_stack2), cI.shape[0], IXavg.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                        for lidx in range(len(cur_lagtimes)):

                            cur_tidx2 = DLS_reftimes[ref_tidx]+cur_lagtimes[lidx]
                            if cur_tidx2>=0 and cur_tidx2 < ISQavg.shape[0]:

                                if self.DebugMode:
                                    sf.LogWrite('Correlating reftime {0} (t={1}) and lagtime {2} (d={3}, t={4})'.format(ref_tidx, DLS_reftimes[ref_tidx], 
                                                                                                    cur_lagidx[lidx], cur_lagtimes[lidx], cur_stack2[lidx]), 
                                                fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                if cur_lagtimes[lidx]==0:
                                    cI[:,ref_tidx,cur_lagidx[lidx]] = all_d0[:,ref_tidx]
                                else:
                                    # 'classic' cI formula
                                    if self.DebugMode:
                                        sf.LogWrite('test: IXavg[0,0] = <I(t={0},ROI0)I(t={1},ROI0)> = {2}'.format(idx_list[DLS_reftimes[ref_tidx]], cur_stack2[0], IXavg[0,0]), 
                                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                        sf.LogWrite('ROIavgs_allExp test: ' + str(ROIavgs_allExp[DLS_reftimes[ref_tidx],e,0]), 
                                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                    cI[:,ref_tidx,cur_lagidx[lidx]] = np.subtract(np.divide(IXavg[lidx,:], 
                                                                                np.multiply(ROIavgs_allExp[DLS_reftimes[ref_tidx],e,:],
                                                                                            ROIavgs_allExp[cur_tidx2,e,:])), 1)
                                # d0 normalization
                                if cur_lagidx[lidx]>0:
                                    cI[:,ref_tidx,cur_lagidx[lidx]] = np.divide(cI[:,ref_tidx,cur_lagidx[lidx]], 0.5 * np.add(all_d0[:,DLS_reftimes[ref_tidx]], all_d0[:,cur_tidx2]))
                                
                                if drift_corr>0:
                                    find_img = self.GetImage(cur_stack2[lidx], buffer=buf_images)
                                    if self.DebugMode:
                                        sf.LogWrite('Image shape for drift correction: ' + str(find_img.shape), 
                                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                    for ridx in range(cI.shape[0]):
                                        if ValidROI[ridx]:
                                            xp, yp, corr_peak = FindCrosscorrPeak(find_img, ref_img, SearchRange=search_range, SearchROI=search_ROIs[ridx], 
                                                                                  ValidateInput=False, debugMode=self.DebugMode)
                                            cIcr[ridx,ref_tidx,cur_lagidx[lidx]] = corr_peak * 2. / (all_d0[ridx, DLS_reftimes[ref_tidx]] + all_d0[ridx, cur_tidx2])
                                            dxdy[ridx,ref_tidx,cur_lagidx[lidx]] = (xp, yp)
                                        
                        if (ref_tidx % log_period == 0 or ref_tidx == len(DLS_reftimes)-1):
                            sf.LogWrite('Reference time {0}/{1} (tref={2}) completed'.format(ref_tidx+1, len(DLS_reftimes), DLS_reftimes[ref_tidx]), 
                                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')

                    else:

                        sf.LogWrite('Reference time {0}/{1} (tref={2}) empty'.format(ref_tidx+1, len(DLS_reftimes), DLS_reftimes[ref_tidx]), 
                                    fLog=fout, logLevel=logging.WARN, add_prefix='\n'+sf.TimeStr()+' | WARN: ')



            # Save data to file
            for ridx in range(cI.shape[0]):
                sf.LogWrite('Now saving ROI {0} to file'.format(ridx), fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
                if save_transposed:
                    np.savetxt(os.path.join(saveFolder, 'cI_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), cI[ridx,:,1:].T, axis=1), 
                               header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)    
                    if drift_corr>0 and ValidROI[ridx]:
                        np.savetxt(os.path.join(saveFolder, 'cIcr_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), cIcr[ridx,:,1:].T, axis=1), 
                                   header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}_cr'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dx_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), dxdy[ridx,:,1:,0].T, axis=1), 
                                   header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}_dx'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dy_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), dxdy[ridx,:,1:,1].T, axis=1), 
                                   header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}_dy'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)                  
                else:
                    first_cols = np.append(idx_list[DLS_reftimes].reshape((-1, 1)), self.imgTimes[idx_list[DLS_reftimes]].reshape((-1, 1)), axis=1)
                    np.savetxt(os.path.join(saveFolder, 'cI_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, cI[ridx], axis=1), 
                               header='idx'+self.txt_delim+'t'+self.txt_delim+'d0_raw'+self.txt_delim + str(self.txt_delim).join(['d{0}'.format(l) for l in DLS_lags[1:]]), **self.savetxt_kwargs)
                    if drift_corr>0 and ValidROI[ridx]:
                        np.savetxt(os.path.join(saveFolder, 'cIcr_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, cIcr[ridx], axis=1), 
                                   header='idx'+self.txt_delim+'t'+self.txt_delim + str(self.txt_delim).join(['d{0}_cr'.format(l) for l in DLS_lags]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dx_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, dxdy[ridx,:,:,0], axis=1), 
                                   header='idx'+self.txt_delim+'t'+self.txt_delim + str(self.txt_delim).join(['d{0}_dx'.format(l) for l in DLS_lags]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dy_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, dxdy[ridx,:,:,1], axis=1), 
                                   header='idx'+self.txt_delim+'t'+self.txt_delim + str(self.txt_delim).join(['d{0}_dy'.format(l) for l in DLS_lags]), **self.savetxt_kwargs)                  
                    
        if save_transposed:
            sf.LogWrite('DLS analysis completed. Transposed result saved (no g2-1 function will be averaged).', 
                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        else:
            sf.LogWrite('DLS analysis completed. Now averaging correlation functions g2-1', 
                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
            self.AverageG2M1(saveFolder, avg_interval=g2m1_averageN, save_stderr=g2m1_reterr)
        fout.close()
            
    def AverageG2M1(self, folder_path, avg_interval=None, search_prefix=['cI_','cIcr_','dx_','dy_'], save_prefix=['g2m1','g2m1cr','avgdx','avgdy'], save_stderr=False, sharp_bound=False, lag_tolerance=1e-2, lag_tolerance_isrelative=True):
        for i in range(len(search_prefix)):
            tres_fnames = sf.FindFileNames(folder_path, Prefix=search_prefix[i], Ext='.dat')
            for cur_f in tres_fnames:
                AverageG2M1(os.path.join(folder_path, cur_f), avg_interval=avg_interval, save_prefix=save_prefix[i], 
                            cut_prefix_len=len(search_prefix[i])-1, delimiter=self.txt_delim, comment=self.txt_comm, save_stderr=save_stderr, sharp_bound=sharp_bound, lag_tolerance=lag_tolerance, lag_tolerance_isrelative=lag_tolerance_isrelative)
            
    def ExportROIs(self, outFolder):
        """
        Exports ROIs:
        - ROIcoords.dat: text file with ROI coordinates and areas
        - ROI_mask.raw: integer mask with indices of the ROI every pixel belongs to. NOTE: no ROI overlap supported
        """
        sf.CheckCreateFolder(outFolder)
        ROIhdr_str = self.txt_delim.join(self.ROIcoord_names+[name+'_err' for name in self.ROIcoord_names]+['norm', 'min_row[bb_base:' + str(self.BoundingBox[0]) + ']', 'min_col[bb_base:' + str(self.BoundingBox[1]) + ']', 'max_row', 'max_col'])
        roi_norms = np.expand_dims(self.ROI_maskSizes, axis=1)
        np.savetxt(os.path.join(outFolder, 'ROIcoords.dat'), np.append(np.append(self.ROIcoords, roi_norms, axis=1), 
                               self.ROIboundingBoxes, axis=1), header=ROIhdr_str, **self.savetxt_kwargs)
        MI.WriteBinary(os.path.join(outFolder, 'ROI_mask.raw'), self.ROIs, 'i')
                
    def ExportConfiguration(self, outFolder, other_params=None, out_fname=None):

        if out_fname is None:
            out_fname = 'ROIprocConfig.ini'
        sf.CheckCreateFolder(outFolder)
        self.ExportROIs(outFolder)
        ini_fpath = os.path.join(outFolder, out_fname)
        np.savetxt(os.path.join(outFolder, 'imgTimes.dat'), self.imgTimes)
        
        my_params = {'General': {'version' : '2.0',
                                 'generated_by' : 'ROIproc.ExportConfiguration',
                                 'generated_on' : sf.NowToStr(),
                                 'folder' : os.path.abspath(outFolder),
                                },
                       'MIfile' : self.MIinput.GetMetadata(),
                       'ROIs' : {'number' : self.CountROIs(),
                                 'box' : list(self.BoundingBox),
                                 'box_margin' : self.BoundingBoxMargin,
                                 'coord_file' : 'ROIcoords.dat',
                                 'mask_file' : 'ROI_mask.raw',
                                },
                       'ImgTimes' : {'number' : len(self.imgTimes),
                                   'file' : 'imgTimes.dat',
                                   'usecol' : 0,
                                   'skiprow' : 0,
                                },
                       'ExpTimes' : {'values' : self.expTimes,
                                },
                       'Constants' : {'max_avg_intensity' : self.MaxSafeAvgIntensity,
                                      'dt_tolerance' : self.dt_tolerance,
                                      'dt_tolerance_isrelative' : self.dt_tolerance_isrelative,
                                      'txt_delim' : self.txt_delim,
                                      'txt_comm' : self.txt_comm,
                                       },
                       }
        if other_params is not None:
            my_params = sf.UpdateDict(my_params, other_params)
        if self.MIinput.IsStack():
            my_params['MIfile']['is_stack'] = True
            my_params['MIfile']['stack_type'] = self.MIinput.StackType
        else:
            my_params['MIfile']['is_stack'] = False
        my_params['MIfile']['filename'] = self.MIinput.GetFilename(absPath=True)
        cf.ExportDict(my_params, ini_fpath)
    
    def RunFromConfig(self, ConfigParams, AnalysisSection='Analysis', OutputSubfolder='reproc', export_configparams=None):
        """Runs an analysis using parameters from a configuration file or a Config object
        
        Parameters
        ----------
        ConfigParams     : full path of the config file to read or dict or Config object
        AnalysisSection  : str, label of the section with analysis parameters
                           keys required:
                           - 'type': type of analysis to perform (supported types: {'DLS'})
                           keys for DLS analysis
                           - 'out_folder': output folder
                           - 'lagtimes': ('all' or list of int. Default: 'all')
                           - 'reftimes': ('all' or list of int. Default: 'all')
                           - 'no_buffer' (bool, default: False)
                           - 'force_SLS' (bool, default: True)
                           - 'drift_corr' (int, default: 0)
                           - 'save_transposed' (bool, default: False)
                           - 'include_negative_lags' (bool, default: False)
        OutputSubfolder  : str, subfolder to use as analysis output. 
                           If None, the out_folder parameter will be used as output folder
                           otherwise, data will be saved in a subfolder of the specified output folder
        export_configparams : None or dict with additional configuration parameters to be exported to the output configuration file
                           
        """
        config = cf.LoadConfig(ConfigParams)
        folder_root = config.Get('General', 'folder', None, str)
        
        if config.HasSection(AnalysisSection):

            an_type = config.Get(AnalysisSection, 'type','UNKNOWN', str)
            logging.info('ROIproc.RunFromConfig running {0} analysis'.format(an_type))

            if an_type=='DLS':
                out_folder = sf.GetAbsolutePath(config.Get(AnalysisSection, 'out_folder', '', str), root_path=folder_root)
                if config.Get(AnalysisSection, 'lagtimes', 'all', str)=='all':
                    lagtimes = 'all'
                else:
                    lagtimes = config.Get(AnalysisSection, 'lagtimes', [0], int)
                if config.Get(AnalysisSection, 'reftimes', 'all', str)=='all':
                    reftimes = 'all'
                else:
                    reftimes = config.Get(AnalysisSection, 'reftimes', [0], int)
                no_buffer = config.Get(AnalysisSection, 'no_buffer', False, bool)
                force_SLS = config.Get(AnalysisSection, 'force_SLS', True, bool)
                drift_corr = config.Get(AnalysisSection, 'drift_corr', 0, int)
                g2m1_averageN = config.Get(AnalysisSection, 'g2m1_averageN', 0, int)
                g2m1_reterr = config.Get(AnalysisSection, 'g2m1_reterr', False, bool)
                save_transposed = config.Get(AnalysisSection, 'save_transposed', False, bool)
                include_negative_lags = config.Get(AnalysisSection, 'include_negative_lags', False, bool)
                if OutputSubfolder is None:
                    new_out_folder = out_folder
                else:
                    new_out_folder = os.path.join(out_folder, OutputSubfolder)
                export_configparams = sf.UpdateDict(export_configparams, {'Analysis': {'out_folder' : new_out_folder}})
                if 'General' not in export_configparams:
                    export_configparams['General'] = {'generated_by' : 'ROIproc.RunFromConfig(DLS)'}
                self.doDLS(new_out_folder, lagtimes=lagtimes, reftimes=reftimes, drift_corr=drift_corr, no_buffer=no_buffer, 
                               force_SLS=force_SLS, save_transposed=save_transposed, include_negative_lags=include_negative_lags, 
                               export_configparams=export_configparams, g2m1_averageN=g2m1_averageN, g2m1_reterr=g2m1_reterr)
                logging.info('DLS analysis run and saved to folder {0}'.format(new_out_folder))

            if an_type=='SLS':
                out_folder = sf.GetAbsolutePath(config.Get(AnalysisSection, 'out_folder', '', str), root_path=folder_root)
                no_buffer = config.Get(AnalysisSection, 'no_buffer', False, bool)
                if OutputSubfolder is None:
                    new_out_folder = out_folder
                else:
                    new_out_folder = os.path.join(out_folder, OutputSubfolder)
                export_configparams = sf.UpdateDict(export_configparams, {'Analysis': {'out_folder' : new_out_folder}})
                if 'General' not in export_configparams:
                    export_configparams['General'] = {'generated_by' : 'ROIproc.RunFromConfig(SLS)'}
                self.doSLS(new_out_folder, no_buffer=no_buffer, force_calc=True)
                logging.info('SLS analysis run and saved to folder {0}'.format(new_out_folder))            
            else:
                logging.warn('ROIproc.RunFromConfig ERROR: unknown analysis type {0}'.format(an_type))
        else:
            logging.warn('ROIproc.RunFromConfig ERROR: analysis section {0} not found'.format(AnalysisSection))
