import os
import logging
import bisect
import collections
import math
import numpy as np
from multiprocessing import Process

from DSH import Config as cf
from DSH import MIfile as MI
from DSH import MIstack as MIs
from DSH import SharedFunctions as sf
from DSH import IOfunctions as iof

def MaskCoordsFromBoundaries(c0_list, c1_list=None, flatten_res=True):
    """Generate a list of polar masks coordinates, each mask of the form [c0, c1, dc0, dc1]
    
    Parameters
    ----------
    c0_list    : list of first coordinate points (for adjacent masks) or list of couples [c0_min, c0_max]
    c1_list    : list of second coordinate points or list of couples [c1_min, c1_max].
    flatten_res: if True, flatten the (c0, c1) dimensions into a single list.
                otherwise, return a 3D array with separate c0 and c1 axes
    
    Returns
    -------
    mSpecs: 2D or 3D array, depending on flatten_res. Last dimension is [c0, c1, dc0, dc1]
    """
    c_list = [c0_list, c1_list]
    for j in range(2):
        if (not sf.IsIterable(c_list[j][0])):
            tmp_list = []
            for i in range(len(c_list[j])-1):
                tmp_list.append([c_list[j][i], c_list[j][i+1]])
            c_list[j] = tmp_list
    mSpecs = np.empty((len(c_list[0]), len(c_list[1]), 4), dtype=float)
    for i in range(mSpecs.shape[0]):
        for j in range(mSpecs.shape[1]):
            mSpecs[i, j] = [0.5*(c_list[0][i][0] + c_list[0][i][1]),\
                            0.5*(c_list[1][j][0] + c_list[1][j][1]),\
                            c_list[0][i][1] - c_list[0][i][0],\
                            c_list[1][j][1] - c_list[1][j][0]]
    logging.debug('MaskCoords created with\n\t- {0} first coords from {1:.2f} (+- {2:.2f}) to {3:.2f} (+- {4:.2f}) '.format(len(c_list[0]), mSpecs[0,0,0], 0.5*mSpecs[0,0,2], mSpecs[-1,0,0], 0.5*mSpecs[-1,0,2]) +
                  'and\n\t- {0} second coords from {1} (+- {2:.2f}) to {3:.2f} (+- {4:.2f})'.format(len(c_list[1]), mSpecs[0,0,1], 0.5*mSpecs[0,0,3], mSpecs[0,-1,1], 0.5*mSpecs[0,-1,3]))
    if flatten_res:
        return mSpecs.reshape(-1, mSpecs.shape[-1])
    else:
        return mSpecs

def GenerateMasks(coords, grid, common_mask=None):
    """Generate a list of regions of interest, labelled in the form of binary images, 
    each one with 0s everywhere and 1 inside the region of interest
    
    Parameters
    ----------
    coords       : list of mask coordinates in the form 
                    - [r, a, dr, da], if coordsystem=='polar'
                    - [x, y, dx, dy], if coordsystem=='cartesian'
    grid         : grid of coordinates, as generated by GenerateGrid2D
    common_mask  : eventually specify common mask to be multiplied to every mask
    
    Returns
    -------
    if binary_res: a 3D array, one page per binary images (mask)
    """
    if common_mask is None:
        common_mask = np.ones_like(grid[0], dtype=int)
        
    res = np.empty((len(coords), grid[0].shape[0], grid[0].shape[1]), dtype=np.dtype('b'))
    for m_idx in range(len(coords)):
        x0min, x0max = coords[m_idx][0]-0.5*coords[m_idx][2], coords[m_idx][0]+0.5*coords[m_idx][2]
        x1min, x1max = coords[m_idx][1]-0.5*coords[m_idx][3], coords[m_idx][1]+0.5*coords[m_idx][3]      
        res[m_idx] = np.where(np.logical_and(common_mask>0,
                                             np.logical_and(np.logical_and(grid[0]>=x0min, grid[0]<x0max),
                                                            np.logical_and(grid[1]>=x1min, grid[1]<x1max))),
                              1, 0)
    
    logging.debug('{0} binary masks created with shape {1}'.format(len(coords), grid[0].shape))
    return res

def GenerateROIgrid(px_coords, GridShape, coord_limits=None, common_mask=None):
    """
    Generate a list of ROIs tiling the entire space
    
    Parameters
    ----------
    px_coords :   pixel coordinates, as generated by SharedFunctions.PixelCoordGrid
    GridShape :   (M, N) int couple: number of ROIs across each coordinate axis
    coord_limits: [[min_x0, max_x0], [min_x1, max_x1]]
                  eventually restrict the tiling to a subset of the entire space
                  defined by its boundaries along each axis. If None, full axis span will be taken
    
    Returns:
    --------
    roi_masks :   3D binary array with ROI masks
    roi_coords:   list of coordinates in the form [c0, c1, dc0, dc1] as accepted by GenerateMasks
    """
    
    if common_mask is None:
        common_mask = np.ones_like(px_coords[0], dtype=int)
    if coord_limits is None:
        coord_limits = [None, None]
    ax_marks = [None, None]
    for i in range(2):
        if coord_limits[i] is None:
            coord_limits[i] = [np.min(px_coords[i]), np.max(px_coords[i])]
        ax_marks[i] = np.linspace(coord_limits[i][0], coord_limits[i][1], num=GridShape[i]+1, endpoint=True)
    
    roi_masks = np.zeros((np.prod(GridShape), px_coords[0].shape[0], px_coords[0].shape[1]), dtype=np.dtype('b'))
    roi_coords = []
    for i in range(GridShape[0]):
        for j in range(GridShape[1]):
            roi_masks[i*GridShape[1]+j] = np.where(np.logical_and(common_mask>0,
                                                       np.logical_and(np.logical_and(px_coords[0]>=ax_marks[0][i], px_coords[0]<ax_marks[0][i+1]),
                                                                      np.logical_and(px_coords[1]>=ax_marks[1][j], px_coords[1]<ax_marks[1][j+1]))),
                                        1, 0)
            roi_coords.append([0.5*(ax_marks[0][i]+ax_marks[0][i+1]), 0.5*(ax_marks[1][j]+ax_marks[1][j+1]), ax_marks[0][i+1]-ax_marks[0][i], ax_marks[1][j+1]-ax_marks[1][j]])
    
    logging.debug('{0} binary masks created with shape {1}'.format(len(roi_coords), px_coords[0].shape))
    return roi_masks, np.asarray(roi_coords)
    
def BinaryToIntegerMask(BinaryMasks):
    """
    Converts a list of binary ROI masks (1 inside the ROI, 0 elsewhere) to an integer mask with ROI index
    
    Parameters
    ----------
    BinaryMasks : list of 2D binary masks, one for every ROI
                  NOTE: no overlap between ROIs is assumed. 
                  In case of overlap, the last ROI will overwrite the first ones
                      
    Returns
    -------
    intMask :     2D integer mask, reporting the ROI index every pixel belongs to
                  pixels belonging to no ROI will be set -1
    """
    res = -1 * np.ones_like(BinaryMasks[0])
    for i in range(len(BinaryMasks)):
        res = np.where(BinaryMasks[i], i, res)
    return res

def FindBoundingBoxROI(ROImasks, margin=0):
    """
    Computes the smallest box enclosing all ROIs
    
    Parameters
    ----------
    ROImasks : 2D or 3D binary mask. if 3D, masks will be overlapped to find the global binding box
    margin : eventually add a margin, in pixel, to all sides of the bounding box

    Returns
    -------
    BBox : [min_row, min_col, max_row+1, max_col+1]
    """
    if ROImasks.ndim>2:
        use_mask = np.sum(ROImasks, axis=0)
    else:
        use_mask = ROImasks
    nonzero_cols = np.nonzero(np.sum(use_mask, axis=0))
    nonzero_rows = np.nonzero(np.sum(use_mask, axis=1))
    return np.asarray([max(0, np.min(nonzero_rows)-margin), 
                       max(0, np.min(nonzero_cols)-margin), 
                       min(use_mask.shape[0], np.max(nonzero_rows)+margin+1), 
                       min(use_mask.shape[1], np.max(nonzero_cols)+margin+1)])

def ROIAverage(image, ROImask, boolMask=False, weights=None, norm=None, masknans=False, BoundingBoxes=None, dtype=float, evalFunc=None, evalParams={}, debug=False):
    """
    Calculate the average value of an image on a series of ROIs.

    Parameters
    ----------
    image        - 2D image or 3D ndarray with list of images
    ROImask      - if boolMask : list of 2D bool arrays.
                          if BoundingBoxes is specified, ROImask[i] should have a size determined by BoundingBoxes[i] 
                          otherwise, each ROImask should have the same size as the input image(s)
                   else : 2D int array with same size as the input image(s), with each pixel labeled with the index of the
                          ROI it belongs to (0 based). Each pixel can only belong to one ROI.
                          Pixels not belonging to any ROI must be labeled with -1
                   It can be none if BoundingBoxes is specified. In this case, 
                   bounding boxes themselves will be used as ROIs
    boolMask     - if True: interpret ROImask as a list of 2D bool arrays
                 - if False: interpret ROImask as a single 2D int array
    weights      - can do a weighted average instead of a simple average if this keyword parameter
                   is set.  weights.shape must = image.shape.
    norm         - Normalization factors. Equals the number of pixel belonging to each ROI if average is not weighted
                   If None (default), it is computed from ROImask
    masknans     - assume the presence of NaNs in the array: mask them and don't count them in
                   the normalization by setting their weight to zero. Set it to False if you are
                   sure that there are no NaNs to improve calculation speed
    BoundingBoxes- None, or list of bounding boxes, as many as the number of ROIs
                   Each bounding box is a 4-element array of type [min_row, min_col, max_row+1, max_col+1]
    dtype        - Datatype of the accumulator in which the elements are summed. 
                   If the accumulator is too small, np.sum generates overflow
    evalFunc     - if None, simple average will be computed.
                   Otherwise, what will be averaged will be a function of the pixel values 
                   ex: to compute the variance, use SquareDistFromMean()
    evalParams   - eventually specify additional parameters for evalFunc

    Returns
    -------
    ROI_avg   : if image is 2D: 1D float array with ROI-averaged data
                if image is 3D: 2D float array, one image per row, one ROI per column.
                If a bin contains NO DATA, it will have a NAN value because of the
                divide-by-sum-of-weights component.
    norm      : sum of weights within the ROI. If weights==None, this reduces to the number of pixels in the ROI
    """
    
    
    if image.shape[0]==0:
        return None, None
    
    rbb = BoundingBoxes
    
    if ROImask is None:
        if BoundingBoxes is None:
            raise ValueError('either ROImask or BoundingBoxes must be specified!')
        else:
            nbins = len(BoundingBoxes)
            ROIboolMask = None
    else:
        if boolMask:
            nbins = len(ROImask)
            ROIboolMask = ROImask
        else:
            nbins = np.max(ROImask)+1
            if BoundingBoxes is None:
                ROIboolMask = [ROImask==b for b in range(nbins)]
            else:
                ROIboolMask = [ROImask[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]]==b for b in range(nbins)]
        
    use_weights = (weights is not None or masknans)
    if weights is None and use_weights:
        if (image.ndim > 2):
            weights = np.ones_like(image[0])
        else:
            weights = np.ones_like(image)
    if masknans:
        weights = np.multiply(weights, ~np.isnan(image))
    if use_weights:
        use_img = np.multiply(image, weights)
        if norm is None:
            # normalization factor for each bin
            if (weights.ndim > 2):
                if BoundingBoxes is None:
                    norm = np.array([[np.sum(np.multiply(weights[i], ROIboolMask[b])) for b in range(nbins)] for i in range(weights.shape[0])])
                else:
                    if ROIboolMask is None:
                        norm = np.array([[np.sum(weights[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]]) for b in range(nbins)] for i in range(weights.shape[0])])
                    else:
                        norm = np.array([[np.sum(np.multiply(weights[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b])) for b in range(nbins)] for i in range(weights.shape[0])])
            else:
                if BoundingBoxes is None:
                    norm = np.array([np.sum(np.multiply(weights, ROIboolMask[b])) for b in range(nbins)])
                else:
                    if ROIboolMask is None:
                        norm = np.array([np.sum(weights[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]]) for b in range(nbins)])
                    else:
                        norm = np.array([np.sum(np.multiply(weights[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b])) for b in range(nbins)])
                if (use_img.ndim > 2):
                    norm = [norm] * use_img.shape[0]
    else:
        use_img = image
        if norm is None:
            if BoundingBoxes is None:
                norm = np.array([np.sum(ROIboolMask[b]) for b in range(nbins)])
            else:
                if ROIboolMask is None:
                    norm = np.array([(rbb[b][2]-rbb[b][0])*(rbb[b][3]-rbb[b][1]) for b in range(nbins)])
                else:
                    norm = np.array([np.sum(ROIboolMask[b]) for b in range(nbins)])
        if (norm.ndim==1 and use_img.ndim > 2):
            norm = np.asarray([norm] * use_img.shape[0])
        
    if debug:
        logging.debug('  ROIAverage function called with {0}D input of shape {1}'.format(use_img.ndim, use_img.shape))
        logging.debug('  Normalization has shape {0}, factors range from {1} to {2}'.format(norm.shape, np.min(norm), np.max(norm)))
        if use_weights:
            if weights is None:
                logging.warn('  WARNING: use_weights is True but weights is None!')
            else:
                logging.debug('  Weight image. Weights has shape {0} and range from {1} to {2}'.format(weights.shape, np.min(weights), np.max(weights)))
            strprint = '  Weighted image'
        else:
            strprint = '  No weighting. Original image'
        logging.debug(strprint + ' has shape {0} and ranges from {1} to {2}'.format(use_img.shape, np.min(use_img), np.max(use_img)))
        if BoundingBoxes is not None:
            logging.debug('  Using bounding boxes (len: {0})'.format(len(rbb)))
            
    
    if (use_img.ndim > 2):
        if evalFunc==None:
            if BoundingBoxes is None:
                ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(use_img[i], ROIboolMask[b]), dtype=dtype), norm[i][b]) 
                                     for b in range(nbins)] for i in range(use_img.shape[0])])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([[np.true_divide(np.sum(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])
                else:
                    ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b]), 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])
        else:
            if BoundingBoxes is None:
                ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(evalFunc(use_img[i], **evalParams), ROIboolMask[b]), dtype=dtype), norm[i][b]) 
                                     for b in range(nbins)] for i in range(use_img.shape[0])])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([[np.true_divide(np.sum(evalFunc(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])                    
                else:
                    ROI_avg = np.array([[np.true_divide(np.sum(np.multiply(evalFunc(use_img[i][rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), ROIboolMask[b]), 
                                                               dtype=dtype), norm[i][b]) for b in range(nbins)] for i in range(use_img.shape[0])])
                
    else:
        if evalFunc==None:
            if BoundingBoxes is None:
                ROI_avg = np.array([np.true_divide(np.sum(np.multiply(use_img, ROIboolMask[b]), dtype=dtype), norm[b]) for b in range(nbins)])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([np.true_divide(np.sum(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])                    
                else:
                    ROI_avg = np.array([np.true_divide(np.sum(np.multiply(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], ROIboolMask[b]), 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])
        else:
            if BoundingBoxes is None:
                ROI_avg = np.array([np.true_divide(np.sum(np.multiply(evalFunc(use_img, **evalParams), ROIboolMask[b]), dtype=dtype),
                                                   norm[b]) for b in range(nbins)])
            else:
                if ROIboolMask is None:
                    ROI_avg = np.array([np.true_divide(np.sum(evalFunc(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])                    
                else:
                    ROI_avg = np.array([np.true_divide(np.sum(np.multiply(evalFunc(use_img[rbb[b][0]:rbb[b][2],rbb[b][1]:rbb[b][3]], **evalParams), ROIboolMask[b]), 
                                                              dtype=dtype), norm[b]) for b in range(nbins)])
                
    
    return ROI_avg, norm

def ROIEval(image, ROImask, evalFuncs, evalParams={}):
    """
    Evaluate a function or a list of functions on image values restricted to ROIs.

    Parameters
    ----------
    image        - The 2D image
    ROImask      - 2D int array with same size as image, with each pixel labeled with the index of the
                   ROI it belongs to (0 based). Each pixel can only belong to one ROI.
                   Pixels not belonging to any ROI must be labeled with -1
    evalFuncs    - Function or list of functions to be evaluated. First argument should be array-like (it will be pixel values)
                   return value can be anything (does not need to be numeric)
    evalParams   - Dict or list of dicts, eventually specify additional parameters for evalFunc

    Returns
    -------
    ROIres   :  if single function is given: list of results, one item per ROI
                if multiple functions : list of lists of results
                                        ROIres[i][j] will be evaluation of i-th function on j-th ROI
    """
    if sf.IsIterable(evalFuncs):
        flatten_res = False
    else:
        evalFuncs = [evalFuncs]
        flatten_res = True
    evalParams = sf.CheckIterableVariable(evalParams, len(evalFuncs), force_length=True)
    ROIres = [[f(image[ROImask==b], **evalParams[i]) for b in range(np.max(ROImask))] for i, f in enumerate(evalFuncs)]
    if flatten_res:
        return ROIres[0]
    else:
        return ROIres
    
def LoadImageTimes(img_times_source, usecols=0, skiprows=1, root_folder=None, default_value=None, return_unique=False):
    '''
    Load image times from file or list of files
    '''
    return iof.LoadImageTimes(img_times_source, usecols=usecols, skiprows=skiprows, root_folder=root_folder, return_unique=return_unique)

def AverageG2M1_multi(cI_file_list, avg_interval=None, save_fname=None, save_prefix='g2m1', cut_prefix_len=2, delimiter='\t', comment='#', save_stderr=False, 
                      sharp_bound=False, lag_tolerance=1e-2, lag_tolerance_isrelative=True, imgTimes=None, num_threads=1):
    for pool in range(len(cI_file_list)//num_threads):
        procs = []
        min_idx, max_idx = pool*num_threads, min(len(cI_file_list), (pool+1)*num_threads)
        for k in range(min_idx, max_idx):
            p = Process(target=AverageG2M1, args=(cI_file_list[k], avg_interval, save_fname, save_prefix, cut_prefix_len, delimiter, comment, save_stderr, sharp_bound, lag_tolerance, lag_tolerance_isrelative, imgTimes))
            p.start()
            procs.append(p)
        for p in procs:
            p.join()


    
def AverageG2M1(cI_file, avg_interval=None, save_fname=None, save_prefix='g2m1', cut_prefix_len=2, delimiter='\t', comment='#', save_stderr=False, 
                sharp_bound=False, lag_tolerance=1e-2, lag_tolerance_isrelative=True, imgTimes=None):
    '''
    Load cI-like file and average g2m1. 
    '''
    
    cur_cI, cur_times, cur_lagidx_list = iof.ReadCIfile(cI_file)
    if imgTimes is None:
        imgTimes = cur_times
        ref_indexes = None
    else:
        if type(imgTimes) in [str]:
            imgTimes = np.loadtxt(imgTimes)
        ref_indexes = np.asarray([np.argmin(np.abs(imgTimes-curt)) for curt in cur_times])
    
    g2m1_avg = AverageCorrTimetrace(cur_cI, imgTimes, cur_lagidx_list, avg_interval=avg_interval, img_indexes=ref_indexes, return_stderr=True, sharp_bound=sharp_bound, lag_tolerance=lag_tolerance, lag_tolerance_isrelative=lag_tolerance_isrelative)
    g2m1, g2m1_lags = g2m1_avg[0], g2m1_avg[1]
    
    if sharp_bound:
        int_suffix = '*'
    else:
        int_suffix = ''
    int_bounds = sf.ValidateAverageInterval(avg_interval, num_datapoints=cur_cI.shape[0])
    
    if save_stderr:
        g2m1_err = g2m1_avg[2]
        str_hdr_g = str(delimiter).join(['dt'+delimiter+'t{0:.2f}[{1}:{2}]'.format(cur_times[int((int_bounds[tavgidx][0]+int_bounds[tavgidx][1])/2)], int_bounds[tavgidx][0], int_bounds[tavgidx][1])+int_suffix+'_avg'+
                                              delimiter+'t{0:.2f}[{1}:{2}]'.format(cur_times[int((int_bounds[tavgidx][0]+int_bounds[tavgidx][1])/2)], int_bounds[tavgidx][0], int_bounds[tavgidx][1])+int_suffix+'_err'
                                         for tavgidx in range(g2m1.shape[0])])
        g2m1_out = np.empty((g2m1.shape[1], 3*g2m1.shape[0]), dtype=float)
        g2m1_out[:,0::3] = g2m1_lags.T
        g2m1_out[:,1::3] = g2m1.T
        g2m1_out[:,2::3] = g2m1_err.T
    else:
        str_hdr_g = str(delimiter).join(['dt'+delimiter+'t{0:.2f}[{1}:{2}]'.format(cur_times[int((int_bounds[tavgidx][0]+int_bounds[tavgidx][1])/2)], int_bounds[tavgidx][0], int_bounds[tavgidx][1])+int_suffix for tavgidx in range(g2m1.shape[0])])
        g2m1_out = np.empty((g2m1.shape[1], 2*g2m1.shape[0]), dtype=float)
        g2m1_out[:,0::2] = g2m1_lags.T
        g2m1_out[:,1::2] = g2m1.T
    
    if save_fname is None:
        save_fname = save_prefix + sf.GetFilenameFromCompletePath(cI_file)[cut_prefix_len:]
    
    np.savetxt(os.path.join(os.path.dirname(cI_file), save_fname), 
           g2m1_out, header=str_hdr_g, delimiter=delimiter, comments=comment)

def AverageCorrTimetrace(CorrData, ImageTimes, Lagtimes_idxlist, avg_interval=None, img_indexes=None, return_stderr=False, sharp_bound=False, lag_tolerance=1e-2, lag_tolerance_isrelative=True, verbose=0):
    '''
    Average correlation timetraces
    
    Parameters
    ----------
    - CorrData: 2D array. Element [i,j] is correlation between t[img_indexes[i]] and t[img_indexes[i]]+tau[j]
    - ImageTimes: 1D array, float. i-th element is the physical time at which img_indexes[i]-th image was taken
    - Lagtimes_idxlist: 1D array, int. i-th element is the lagtime, in image units
                 NOTE: they must be positive. Not yet compatible with negative time lags
    - avg_interval: None, int, single interval=[min_idx, max_idx, [step_idx=1]] or list of intervals [interval1, ..., intervalN]. 
                 if None or int<=0, result will be averaged on the entire stack
                 if int>0, resolve the average on consecutive chunks of avg_interval images each
                 if interval=[min_idx, max_idx, [step_idx=1]], average will be done in the specified interval
                 if [interval1, ..., intervalN], a list of intervals will be computed as above
                 WARNING: avg_interval is based on CorrData rows. If CorrData was computed using a subset of reference times,
                 this may lead to non-uniform averaging intervals
    - img_indexes: None or 1D array, int. Index of reference image relative to i-th row of CorrData array.
                 if None (default), img_indexes=[0,1,...,CorrData.shape[0]-1]
                 Specify it when CorrData was computed using a subset of reference times
    - return_stderr: bool. If True, return standard deviation of the correlation points averaged to obtain the g2-1
    - sharp_bound: bool. 
                 if False, use interval bounds to associate any reference time to the corresponding interval, 
                           but average on all time lags, including those for which ref_time + time_lag >= max_idx
                 if True, for each reference time, restrict average such that both correlated timepoints are in the specified interval
    - lag_tolerance : average together time lags closer than lag_tolerance
    - lag_tolerance_isrelative : if False, time lags lag[i] and lag[j] will be averaged together if |lag[i] - lag[j]| < lag_tolerance
                                 if True, the criterion will be that |lag[i] - lag[j]| < lag_tolerance * |lag[i] + lag[j]| / 2
    
    Returns
    -------
    - g2m1: 2D array. Element [i,j] represents j-th lag time and i-th time-resolved chunk
    - g2m1_lags: 2D array. It contains the time delays, in physical units, of the respective correlation data
    - g2m1_stderr: 2D array. Element [i,j] represents the standard deviation of the data averaged to obtain g2m1[i,j]
                    Only returned if return_stderr==True
    '''
    if (img_indexes is None):
        img_indexes = np.arange(CorrData.shape[0], dtype=int)    
        
    int_bounds = sf.ValidateAverageInterval(avg_interval, num_datapoints=CorrData.shape[0])

    if lag_tolerance_isrelative:
        strmsg = '{0:.1f}% relative tolerance'.format(lag_tolerance*100)
    else:
        strmsg = 'an absolute tolerance of {0} time units'.format(lag_tolerance)

    start_imtime = np.min([b[0] for b in int_bounds])
    end_imtime = min(len(ImageTimes), np.max(Lagtimes_idxlist)+np.max([b[1] for b in int_bounds]))
    if sf.AllWithinTolerance(np.diff(ImageTimes[start_imtime:end_imtime]), lag_tolerance, lag_tolerance_isrelative):

        tdiff = np.nanmean(np.diff(ImageTimes[start_imtime:end_imtime]))
        g2m1_lags = np.multiply(Lagtimes_idxlist, tdiff)

        logging.debug('Acquisition at constant rate within ' + strmsg + '. Average time between frames: {0} time units'.format(tdiff))

        g2m1 = np.zeros((len(int_bounds), len(Lagtimes_idxlist)), dtype=float)
        g2m1_err = np.zeros_like(g2m1, dtype=float)
        for cur_tavg_idx in range(len(int_bounds)):
            for lidx in range(CorrData.shape[1]): # all available lagtimes in input correlation matrix
                cur_tmin, cur_tmax = int_bounds[cur_tavg_idx][0], int_bounds[cur_tavg_idx][1]
                if len(int_bounds[cur_tavg_idx])>2:
                    cur_tstep = int_bounds[cur_tavg_idx][2]
                else:
                    cur_tstep = 1
                if sharp_bound:
                    cur_tmax = max(cur_tmin, int_bounds[cur_tavg_idx][1]-Lagtimes_idxlist[lidx])
                if (cur_tmax > cur_tmin):
                    avg_slice = slice(cur_tmin, cur_tmax, cur_tstep)
                    g2m1[cur_tavg_idx,lidx] = np.nanmean(CorrData[avg_slice,lidx])
                    g2m1_err[cur_tavg_idx,lidx] = np.nanstd(CorrData[avg_slice,lidx])
                elif verbose:
                    logging.debug('Lag index {0} (d{1}) skipped because of sharp bound {2}'.format(lidx, Lagtimes_idxlist[lidx], int_bounds[cur_tavg_idx]))

    else:

        logging.debug('Acquisition rate was not constant within ' + strmsg + '. Running generalized procedure')
            
        g2m1_alllags, g2m1_laglist = sf.FindLags(series=ImageTimes, lags_index=Lagtimes_idxlist, refs_index=img_indexes, subset_intervals=int_bounds, tolerance=lag_tolerance, tolerance_isrelative=lag_tolerance_isrelative, verbose=verbose)
        
        g2m1 = np.zeros((len(int_bounds), np.max([len(l) for l in g2m1_laglist])), dtype=float)
        g2m1_lags = np.nan * np.ones_like(g2m1, dtype=float)
        g2m1_avgnum = np.zeros_like(g2m1, dtype=int)
        if verbose:
            logging.debug('AverageCorrTimetrace: cI time averages will be performed by dividing the {0} time points into {1} windows of sizes {2}'.format(CorrData.shape[0], len(int_bounds), [intb[1]-intb[0] for intb in int_bounds]))
            logging.debug('Detailed interval bounds: {0}'.format(int_bounds))
            logging.debug('original cI has shape ' + str(CorrData.shape) + '. Averaged g2m1 has shape ' + str(g2m1.shape) + ' (check: ' + str(g2m1_avgnum.shape) + ')')
        
        for cur_tavg_idx in range(len(int_bounds)):
            g2m1_lags[cur_tavg_idx,:len(g2m1_laglist[cur_tavg_idx])] = g2m1_laglist[cur_tavg_idx]
            for tidx in range(*int_bounds[cur_tavg_idx]): # all timepoints in current interval
                for lidx in range(CorrData.shape[1]): # all available lagtimes in input correlation matrix
                    if (tidx < len(g2m1_alllags[lidx])): # g2m1_alllags[lidx] is the list of time delays in physical units associated to lidx-th integer lagtime
                                                        # don't consider reference times tidx >= len(g2m1_alllags[lidx]), as the second time points is beyond maximum timepoint
                        if (sharp_bound == False) or (tidx + Lagtimes_idxlist[lidx] >= int_bounds[cur_tavg_idx][0] and \
                                                    tidx + Lagtimes_idxlist[lidx] < int_bounds[cur_tavg_idx][1]):
                            # find which lagtime in physical units is closest to the current lagtime
                            cur_lagidx = np.argmin(np.abs(np.subtract(g2m1_laglist[cur_tavg_idx], g2m1_alllags[lidx][tidx])))
                            if (~np.isnan(CorrData[tidx,lidx])):
                                g2m1_avgnum[cur_tavg_idx,cur_lagidx] += 1
                                g2m1[cur_tavg_idx,cur_lagidx] += CorrData[tidx,lidx]
                        elif verbose:
                            logging.debug('Time index {0} and lag index {1} (d{2}) skipped because of sharp bound {3}'.format(tidx, lidx, Lagtimes_idxlist[lidx], int_bounds[cur_tavg_idx]))
        g2m1 = np.divide(g2m1, g2m1_avgnum)
        
        if return_stderr:
            g2m1_err = np.zeros_like(g2m1)
            for cur_tavg_idx in range(len(int_bounds)):
                for tidx in range(*int_bounds[cur_tavg_idx]):
                    for lidx in range(CorrData.shape[1]):
                        if (tidx < len(g2m1_alllags[lidx])):
                            if (sharp_bound == False) or (tidx + Lagtimes_idxlist[lidx] >= int_bounds[cur_tavg_idx][0] and \
                                                        tidx + Lagtimes_idxlist[lidx] < int_bounds[cur_tavg_idx][1]):
                                cur_lagidx = np.argmin(np.abs(np.subtract(g2m1_laglist[cur_tavg_idx], g2m1_alllags[lidx][tidx])))
                                if (~np.isnan(CorrData[tidx,lidx])):
                                    g2m1_err[cur_tavg_idx,cur_lagidx] += np.square(CorrData[tidx,lidx]-g2m1[cur_tavg_idx,cur_lagidx])
            g2m1_err = np.sqrt(np.divide(g2m1_err, g2m1_avgnum))

    if return_stderr:
        return g2m1, g2m1_lags, g2m1_err
    else:
        return g2m1, g2m1_lags

def ValidateShiftRange(ShiftRange, ImageShape=None, useROI=None, minRange=None, maxRange=None):
    '''
    Validates the range of drifts that can be detected for a given image size and a given ROI size
    
    Parameters
    ----------
    - SearchRange : integer, couple of integers or 4D vector with boundaries for search range, in the form [min_x, max_x, min_y, max_y]
                    where min_x, max_x are minimum and maximum x lags (along columns)
                          min_y, max_y are minimum and maximum y lags (along rows)
                    if integer, search range is [-margin, margin, -margin, margin]
                    if couple of integers [margin_x, margin_y] search range is [-margin_x, margin_x, -margin_y, margin_y]
    - ImageShape :  shape of the input images [num_row, num_col], or None
                    if None, the program won't check that the correctly-formatted SearchRange is compatible with image size
    - useROI :      ROI coordinates, in the form [min_row, min_col, max_row, max_col], or None
    - minRange, maxRange: positive integers. If specified, limit range to the [minRange, maxRange] interval
                    NOTE: 'shape' validation based on ImageShape and useROI has the priority over minRange
                    
    Returns
    -------
    - res:          validated range, in the form [min_x, max_x, min_y, max_y]
    '''
    if not isinstance(ShiftRange, collections.abc.Iterable):
        res = [ShiftRange, ShiftRange]
    else:
        res = ShiftRange
    if len(res) == 2:
        res = [-res[0], res[0], -res[1], res[1]]
    if minRange is not None:
        res = [min(res[0], -minRange), max(res[1], minRange), min(res[2], -minRange), max(res[3], minRange)]
    if maxRange is not None:
        res = [max(res[0], -maxRange), min(res[1], maxRange), max(res[2], -maxRange), min(res[3], maxRange)]
    if ImageShape is not None and useROI is not None:
        max_range = [-useROI[1], ImageShape[1]-useROI[3], -useROI[0], ImageShape[0]-useROI[2]]
        res = [max(res[0], max_range[0]), min(res[1], max_range[1]), max(res[2], max_range[2]), min(res[3], max_range[3])]
    return res

def ValidateShiftROI(SearchROI, ShiftRange, ImageShape, ValidateRange=True, debugMode=False):
    if ValidateRange:
        ShiftRange = ValidateShiftRange(ShiftRange)
    if debugMode:
        if SearchROI is None:
            logging.debug('Searching for the largest ROI able to be shifted in range {0} within image of shape {1}'.format(ShiftRange, ImageShape))
        else:
            logging.debug('Validating compatibility of ROI with boundaries {0} to be shifted in range {1} within image of shape {2}'.format(SearchROI, ShiftRange, ImageShape))
    min_x, min_y = max(0, -ShiftRange[0]), max(0, -ShiftRange[2])
    max_x, max_y = min(ImageShape[1], ImageShape[1]-ShiftRange[1]), min(ImageShape[0], ImageShape[0]-ShiftRange[3])
    largestROI = [min_y, min_x, max_y, max_x]
    if (largestROI[2] <= largestROI[0] or largestROI[3] <= largestROI[1]):
        logging.error('No ROI compatible with image shape {0} and search range {1}: ROI {2} has negative edge sizes'.format(ImageShape, ShiftRange, largestROI))
        return None
    if SearchROI is None:
        resROI = largestROI
    else:
        resROI = [min(largestROI[2]-1, max(SearchROI[0], largestROI[0])), min(largestROI[3]-1, max(SearchROI[1], largestROI[1])), 
                  max(largestROI[0]+1, min(SearchROI[2], largestROI[2])), max(largestROI[1]+1, min(SearchROI[3], largestROI[3]))]
    if debugMode:
        logging.debug('Largest ROI able to be shifted: {0}. Final result: {1}'.format(largestROI, resROI))
    return resROI
    

def CalcCrosscorrMatrix(Image, Reference, SearchRange=1, SearchROI=None, ValidateInput=True, CovarianceNorm='mean', debugMode=False):
    '''
    Calculates the spatial crosscorrelation matrix between two images
    
    Parameters
    ----------
    - Image, Reference: 2D matrices (input images). They must have the same shape.
    - SearchRange : integer, couple of integers or 4D vector with boundaries for search range, in the form [min_x, max_x, min_y, max_y]
                    where min_x, max_x are minimum and maximum x lags (along columns)
                          min_y, max_y are minimum and maximum y lags (along rows)
                    if integer, search range is [-margin, margin, -margin, margin]
                    if couple of integers [margin_x, margin_y] search range is [-margin_x, margin_x, -margin_y, margin_y]
    - SearchROI :   4D vector with ROI definition, in the form [min_row, min_col, max_row, max_col], 
                    compatible with ROIproc.ROIboundingBoxes
                    If None, it is set to the largest subset allowing to explore the whole SearchRange
                    NOTE: ROI should be far enough from the boundaries to allow searching in the expected range.
                          If this is not the case, SearchRange will be adapted
    - ValidateInput : if True, validate SearchRange and SearchROI. Else, assume that it will be valid (to increase computation speed)
    - CovarianceNorm : ['variance', 'mean', 'none'] 
                    if 'none', compute (non-normalized) covariance matrix: covar = <IJ>-<I><J>
                    if 'variance', compute normalized correlation: corr = covar / sqrt((<I2>-<I>2)(<J2>-<J>2))
                    if 'mean', normalize correlation using product of means, as in cI: corr = covar/(<I><J>)
                    

    Returns
    -------
    - Xcorr:        cross-correlation matrix.
    '''
    MaximizeReference = (SearchROI is None)
    if (ValidateInput):
        im_shape = Image.shape
        SearchRange = ValidateShiftRange(SearchRange, im_shape, SearchROI)
        SearchROI = ValidateShiftROI(SearchROI, SearchRange, im_shape)
    Xcorr = np.empty((SearchRange[3]-SearchRange[2]+1, SearchRange[1]-SearchRange[0]+1), dtype=float)
    if MaximizeReference:
        Im_square = np.square(Image)
        Ref_square = np.square(Reference)
        if debugMode:
            logging.debug('CalcCrosscorrMatrix without SearchROI: Image has shape {0} (check: {1}), search range is {2}, Xcorr has shape {3}'.format(Image.shape, 
                                                                                                    Reference.shape, SearchRange, Xcorr.shape))
        for iRow in range(Xcorr.shape[0]):
            lagy = iRow + SearchRange[2]  # SearchRange[2]==min_y
            if lagy < 0:
                row_img = Image[:lagy]
                row_imsq = Im_square[:lagy]
                row_ref = Reference[-lagy:]
                row_refsq = Ref_square[-lagy:]
            elif lagy > 0:
                row_img = Image[lagy:]
                row_imsq = Im_square[lagy:]
                row_ref = Reference[:-lagy]
                row_refsq = Ref_square[:-lagy]
            else:
                row_img, row_imsq, row_ref, row_refsq = Image, Im_square, Reference, Ref_square
            if debugMode:
                logging.debug('Row #{0} (lagy={1}). Image shapes: {2} and {3}'.format(iRow, lagy, row_img.shape, row_ref.shape))
            for iCol in range(Xcorr.shape[1]):
                lagx = iCol + SearchRange[0]  # SearchRange[0]==min_x
                if lagx < 0:
                    Image_crop = row_img[:,:lagx]
                    Imsq_crop = row_imsq[:,:lagx]
                    Ref_crop = row_ref[:,-lagx:]
                    Refsq_crop = row_refsq[:,-lagx:]
                elif lagx > 0:
                    Image_crop = row_img[:,lagx:]
                    Imsq_crop = row_imsq[:,lagx:]
                    Ref_crop = row_ref[:,:-lagx]
                    Refsq_crop = row_refsq[:,:-lagx]
                else:
                    Image_crop, Imsq_crop, Ref_crop, Refsq_crop = row_img, row_imsq, row_ref, row_refsq
                if debugMode and iRow==0:
                    logging.debug('Column #{0} (lagx={1}). Image shapes: {2} and {3}'.format(iCol, lagx, Image_crop.shape, Ref_crop.shape))
                Imean, Rmean = np.mean(Image_crop), np.mean(Ref_crop)
                if CovarianceNorm=='variance':
                    Xcorr[iRow, iCol] = (np.mean(Image_crop * Ref_crop) - Imean*Rmean) / np.sqrt((np.mean(Imsq_crop)-Imean**2) * (np.mean(Refsq_crop)-Rmean**2))
                elif CovarianceNorm=='mean':
                    Xcorr[iRow, iCol] = np.mean(Image_crop * Ref_crop) / (Imean*Rmean) - 1
                elif CovarianceNorm=='none':
                    Xcorr[iRow, iCol] = np.mean(Image_crop * Ref_crop) - Imean*Rmean
                else:
                    raise ValueError('CalcCrosscorrMatrix: unknown normalization scheme "{0}"'.format(CovarianceNorm))
    else:
        Ref_crop = Reference[SearchROI[0]:SearchROI[2], SearchROI[1]:SearchROI[3]]
        Ref_crop_mean = np.mean(Ref_crop)
        Ref_crop_meansquare = np.mean(np.square(Ref_crop))
        Ref_crop_meanvar = Ref_crop_meansquare - Ref_crop_mean**2
        if debugMode:
            logging.debug('CalcCrosscorrMatrix with specified SearchROI: Image has shape {0} (check: {1}), cropped ROI is {2} (shape: {3}), search range is {4}, Xcorr has shape {5}'.format(Image.shape, 
                                                                                                        Reference.shape, SearchROI, (SearchROI[2]-SearchROI[0],SearchROI[3]-SearchROI[1]), SearchRange, Xcorr.shape))
            logging.debug('Cropped reference statistics: mean={0}, meansquare={1}, variance={2}'.format(np.mean(Reference), np.mean(np.square(Reference)), np.std(Reference)**2))
            logging.debug('Reference statistics: mean={0}, meansquare={1}, variance={2}'.format(Ref_crop_mean, Ref_crop_meansquare, Ref_crop_meanvar))
            logging.debug(str(Reference.shape) + '[' + str(SearchROI[0]) + ':' + str(SearchROI[2]) + ', ' 
                        + str(SearchROI[1]) + ':' + str(SearchROI[3]) + '] -> ' + str(Ref_crop.shape))
        for iRow in range(Xcorr.shape[0]):
            lagy = iRow + SearchRange[2]
            for iCol in range(Xcorr.shape[1]):
                lagx = iCol + SearchRange[0]
                Image_crop = Image[SearchROI[0]+lagy:SearchROI[2]+lagy, SearchROI[1]+lagx:SearchROI[3]+lagx]
                Imean = np.mean(Image_crop)
                if CovarianceNorm=='variance':
                    Xcorr[iRow, iCol] = (np.mean(Image_crop * Ref_crop) - Imean * Ref_crop_mean) / np.sqrt((np.mean(np.square(Image_crop))-Imean**2) * Ref_crop_meanvar)
                elif CovarianceNorm=='mean':
                    Xcorr[iRow, iCol] = np.mean(Image_crop * Ref_crop) / (Imean * Ref_crop_mean) - 1
                elif CovarianceNorm=='none':
                    Xcorr[iRow, iCol] = np.mean(Image_crop * Ref_crop) - Imean * Ref_crop_mean
                else:
                    raise ValueError('CalcCrosscorrMatrix: unknown normalization scheme "{0}"'.format(CovarianceNorm))
                if debugMode:
                    logging.debug('Xcorr[{0},{1}] : cropping image to [{2}:{3},{4}:{5}]. Raw product: {6}. Corr: {7}'.format(iRow, iCol, SearchROI[0]+lagy, SearchROI[2]+lagy, SearchROI[1]+lagx, SearchROI[3]+lagx, np.mean(Image_crop * Ref_crop), Xcorr[iRow, iCol]))
    return Xcorr

def FindParabolaExt(_corr, _x0, _y0, _x3, _y3, _a0, _a1, _a2, _a4, _a5):
    '''
    Function called by CalcDisplacement() to find subpixel parabola coordinates given 5 fixed points, 5 computed coefficients and a sixth new point
    '''
    _a3 = (_corr[_y3,_x3] - _a0 - _a1*(_x3-_x0) - _a2*(_x3-_x0)*(_x3-_x0+1) - _a4*(_y3-_y0) - _a5*(_y3-_y0)*(_y3-_y0+1))/((_x3-_x0)*(_y3-_y0))
    #here is the extremum
    ooden = 1./(4*_a2*_a5 - _a3**2)
    _f1 = (_a5*(_y0-1) - _a4)
    _f2 = (2*_a2*_a5 - _a3**2)
    xp = (_a5*(2*_a2*(_x0-1) + _a3*_y0 - 2*_a1) - _f1*_a3 + _f2*_x0)*ooden
    yp = (_a3*(_a2 + _a1) + 2*_f1*_a2 + _f2*_y0)*ooden
    zp = _a0 + _a1*(xp-_x0) + _a2*(xp-_x0)*(xp-_x0+1) + _a3*(xp-_x0)*(yp-_y0) + _a4*(yp-_y0) + _a5*(yp-_y0)*(yp-_y0+1)
    return xp, yp, zp


def FindSubpixelPeak(Matrix, TopLeftCoord=(0,0), Method='paraboloid', SubPixelOnly=False, MaxCoords=None, debugMode=False, verbose=0):
    '''
    Finds the coordinates of the local maximum with subpixel resolution
    
    Parameters
    ----------
    - Matrix: 2D vector
    - TopLeftCoord : couple of float representing the coordinates (x, y) of top-left pixel of the Matrix
    - Method : ['none', 'paraboloid', 'centroid', 'overlap']. Method to compute the position with subpixel resolution
                - 'none': return pixel-resolved peak position
                - 'paraboloid': approximate the peak height with a 2D paraboloid, 
                                use peak neighborhood to fix paraboloid coefficients,
                                find vertex height and position
                - 'centroid': weighted intensities in quadrants
    - SubPixelOnly: if True, only get the subpixel component of the position, relative to the discretized max position
                    if False, get the absolute position (positive downwards and rightwards), added to the top-left coordinate position
    - MaxCoords:    speed up computation by passing the coordinates (col_index, row_index) of the max, if known. (0,0) is the top-left corner

    Returns
    -------
    - xpeak, ypeak: subpixel-resolved coordinates of the crosscorrelation peak 
    - peak_height:  height of the subpixel-resolved crosscorrelation peak
                    If the pixel-resolved max falls on the edge of the search area, no subpixel search is performed and 
                    the routines returns the pixel-resolved position of the (apparent) max. peak_height is then set to -1
    '''
    if MaxCoords is None:
        imax = np.argmax(Matrix)
        cmax = imax % Matrix.shape[1]
        rmax = int((imax-cmax)/Matrix.shape[1])
    else:
        cmax, rmax = MaxCoords
    if debugMode:
        logging.debug('FindSubpixelPeak: max of crosscorrelation found in position {0} (matrix shape: {1}, coordinate of top-left corner: {2})'.format([rmax, cmax], Matrix.shape, TopLeftCoord))
    #check whether the max of corr lies on the edge of the search interval. If so, do not perform subpixel search
    if sf.CheckEdgePosition(rmax, cmax, Matrix.shape):
        if verbose > 0:
            logging.warning('FindSubpixelPeak: peak position {0} at the edge of crosscorrelation matrix with shape {1}'.format((rmax, cmax), Matrix.shape))
        if SubPixelOnly:
            return 0, 0, -1
        else:
            return cmax + TopLeftCoord[0], rmax + TopLeftCoord[1], -1

    if Method=='none':

        if SubPixelOnly:
            logging.warning('FindSubpixelPeak: SubPixelOnly option with Method="none" is trivial')
            return 0, 0, Matrix[rmax,cmax]
        else:
            return cmax + TopLeftCoord[0], rmax + TopLeftCoord[1], Matrix[rmax,cmax]

    elif Method=='paraboloid':

        # Now we find the 2D parabola through 6 points around the maximum of corr
        # to get the maximum of crosscorrelation with subpixel resolution using Newton polynomial-like method
        # we define the paraboloid as z = a0 + a1*(x-x0) + a2*(x-x0)*(x-x1) + a3*(x-x0)*(y-y0) + a4*(y-y0) + a5*(y-y0)*(y-y1).
        # Then, one can easily find by hand the coefficients a0...a5

        #here we impose the first 5 conditions, based on 5 points forming a cross centered on rmax,cmax
        a0 = Matrix[rmax,cmax]
        a1 = a0 - Matrix[rmax,cmax-1]
        a2 = (Matrix[rmax,cmax+1] - a0 - a1)*0.5
        a4 = a0 - Matrix[rmax-1,cmax]
        a5 = (Matrix[rmax+1,cmax] - a0 - a4)*0.5
        if debugMode:
            logging.debug('Paraboloid mode with peak value {0} (pos {1}) and cross values {2}'.format(Matrix[rmax,cmax], [rmax,cmax], [Matrix[rmax,cmax+1], Matrix[rmax-1,cmax], Matrix[rmax,cmax-1], Matrix[rmax+1,cmax]]))
            logging.debug('Common paraboloid coefficients: ' + str([a0,a1,a2,a4,a5]))

        #we choose the 6th point in 4 different ways (to insure symmetry) and average over the corresponding results. Note that
        #the paraboloid equation was written in such a way that the choice of the 6th point only affects a3.    
        peaks = np.empty((4, 3), dtype=float)
        p3s = [[rmax-1, cmax-1], [rmax+1, cmax+1], [rmax+1, cmax-1], [rmax-1, cmax+1]]
        if debugMode:
            logging.debug('Now computing the last coefficient (a3) using corner positions: {0} relative to peak position {1} (corner values: {2})'.format(p3s, [rmax,cmax], [Matrix[p[0],p[1]] for p in p3s]))
        for i in range(len(p3s)):
            peaks[i] = FindParabolaExt(Matrix, cmax, rmax, p3s[i][1], p3s[i][0], a0, a1, a2, a4, a5)
        peak_avg = np.mean(peaks, axis=0)
        if debugMode:
            peak_std = np.std(peaks, axis=0)
            logging.debug('Averaging on the four corners yielded: x={0}+/-{1}; y={2}+/-{3}; z={4}+/-{5}'.format(peak_avg[0], peak_std[0], peak_avg[1], peak_std[1], peak_avg[2], peak_std[2]))

        
        if SubPixelOnly:
            return peak_avg[0]-cmax, peak_avg[1]-rmax, peak_avg[2]
        else:
            return peak_avg[0] + TopLeftCoord[0], peak_avg[1] + TopLeftCoord[1], peak_avg[2]
    
    elif Method=='centroid':

        # Find quadrant weights
        alpha1, alpha2, alpha3 = 0.485869913, 0.545406041, 0.25 # see: Cipelletti et al.
        weights = np.zeros(4, dtype=float)
        weights[0] = alpha2 * Matrix[rmax-1, cmax-1] + alpha1 * (Matrix[rmax-1, cmax] + Matrix[rmax, cmax-1]) + alpha3 * Matrix[rmax, cmax] # top left quadrant
        weights[1] = alpha2 * Matrix[rmax-1, cmax+1] + alpha1 * (Matrix[rmax-1, cmax] + Matrix[rmax, cmax+1]) + alpha3 * Matrix[rmax, cmax] # top right quadrant
        weights[2] = alpha2 * Matrix[rmax+1, cmax+1] + alpha1 * (Matrix[rmax+1, cmax] + Matrix[rmax, cmax+1]) + alpha3 * Matrix[rmax, cmax] # bottom right quadrant
        weights[3] = alpha2 * Matrix[rmax+1, cmax-1] + alpha1 * (Matrix[rmax+1, cmax] + Matrix[rmax, cmax-1]) + alpha3 * Matrix[rmax, cmax] # bottom left quadrant

        dy = (weights[2]+weights[3]-weights[0]-weights[1])/np.sum(weights)
        dx = (weights[1]+weights[2]-weights[0]-weights[3])/np.sum(weights)

        if SubPixelOnly:
            return dx, dy, Matrix[rmax,cmax]
        else:
            return cmax + TopLeftCoord[0] + dx, rmax + TopLeftCoord[1] + dy, Matrix[rmax,cmax] # TODO: unclear if the centroid method can get us any further in correcting the height of the crosscorrelation peak
    
    else:

        raise ValueError('Method "' + str(Method) + '" not valid. Accepted methods: ["none", "paraboloid", "centroid"]')

def CalcDriftCorrKernel(x, M=None):
    if M is None:
        M = len(x)
    cosarg = x*2*np.pi/M
    window = 0.42323 + 0.49755*np.cos(cosarg)+0.07922*np.cos(2*cosarg)
    return window*np.sinc(x) #note: np.sinc = sin(pi*x)/(pi*x)  
        
def FindCrosscorrPeak(Image, Reference, SearchRange, SearchROI=None, SubgridShape=None, ValidateInput=True, SubpixelMethod='paraboloid', OverlapKernelSize=8, debugMode=False):
    '''
    Calculates the position and height of the crosscorrelation between two images with subpixel resolution
    Peak position is identified as the max of a paraboloid that goes through 6 points about the pixel-resolved peak
    
    Parameters
    ----------
    - Image, Reference: 2D vectors (input images). They should have the same shape
    - SearchRange : integer, couple of integers or 4D vector with boundaries for search range, in the form [min_x, max_x, min_y, max_y]
                    where min_x, max_x are minimum and maximum x lags (along columns)
                          min_y, max_y are minimum and maximum y lags (along rows)
                    if integer, search range is [-margin, margin, -margin, margin]
                    if couple of integers [margin_x, margin_y] search range is [-margin_x, margin_x, -margin_y, margin_y]
    - SearchROI :   None, 4D vector with ROI definition, in the form [min_row, min_col, max_row, max_col], 
                    compatible with ROIproc.ROIboundingBoxes
    - SubgridShape: None or couple of int (M,N). Eventually divide the validated SearchROI into a grid of MxN ROIs
                    (M ROIs along x axis, N along y axis), and process drifts independently in each subROI, 
                    to obtain statistics (average and standard deviation).
                    None corresponds to (1,1)
    - SubpixelMethod = ['none', 'paraboloid', 'centroid', 'overlap']
    - OverlapKernelSize : size of the overlap support kernel. Should be an even integer

    Returns
    -------
    - xpeak, ypeak: subpixel-resolved coordinates of the crosscorrelation peak 
    - peak_height:  height of the subpixel-resolved crosscorrelation peak
                    If the pixel-resolved max falls on the edge of the search area, no subpixel search is performed and 
                    the routines returns the pixel-resolved position of the (apparent) max. peak_height is then set to -1
    - xperr, yperr, zperr: standard errors on above quantities, only provided if SubgridShape is defined
    '''
    assert Image.shape == Reference.shape, 'Input images in FindCrosscorrPeak should have the same shape. Instead, here we have {0} and {1}'.format(Image.shape, Reference.shape)
    MaximizeReference = (SearchROI is None and SubgridShape is None)
    if SubpixelMethod=='overlap':
        assert OverlapKernelSize%2==0, 'FindCrosscorrPeak: Kernel size in overlap method should be an even integer. Invalid size: ' + str(OverlapKernelSize)
        max_search = np.max(np.abs(SearchRange))
        if max_search <= OverlapKernelSize//2:
            old_range = SearchRange
            SearchRange = ValidateShiftRange(SearchRange, minRange=OverlapKernelSize//2+1)
            logging.warning('FindCrosscorrPeak: search range with overlap method should be larger than half the kernel size ({0}) in all directions. Invalid search range {1} extended to {2}'.format(OverlapKernelSize, old_range, SearchRange))
    if (ValidateInput):
        SearchRange = ValidateShiftRange(SearchRange, Image.shape, SearchROI)
        SearchROI = ValidateShiftROI(SearchROI, SearchRange, Image.shape)
    if SubgridShape is None:
        SubgridShape = (1,1)
    if debugMode:
        logging.debug('ROIproc.FindCrosscorrPeak started. Search range: {0}, SearchROI: {1}, SubgridShape: {2}'.format(SearchRange, SearchROI, SubgridShape))
    if np.prod(SubgridShape) == 1:
        if MaximizeReference:
            SearchROI = None
        Xcorr = CalcCrosscorrMatrix(Image, Reference, SearchRange, SearchROI, ValidateInput=False, CovarianceNorm='mean', debugMode=debugMode)
        if SubpixelMethod == 'overlap':
            px_peak_x, px_peak_y, px_peak_z = FindSubpixelPeak(Xcorr, TopLeftCoord=(0, 0), Method='none')
            ctr_dx, ctr_dy, _ = FindSubpixelPeak(Xcorr, TopLeftCoord=(0, 0), Method='centroid', SubPixelOnly=True)
            r = px_peak_y+int(np.floor(ctr_dy))
            c = px_peak_x+int(np.floor(ctr_dx))
            TopQuadCoords = [[r,c], [r,c+1], [r+1,c], [r+1,c+1]]
            CovarIJ = CalcCrosscorrMatrix(Image, Reference, SearchRange=SearchRange, SearchROI=SearchROI, ValidateInput=False, CovarianceNorm='none')
            CovarII = CalcCrosscorrMatrix(Image, Image, SearchRange=SearchRange, SearchROI=SearchROI, ValidateInput=False, CovarianceNorm='none')
            b = np.zeros((4,1), dtype=float)
            M = np.zeros((4,4), dtype=float)
            for i in range(4):
                ki,li = TopQuadCoords[i]
                b[i] = CovarIJ[ki,li]
                for j in range(4):
                    kj,lj = TopQuadCoords[j]
                    M[i,j] = CovarII[ki-kj-SearchRange[2],li-lj-SearchRange[0]]
            a = np.matmul(np.linalg.inv(M), b)
            dx = float(TopQuadCoords[0][1] + (a[1]+a[3])/np.sum(a) + SearchRange[0])
            dy = float(TopQuadCoords[0][0] + (a[2]+a[3])/np.sum(a) + SearchRange[2])
            j_x, i_y = int(np.floor(dx))-SearchRange[0], int(np.floor(dy))-SearchRange[2]
            delta_x, delta_y = dx-np.floor(dx), dy-np.floor(dy)
            ker_x = CalcDriftCorrKernel(np.linspace(delta_x+OverlapKernelSize/2-1, delta_x-OverlapKernelSize/2, num=OverlapKernelSize, endpoint=True), M=OverlapKernelSize)
            ker_y = CalcDriftCorrKernel(np.linspace(delta_y+OverlapKernelSize/2-1, delta_y-OverlapKernelSize/2, num=OverlapKernelSize, endpoint=True), M=OverlapKernelSize)
            ker2D = np.zeros((OverlapKernelSize, OverlapKernelSize), dtype=float)
            for ridx in range(OverlapKernelSize):
                ker2D[ridx] = ker_y[ridx]*ker_x
            minr, maxr, minc, maxc = i_y-OverlapKernelSize//2+1, i_y+OverlapKernelSize//2+1, j_x-OverlapKernelSize//2+1, j_x+OverlapKernelSize//2+1
            if minr < 0 or minc < 0 or maxr > CovarIJ.shape[0] or maxc > CovarIJ.shape[1]:
                logging.debug('FindCrosscorrPeak computing convolution on trimmed area to fit covariance matrix size')
                if minr < 0:
                    ker2D = ker2D[-minr:]
                    minr = 0
                if maxr > CovarIJ.shape[0]:
                    maxr = CovarIJ.shape[0]
                    ker2D = ker2D[:maxr]
                if minc < 0:
                    ker2D = ker2D[:,-minc:]
                    minc = 0
                if maxc > CovarIJ.shape[1]:
                    maxc = CovarIJ.shape[1]
                    ker2D = ker2D[:,:maxc]
            CovarCrop = CovarIJ[minr:maxr, minc:maxc]
            if SearchROI is None:
                norm = np.mean(Image)*np.mean(Reference)
            else:
                norm = np.mean(Image[SearchROI[0]:SearchROI[2], SearchROI[1]:SearchROI[3]])*np.mean(Reference[SearchROI[0]:SearchROI[2], SearchROI[1]:SearchROI[3]])
            gcorr = np.sum(np.multiply(CovarCrop, ker2D)) / norm
            res = (dx, dy, gcorr)
        else:
            res = FindSubpixelPeak(Xcorr, TopLeftCoord=(SearchRange[0],SearchRange[2]), Method=SubpixelMethod, debugMode=debugMode)
        if debugMode:
            logging.debug('FindCrosscorrPeak function returned ' + str(res))
        return res
    else:
        logging.debug('Processing {0} subgrid regions'.format(np.prod(SubgridShape)))
        grid_x = np.linspace(SearchROI[1], SearchROI[3], num=SubgridShape[0]+1, endpoint=True, dtype=int)
        grid_y = np.linspace(SearchROI[0], SearchROI[2], num=SubgridShape[1]+1, endpoint=True, dtype=int)
        if debugMode:
            logging.debug('SubROI array generated: x={0}; y={1}'.format(grid_x, grid_y))
        xp_all, yp_all, zp_all = [], [], []
        count_bad = 0
        for ix in range(SubgridShape[0]):
            for iy in range(SubgridShape[1]):
                cur_xp, cur_yp, cur_zp = FindCrosscorrPeak(Image=Image, Reference=Reference, SearchRange=SearchRange, 
                                                           SearchROI=[grid_y[iy], grid_x[ix], grid_y[iy+1], grid_x[ix+1]], 
                                                           SubgridShape=None, ValidateInput=False, SubpixelMethod=SubpixelMethod, debugMode=debugMode)
                if cur_zp>0:
                    xp_all.append(cur_xp)
                    yp_all.append(cur_yp)
                    zp_all.append(cur_zp)
                else:
                    if debugMode:
                        logging.warning('SubROI [{0},{1}] with coordinates {2} returned analysis error - peak likely outside search range'.format(ix, iy, [grid_y[iy], grid_x[ix], grid_y[iy+1], grid_x[ix+1]]))
                    count_bad += 1
        if count_bad > 0:
            logging.warning('Spatial crosscorrelation on ROI {0} divided in subroi grid of shape {1} '.format(SearchROI, SubgridShape) +\
                         'returned analysis error for {0}/{1} ROIs (presumably peak outside search range)'.format(count_bad, np.prod(SubgridShape)))
        if len(xp_all)>0:
            return np.mean(xp_all), np.mean(yp_all), np.mean(zp_all), np.std(xp_all), np.std(yp_all), np.std(zp_all)
        else:
            return cur_xp, cur_yp, cur_zp, np.nan, np.nan, np.nan

def LoadFromConfig(ConfigParams, runAnalysis=True, outputSubfolder='reproc'):
    """Loads a ROIproc object from a config file like the one exported in ROIproc.ExportConfiguration
    
    Parameters
    ----------
    ConfigParams : full path of the config file to read or dict or Config object
    runAnalysis  : if the config file has an Analysis section, 
                   set runAnalysis=True to run the analysis after initializing the object
    outputSubfolder : save analysis output in a subfolder of Analysis.out_folder from configuration
                    if None, directly save output in Analysis.out_folder
                
    Returns
    -------
    the ROIproc object
    """
    
    config = cf.LoadConfig(ConfigParams)
    folder_root = config.Get('General', 'folder', None, str)
    
    strlog = 'ROIproc.LoadFromConfig reading configuration '
    if (type(ConfigParams) in [str]):
        strlog += 'from filename ' + str(ConfigParams)
        config_folder = sf.GetFolderFromCompletePath(ConfigParams)
        if folder_root is None:
            folder_root = config_folder
            logging.warn('ROIproc.LoadFromConfig: root folder inferred from configuration filename: ' + str(folder_root))
        elif folder_root != config_folder:
            logging.warn('ROIproc.LoadFromConfig: root folder from configuration file ({0}) is different from that inferred from configuration filename ({1})'.format(folder_root, config_folder))
    elif (type(ConfigParams) in [dict, collections.OrderedDict]):
        strlog += 'from dictionnary ({0} sections)'.format(len(ConfigParams.keys()))
    else:
        strlog += 'from Config object ({0} sections)'.format(ConfigParams.CountSections())
    if folder_root is None:
        strlog += ' -- No root folder specified!'
    else:
        strlog += ' -- Root folder: ' + str(folder_root)
    if config.HasSection('General'):
        strlog += ' -- version {0}, generated by {1} on {2}'.format(config.Get('General', 'version', 'NP', str),
                        config.Get('General', 'generated_by', 'NP', str), config.Get('General', 'generated_on', 'NP', str))
    logging.info(strlog)
    
    # MIfile
    if config.HasOption('MIfile', 'metadata_file'):
        miin_metadata = sf.GetAbsolutePath(config.Get('MIfile', 'metadata_file', None, str), root_path=folder_root)
    else:
        MIfile_keys = config.GetKeys('MIfile')
        logging.info('ROIproc.LoadFromConfig no "metadata_file" key found in section [MIfile] of config file: loading metadata directly from config file section (keys: {0}, expected keys: {1})'.format(MIfile_keys, MI.GetMetadataKeys()))
        if 'shape' not in MIfile_keys:
            logging.warn('ROIproc.LoadFromConfig no "shape" key in [MIfile] section of config file: resulting MIfile object will likely be corrupted')
        miin_metadata = config.ToDict(section='MIfile')
    miin_fname = config.Get('MIfile', 'filename', None, str)
    if config.HasOption('MIfile', 'is_stack'):
        miin_isstack = config.Get('MIfile', 'is_stack', False, bool)
        if miin_isstack and (type(miin_fname) in [str]):
            logging.warn('ROIproc.LoadFromConfig MIfile.is_stack={0} potentially inconsistent with MIfile.filename {1} of type {2}'.format(miin_isstack, miin_fname, type(miin_fname)))
    else:
        miin_isstack = (type(miin_fname) not in [str])
    if miin_isstack:
        mifile_info = 'MIstack ' + str(miin_fname)
        MIin = MIs.MIstack([sf.GetAbsolutePath(fname, root_path=folder_root) for fname in miin_fname], miin_metadata, Load=True, 
                           StackType=config.Get('MIfile', 'stack_type', 't', str))
    else:
        mifile_info = 'MIfile ' + str(miin_fname)
        MIin = MI.MIfile(sf.GetAbsolutePath(miin_fname, root_path=folder_root), miin_metadata)
    logging.debug('ROIproc.LoadFromConfig loading ' + str(mifile_info) + ' (metadata: ' + str(miin_metadata) + ')')

    # ROIs
    if config.HasSection('ROIs'):
        roi_num = config.Get('ROIs', 'number', None, int)
        if roi_num is not None:
            logging.debug('ROIproc.LoadFromConfig loading {0} ROIs'.format(roi_num))
        roi_maskfile = sf.GetAbsolutePath(config.Get('ROIs', 'mask_file', None, str), root_path=folder_root)
        logging.debug('ROIproc.LoadFromConfig reading integer mask {0} (shape {1} expected)'.format(roi_maskfile, MIin.ImageShape()))
        roi_int_mask = MI.ReadBinary(roi_maskfile, MIin.ImageShape(), 'i')
        if roi_int_mask is None:
            ROImasks = None
            logging.warn('ROIproc.LoadFromConfig error reading binary ROI masks from integer mask {0}'.format(roi_maskfile))
        else:
            ROImasks = [roi_int_mask==b for b in range(np.max(roi_int_mask)+1)]
            logging.debug('ROIproc.LoadFromConfig: {0} binary ROI masks loaded from integer mask {1} of shape {2}'.format(len(ROImasks), roi_maskfile, roi_int_mask.shape))
        ROImetadata = config.ToDict(section='ROIs')
        if config.HasOption('ROIs', 'coord_file'):
            roi_coord_file = sf.GetAbsolutePath(config.Get('ROIs', 'coord_file', '', str), root_path=folder_root)
            ROIcoords, ROInames, NormFact, ROIbb = iof.LoadROIcoords(roi_coord_file)
            ROImetadata['coords'] = np.array2string(ROIcoords, separator=',')
            ROImetadata['coord_names'] = ROInames
            logging.debug('ROIproc.LoadFromConfig: ROI coordinates has shape {0} (coordinate names: {1})'.format(ROIcoords.shape, ROInames))
        else:
            ROIcoords, ROInames, NormFact, ROIbb = None, None, None, None
            logging.info('ROIproc.LoadFromConfig: No ROIcoord file: ROI coordinates will be automatically generated')
    else:
        ROImasks = None
        ROImetadata = None
        if config.HasSection('SALS'):
            logging.info('ROIproc.LoadFromConfig: No ROIs section in configuration file. SALS section has been detected: ROIs will probably be inherited')
        else:
            logging.warn('ROIproc.LoadFromConfig: No ROIs section in configuration file. ROIproc will be initialized with default ROI')
    
    # Times
    if config.HasOption('ImgTimes', 'values'):
        img_times = config.Get('ImgTimes', 'values', [1.], float)
        logging.debug('ROIproc.LoadFromConfig: {0} image times loaded from config file: {1}'.format(len(img_times), img_times))
    else:
        num_times = config.Get('ImgTimes', 'number', 0, str)
        imgtimes_fname = config.Get('ImgTimes', 'file', None, str)
        if miin_isstack:
            imgtimes_fname = [sf.GetAbsolutePath(fname, root_path=folder_root) for fname in imgtimes_fname]
        else:
            imgtimes_fname = sf.GetAbsolutePath(imgtimes_fname, root_path=folder_root)
        imgtimes_usecol = config.Get('ImgTimes', 'usecol', 0, int)
        imgtimes_skiprow = config.Get('ImgTimes', 'skiprow', 0, int)
        img_times = iof.LoadImageTimes(imgtimes_fname, usecols=imgtimes_usecol, skiprows=imgtimes_skiprow, root_folder=None, return_unique=False)
        logging.debug('ROIproc.LoadFromConfig: {0} image times loaded from file {1}, column {2} (expected: {3})'.format(len(img_times), imgtimes_fname, imgtimes_usecol, num_times))
    if config.HasOption('ExpTimes', 'values'):
        exp_times = config.Get('ExpTimes', 'values', [1.], float)
        logging.debug('ROIproc.LoadFromConfig: {0} exposure times loaded from config file: {1}'.format(len(exp_times), exp_times))
    else:
        num_exptimes = config.Get('ExpTimes', 'number', 0, str)
        exptimes_fname = sf.GetAbsolutePath(config.Get('ExpTimes', 'file', None, str), root_path=folder_root)
        if miin_isstack:
            exptimes_fname = [sf.GetAbsolutePath(fname, root_path=folder_root) for fname in exptimes_fname]
        else:
            exptimes_fname = sf.GetAbsolutePath(exptimes_fname, root_path=folder_root)
        exptimes_usecol = config.Get('ExpTimes', 'usecol', 0, int)
        exptimes_skiprow = config.Get('ExpTimes', 'skiprow', 0, int)
        exp_times = iof.LoadImageTimes(exptimes_fname, usecols=exptimes_usecol, skiprows=exptimes_skiprow, root_folder=None, return_unique=True)
        logging.debug('ROIproc.LoadFromConfig: {0} exposure times loaded from file {1}, column {2} (expected: {3})'.format(len(exp_times), exptimes_fname, exptimes_usecol, num_exptimes))

    ROI_proc = ROIproc(MIin, ROImasks, ROImetadata=ROImetadata, imgTimes=img_times, expTimes=exp_times)
    logging.info('ROIproc object loaded!')
        
    if runAnalysis:
        ROI_proc.RunFromConfig(config, AnalysisSection='Analysis', OutputSubfolder=outputSubfolder)
            
    return ROI_proc




class ROIproc():
    """ Class to process MIfile computing averages and time correlations on Regions Of Interest (ROIs) """
    
    def __init__(self, MIin, ROImasks, ROImetadata=None, imgTimes=None, expTimes=[1], PDdata=None, BkgCorr=None):
        """
        Initialize SALS

        Parameters
        ----------
        MIin :      input MIfile or MIstack. It can be empty (i.e. initialized with metadata only)
        ROImasks :  list of binary masks. Each mask is a 2D binary array with same shape as MIin.ImageShape()
                    True values (nonzero) denote pixels that will be included in the analysis,
                    False values (zeroes) will be excluded
                    If None, all pixels will be included.
        ROImetadata : None or dict with additional ROI metadata (see SetROIs function)
                    
        imgTimes :  Float array of length Nimgs. i-th element will be the time of the image.
                    If none, images will be 
        PDdata :    None, 1D float array or list of 2 arrays [Iin, Itr], each of length equal to the number of images.
        BkgCorr :   Eventually, dict with data for background correction. A few keys:
                    ['Dark', 'Opt'] : dict with dark measurement and empty beam measurement. Each with a few keys: 
                                      - 'Iavg'     : 1D float array, i-th element is I averaged on i-th ROI, 
                                                     normalized the same way as the output of Iavg.dat
                                      - 'Iavg_norm': float, normalization factor that needs to be removed from Iavg
                                                     for it to be expressed as raw (8-bit grayscale) per unit exptime [ms]
                                                     If missing, Iavg will be interpreted as already expressed in these units
                                      - 'Iavg_raw' : 2D float array, element [i,j] is the average of i-th exposure time on j-th ROI
                                      - 'exptimes' : 1D float array with exposure times, in ms
                                      - 'PDdata'   : [Iin, Itr], each being a float value
        """
        
        self.MIinput   = MIin
        self.SetROIs(ROImasks, ROImetadata)
        self._loadTimes(imgTimes)
        self.SetExptimes(expTimes)
        self._initConstants()
        
        self.LoadPD(PDdata)
        self.LoadBkg(BkgCorr)
        
    def _loadTimes(self, imgTimes=None):
        if imgTimes is None:
            self.imgTimes = np.arange(0, self.MIinput.ImageNumber() * 1./self.MIinput.GetFPS(), 1./self.MIinput.GetFPS())
            logging.debug('{0} image times automatically generated from MI metadata (fps={1}Hz)'.format(len(self.imgTimes), self.MIinput.GetFPS()))
        else:
            if (len(imgTimes) < self.MIinput.ImageNumber()):
                raise ValueError('Image times ({0}) should at least be as many as the image number ({1}).'.format(len(imgTimes), self.MIinput.ImageNumber()))
            elif (len(imgTimes) > self.MIinput.ImageNumber()):
                logging.warn('More image times ({0}) than images ({1}). Only first {1} image times will be considered'.format(len(imgTimes), self.MIinput.ImageNumber()))
                self.imgTimes  = imgTimes[:self.MIinput.ImageNumber()]
            else:
                self.imgTimes  = imgTimes
                logging.debug('{0} image times loaded (Image number: {1})'.format(len(self.imgTimes), self.MIinput.ImageNumber()))

    def LoadPD(self, PDdata):
        if not sf.IsIterable(PDdata):
            PDdata = None
        if PDdata is None:
            PDdata = [None, None]
        if len(PDdata) < 2 or (not sf.IsIterable(PDdata[0]) and PDdata[0] is not None):
            PDdata = [PDdata, None]
        if PDdata[0] is not None and PDdata[1] is None:
            logging.info('ROIproc.LoadPD called with single scalar array: interpreted as incoming intensity')
        self.PDdata = PDdata
        for i in range(2):
            if sf.IsIterable(PDdata[i]):
                if PDdata[i].ndim > 1:
                    logging.warning('LoadPD: flattening multi-dimensional PD data')
                    PDdata[i] = PDdata[i].flatten()

    def LoadBkg(self, BkgCorr):
        if BkgCorr is None:
            BkgCorr = {}
        self.BkgCorr = BkgCorr
        for bkg_key in ['Dark', 'Opt']:
            if self.HasBkg(bkg_key=bkg_key):
                if self.HasBkgIavg(bkg_key=bkg_key):
                    if self.BkgCorr[bkg_key]['Iavg'].ndim > 1:
                        cur_shape = self.BkgCorr[bkg_key]['Iavg'].shape
                        logging.warn('ROIproc.LoadBkg: {0} background has {1}d Iavg (shape: {2}). Averaging on all but last dimension'.format(bkg_key, len(cur_shape), cur_shape))
                        self.BkgCorr[bkg_key]['Iavg'] = self.BkgCorr[bkg_key]['Iavg'].reshape(-1, cur_shape[-1]).mean(axis=0)
                if self.HasBkgIraw(bkg_key=bkg_key):
                    if self.BkgCorr[bkg_key]['Iavg_raw'].ndim > 2:
                        cur_shape = self.BkgCorr[bkg_key]['Iavg_raw'].shape
                        logging.warn('ROIproc.LoadBkg: {0} background has {1}d Iavg_raw (shape: {2}). Averaging on all but last two dimensions'.format(bkg_key, len(cur_shape), cur_shape))
                        self.BkgCorr[bkg_key]['Iavg_raw'] = self.BkgCorr[bkg_key]['Iavg_raw'].reshape(-1, cur_shape[-2], cur_shape[-1]).mean(axis=0)
    
    def HasBkg(self, bkg_key):
        return (bkg_key in self.BkgCorr)
    def HasDarkBkg(self):
        return self.HasBkg(bkg_key='Dark')
    def HasOptBkg(self):
        return self.HasBkg(bkg_key='Opt')
        
    def HasBkgIavg(self, bkg_key):
        if self.HasBkg(bkg_key=bkg_key):
            return ('Iavg' in self.BkgCorr[bkg_key])
        else:
            return False
    def HasDarkBkgIavg(self):
        return self.HasBkgIavg(bkg_key='Dark')
    def HasOptBkgIavg(self):
        return self.HasBkgIavg(bkg_key='Opt')
    
    def HasBkgIraw(self, bkg_key):
        if self.HasBkg(bkg_key=bkg_key):
            return ('Iavg_raw' in self.BkgCorr[bkg_key])
        else:
            return False
    def HasDarkBkgIraw(self):
        return self.HasBkgIraw(bkg_key='Dark')
    def HasOptBkgIraw(self):
        return self.HasBkgIraw(bkg_key='Opt')
    
    def GetBkgIavg(self, bkg_key, texp=1, use_norm=True):
        if self.HasBkgIavg(bkg_key=bkg_key):
            if use_norm and 'Iavg_norm' in self.BkgCorr[bkg_key]:
                cur_norm = self.BkgCorr[bkg_key]['Iavg_norm']
            else:
                cur_norm = 1
            return (self.BkgCorr[bkg_key]['Iavg'] / cur_norm) * texp
        else:
            return None
    def GetDarkBkgIavg(self, texp=1):
        return self.GetBkgIavg(bkg_key='Dark', texp=texp)
    def GetOptBkgIavg(self, texp=1):
        return self.GetBkgIavg(bkg_key='Opt', texp=texp)
    
    def GetBkgIraw(self, bkg_key, exp_idx=-1):
        if self.HasBkgIraw(bkg_key=bkg_key):
            res = self.BkgCorr[bkg_key]['Iavg_raw']
            #logging.debug('GetBkgIraw: raw {0} background has shape {1}. Exposure time to be extracted: {2}'.format(bkg_key, res.shape, exp_idx))
            if exp_idx < 0:
                return res
            elif exp_idx < len(res):
                #logging.debug('GetBkgIraw: returning {0}-th exposure time for raw {1} background. Result has shape: {2}'.format(exp_idx, bkg_key, res[exp_idx].shape))
                return res[exp_idx]
            else:
                raise ValueError('Unable to get {0}-th exposure time in {1} dark background: {2} exposure times available'.format(exp_idx, bkg_key, len(res)))
        else:
            return None
    def GetDarkBkgIraw(self, exp_idx=-1):
        return self.GetBkgIraw(bkg_key='Dark', exp_idx=exp_idx)
    def GetOptBkgIraw(self, exp_idx=-1):
        return self.GetBkgIraw(bkg_key='Opt', exp_idx=exp_idx)
    
    def GetBkgIbest(self, bkg_key, exp_idx, nobkg_res=None):
        if self.HasBkgIraw(bkg_key=bkg_key):
            #logging.debug('GetBkgIbest found raw data for {0} background. Extracting {1}-th exposure time'.format(bkg_key, exp_idx))
            return self.GetBkgIraw(bkg_key=bkg_key, exp_idx=exp_idx)
        elif self.HasBkgIavg(bkg_key=bkg_key):
            logging.debug('GetBkgIbest found only averaged data for {0} background. Renormalizing it to map it to {1}-th exposure time ({2}ms)'.format(bkg_key, exp_idx, self.GetExptimes(exp_idx=exp_idx, corr=exp_idx)))
            return self.GetBkgIavg(bkg_key=bkg_key, texp=self.GetExptimes(exp_idx=exp_idx, corr=exp_idx))
        else:
            return nobkg_res
    def GetDarkBkgIbest(self, exp_idx, nobkg_res=None):
        return self.GetBkgIbest(bkg_key='Dark', exp_idx=exp_idx, nobkg_res=nobkg_res)
    def GetOptBkgIbest(self, exp_idx, nobkg_res=None):
        return self.GetBkgIbest(bkg_key='Opt', exp_idx=exp_idx, nobkg_res=nobkg_res)
    
    def HasBkgPDdata(self, bkg_key):
        if self.HasBkg(bkg_key=bkg_key):
            return ('PDdata' in self.BkgCorr[bkg_key])
        else:
            return False
    def HasDarkBkgPDdata(self):
        return self.HasBkgPDdata(bkg_key='Dark')
    def HasOptBkgPDdata(self):
        return self.HasBkgPDdata(bkg_key='Opt')
    
    def GetBkgPDdata(self, bkg_key, PDidx=-1, calc_avg=False):
        if self.HasBkgPDdata(bkg_key=bkg_key):
            res = self.BkgCorr[bkg_key]['PDdata']
        else:
            res = [0, 0]
        if PDidx < 0:
            if calc_avg and sf.IsIterable(res[0]):
                return [np.mean(x) for x in res]
            else:
                return res
        elif PDidx < len(res):
            res = res[PDidx]
            if calc_avg and sf.IsIterable(res):
                #logging.debug('ROIproc.GetBkgPDdata: now averaging on {0} available PD datapoints for {1} background (channel {2})'.format(len(res), bkg_key, PDidx))
                res = np.mean(res)
            return res
        else:
            return None
    def GetDarkBkgPDdata(self, PDidx=-1, calc_avg=False):
        return self.GetBkgPDdata(bkg_key='Dark', PDidx=PDidx, calc_avg=calc_avg)
    def GetOptBkgPDdata(self, PDidx=-1, calc_avg=False):
        return self.GetBkgPDdata(bkg_key='Opt', PDidx=PDidx, calc_avg=calc_avg)
    
    def CountPDchannels(self):
        if self.PDdata is not None:
            return np.sum([1 for x in self.PDdata if x is not None])
        else:
            return 0

    def CountPDdata(self, chIdx=None):
        res = -1
        if self.PDdata is not None:
            if chIdx is None:
                for i in range(len(self.PDdata)):
                    if sf.IsIterable(self.PDdata[i]):
                        res = len(self.PDdata[i])
                        break
            else:
                if chIdx < len(self.PDdata):
                    res = len(self.PDdata[chIdx])
        return res
    
    def HasPDdata(self, chIdx=-1, tidx=-1, tgroup_len=1):
        if chIdx<0:
            return self.CountPDchannels() > 0 and self.CountPDdata() > 0
        else:
            res = self.CountPDchannels() > chIdx
            if tidx < 0:
                res = res and self.CountPDdata(chIdx=chIdx) > 0
            else:
                res = res and self.CountPDdata(chIdx=chIdx) > tidx*tgroup_len
            return res

    def GetPDdata(self, chIdx, tidx=-1, tgroup_len=1, nopd_res=None):
        if self.HasPDdata(chIdx=chIdx, tidx=tidx, tgroup_len=tgroup_len):
            if chIdx < 0:
                return self.PDdata
            else:
                cur_startidx, cur_endidx = tidx*tgroup_len, min((tidx+1)*tgroup_len, self.CountPDdata(chIdx=chIdx))
                if cur_startidx < cur_endidx:
                    if cur_endidx - cur_startidx > 1:
                        return np.mean(self.PDdata[chIdx][cur_startidx:cur_endidx])
                    else:
                        return self.PDdata[chIdx][cur_startidx]
                else:
                    raise ValueError('Unable to access data for PD #{0}: {1}-th time sweep of length {2} corresponds to interval [{3}:{4}] exceeding data size {5}'.format(chIdx, tidx, tgroup_len, cur_startidx, cur_endidx, self.CountPDdata(chIdx=chIdx)))
        else:
            return nopd_res
     
    def _initConstants(self):
        #Constants
        self.ExpTimeCorrFactor = 0
        self.MaxSafeAvgIntensity = 30
        self.LoopMaxInfoN = 100
        self.dt_tolerance = 1e-2
        self.dt_tolerance_isrelative = True
        self.DebugMode = False
        self.txt_delim = '\t'
        self.txt_comm = '#'
        self.savetxt_kwargs = {'delimiter': self.txt_delim, 'comments': self.txt_comm}
        self.out_names = {'imgTimes' : 'imgTimes.dat',
                          'ROIcoords': 'ROIcoords.dat',
                          'ROI_mask' : 'ROI_mask.raw',
                          'PDdata'   : 'PDdata.dat',
                          'Iavg'     : 'Iavg.dat',
                          'Iavg_raw' : 'Iavg_raw.dat'}
        self.log_verbose = 2 # 0: only errors; 1: also warnings; 2: also info; 3: full logging, including debug info 
    
    def GetExptimes(self, exp_idx=-1, corr=True):
        if exp_idx < 0:
            res = self.expTimes
        elif exp_idx < len(self.expTimes):
            res = self.expTimes[exp_idx]
        else:
            raise ValueError('{0}-th exposure time unavailable ({1} exposure times total)'.format(exp_idx, len(self.expTimes)))
        if corr and self.ExpTimeCorrFactor != 0:
            return res * (1+0.01/res)
        else:
            return res
        
    def __repr__(self):
        if (self.MIinput.IsStack()):
            return '<ROIproc object: MIstack (' + str(self.MIinput.Count()) + ' MIfiles) + ' + str(self.CountROIs()) + ' ROIs>'
        else:
            return '<ROIproc object: MIfile (' + str(self.MIinput.FileName) + ') + ' + str(self.CountROIs()) + ' ROIs>'

    def __str__(self):
        str_res  = '\n|-----------------|'
        str_res += '\n|  ROIproc class: |'
        str_res += '\n|-----------------+---------------'
        str_res += '\n| Input           : '
        if (self.MIinput.IsStack()):
            str_res += 'MIstack (' + str(self.MIinput.Count()) + ' MIfiles)'
        else:
            str_res += 'MIfile (' + self.MIinput.FileName + ')'
        str_res += ', ' + str(self.MIinput.ImageNumber()) + ' images'
        str_res += '\n| ROIs            : ' + str(self.CountROIs()) + ' (' + str(self.CountValidROIs()) + ' valid, ' + str(self.CountEmptyROIs()) + ' empty)'
        str_res += '\n| Exposure times  : ' + str(self.NumExpTimes()) + ', from ' + str(self.expTimes[0]) + ' to ' + str(self.expTimes[-1])
        str_res += '\n|-----------------+---------------'
        return str_res
        
    def SetROIs(self, ROImasks, ROImetadata=None):
        '''
        Sets ROIs from list of binary masks
        
        Parameters
        ----------
        ROImasks :  list of binary masks
        ROImetadata : None or dict with ROI metadata. Can contain one or more of the following keys:
                    'coords' : ndarray with ROI coordinates. If not set, ROIcoords=np.arange(self.CountROIs())
                    'coord_names' : list of str: coordinate names. If not set, ROIcoord_names = [coord0, coord1, ...]
                    'box_margin' : int with margin left for drift measurement. Default: 0

        '''
        
        if ROImasks is None:
            ROImasks = [np.ones(self.MIinput.ImageShape(), dtype=np.dtype('b'))]
            
        if ROImetadata is not None:
            roi_meta = cf.LoadConfig(ROImetadata, SectionName='ROIs')
            ROIcoords = roi_meta.Get('ROIs', 'coords', None, float)
            ROIcoord_names = roi_meta.Get('ROIs', 'coord_names', None, str)
            BoundingBoxMargin = roi_meta.Get('ROIs', 'box_margin', 0, int)
        else:
            ROIcoords = None
            ROIcoord_names = None
            BoundingBoxMargin = 0
        
        self.ROIs = BinaryToIntegerMask(ROImasks)
        self.ROI_masks = np.asarray(ROImasks)
        self.ROI_maskSizes = np.array([np.sum(self.ROI_masks[b]) for b in range(self.CountROIs())])
        
        self.UpdateBBmargin(BoundingBoxMargin)
        
        if self.CountEmptyROIs() > 0:
            if self.CountValidROIs() > 0:
                logging.warning('There are {0} out of {1} empty masks'.format(self.CountEmptyROIs(), self.CountROIs()))
            else:
                logging.error('ROI mask is empty (no valid ROIs found)')
        else:
            logging.info('Set {0} valid ROIs'.format(self.CountROIs()))
            
        if ROIcoords is None:
            ROIcoords = np.arange(self.CountROIs())
        else:
            ROIcoords = np.asarray(ROIcoords)
        if ROIcoords.ndim == 1:
            ROIcoords = np.expand_dims(ROIcoords, axis=1)
        self.ROIcoords = ROIcoords
        if ROIcoord_names is None:
            ROIcoord_names = ['coord' + str(i) for i in range(ROIcoords.shape[1])]
        self.ROIcoord_names = ROIcoord_names
        
    def UpdateBBmargin(self, BoundingBoxMargin):
        if BoundingBoxMargin<0:
            logging.error('Bounding box margin cannot be negative. Set to 0')
            BoundingBoxMargin = 0
        self.BoundingBoxMargin = BoundingBoxMargin
        self.BoundingBox = FindBoundingBoxROI(self.ROI_masks, BoundingBoxMargin)
        self.CropROIbb = [self.BoundingBox[1], self.BoundingBox[0], self.BoundingBox[3]-self.BoundingBox[1], self.BoundingBox[2]-self.BoundingBox[0]]
        self.ROIboundingBoxes = []
        self.ROI_masks_crop = []
        trivial_cropbb = True
        for ridx in range(self.CountROIs()):
            if self.IsROIvalid(ridx):
                cur_bb = FindBoundingBoxROI(self.ROI_masks[ridx], margin=0)
                self.ROIboundingBoxes.append([cur_bb[0]-self.BoundingBox[0], cur_bb[1]-self.BoundingBox[1], cur_bb[2]-self.BoundingBox[0], cur_bb[3]-self.BoundingBox[1]])
                cur_maskcrop = self.ROI_masks[ridx,cur_bb[0]:cur_bb[2],cur_bb[1]:cur_bb[3]]
                if np.any(cur_maskcrop==0):
                    self.ROI_masks_crop.append(cur_maskcrop)
                    trivial_cropbb = False
                else:
                    self.ROI_masks_crop.append(np.ones((cur_bb[2]-cur_bb[0],cur_bb[3]-cur_bb[1])))
            else:
                logging.debug('ROI {0} is empty'.format(ridx))
                self.ROIboundingBoxes.append([0, 0, self.BoundingBox[2]-self.BoundingBox[0], self.BoundingBox[3]-self.BoundingBox[1]])
                self.ROI_masks_crop.append(np.zeros((self.BoundingBox[2]-self.BoundingBox[0], self.BoundingBox[3]-self.BoundingBox[1]), dtype=np.dtype('b')))
                trivial_cropbb = False
        self.ROIboundingBoxes = np.asarray(self.ROIboundingBoxes)
        if trivial_cropbb:
            self.ROI_masks_crop = None
            
    def SetExptimeCorrFactor(self, corrFactor):
        logging.warn('ROIproc: changing exposure time correction factor from {0:.4f} to {1:.4f} ms'.format(self.ExpTimeCorrFactor, corrFactor))
        self.ExpTimeCorrFactor = corrFactor

    def SetExptimes(self, expTimes):
        if not hasattr(expTimes, "__len__"):
            try:
                float_expt = float(expTimes)
                expTimes = [float_expt]  # if expTimes is a scalar, turn it into a 1-element list
            except:
                logging.error('ROIproc.SetExptimes() called with non-numeric argument: ' + str(expTimes))
                expTimes = []
        expTimes = np.asarray(expTimes)
        if expTimes.size > 0:
            _exps = np.unique(expTimes)
            # check that expTimes is sorted:
            #assert np.all(np.diff(expTimes) >= 0), 'Exposure times ' + str(expTimes) + ' must be sorted!'
            self.expTimes = np.asarray(sorted(_exps))
            if len(self.expTimes) > 1:
                logging.debug('Set {0} exptimes, sorted from {1} to {2}'.format(len(self.expTimes), self.expTimes[0], self.expTimes[-1]))
            else:
                logging.debug('Set one single exposure time: {0}'.format(self.expTimes[0]))
        else:
            logging.error('ROIproc.SetExptimes() called with empty expTimes list: ' + str(expTimes))

    def IsROIvalid(self, ROIidx):
        return self.ROI_maskSizes[ROIidx]>0
    def CountROIs(self):
        return len(self.ROI_masks)
    def CountEmptyROIs(self):
        return self.CountROIs() - self.CountValidROIs()
    def CountValidROIs(self):
        return np.count_nonzero(self.ROI_maskSizes)
    def ImageNumber(self):
        return self.MIinput.ImageNumber()
    def NumTimes(self):
        return self.MIinput.ImageNumber() // len(self.expTimes)
    def NumExpTimes(self):
        return len(self.expTimes)
    def StackInput(self):
        return self.MIinput.IsStack()
    def ReadCIfile(self, fpath):
        return iof.ReadCIfile(fpath)
    
    def GetImage(self, image_idx, buffer=None):
        '''
        Assumes that buffer contains images already cropped to size CropROIbb
        '''
        if (self.MIinput.ReadFileHandle is None):
            self.MIinput.OpenForReading()
        return self.MIinput.GetImage(image_idx, cropROI=self.CropROIbb, buffer=buffer, buffer_crop=False)
    def GetCroppedShape(self):
        return (self.CropROIbb[3], self.CropROIbb[2])
        
    def ReadMI(self):
        return self.MIinput.Read(cropROI=self.CropROIbb)

    def SaveIavg(self, SaveFolder, Iavg, AllExpData=None, NormData=None, CorrExptime=True):
        """ Saves output of SLS analysis

        Parameters
        ----------
        Iavg :          2D array of shape (NumTimes(), NumROIs())
        AllExpData :    None or [Iav_allexp, best_exptime_idx], data with all exposure times
        NormData :      None or [Norm, Base], data with normalization (Norm, Base are both 2D arrays with same shape as Iavg)
        CorrExptime :   bool. If False, disable exposure time correction even if a correcting factor (self.ExpTimeCorrFactor) is specified
        
        Saves
        -----
        - Iavg.dat :     file with normalized average intensity computed using best exposure times
        - exptimes.dat:  file with index of best exposure times chosen (only if AllExpData is not None)
        - Iavg_raw.dat:  file with all average intensities for all exposure times (only if AllExpData is not None)
        - ROIcoords:     file with ROI coordinates and normalization factors
        - ROI_mask:      raw file with pixel mask
        """
        
        sf.CheckCreateFolder(SaveFolder)
        ROIhdr_str = self.txt_delim.join(self.ROIcoord_names)
        flat_times = self.imgTimes.reshape(-1)
        
        str_hdr = ''.join([self.txt_delim+'t{0:.3f}'.format(flat_times[i])
                                            for i in range(0, self.ImageNumber(), self.NumExpTimes())])
        str_hdr_avg = ROIhdr_str + str_hdr
        np.savetxt(os.path.join(SaveFolder, self.out_names['Iavg']), np.append(self.ROIcoords[:,:2], Iavg.T, axis=1), 
                   header=str_hdr_avg, **self.savetxt_kwargs)
        
        if AllExpData is not None:
            ROIavgs_allExp, BestExptime_Idx = AllExpData
            np.savetxt(os.path.join(SaveFolder, 'exptimes.dat'), BestExptime_Idx.T.astype(int), header=str_hdr, fmt='%i', **self.savetxt_kwargs)
            str_hdr_raw = ROIhdr_str + ''.join([self.txt_delim+'t{0:.3f}_e{1:.3f}'.format(flat_times[i], self.expTimes[i%len(self.expTimes)])
                                                for i in range(len(flat_times))])
            np.savetxt(os.path.join(SaveFolder, self.out_names['Iavg_raw']), np.append(self.ROIcoords[:,:2], ROIavgs_allExp.reshape((-1, ROIavgs_allExp.shape[-1])).T, axis=1), 
                       header=str_hdr_raw, **self.savetxt_kwargs)
            
        if self.ExpTimeCorrFactor != 0:
            np.savetxt(os.path.join(SaveFolder, 'exptimes_corr.dat'), np.asarray([self.expTimes, self.GetExptimes(corr=CorrExptime)]).T, 
                       header='texp_nominal[ms]'+self.txt_delim+'texp_corr[ms]', **self.savetxt_kwargs)
        
        if NormData is not None:
            np.savetxt(os.path.join(SaveFolder, 'normalization.dat'), np.append(self.ROIcoords[:,:2], NormData[0].T, axis=1),
                       header=str_hdr_avg, **self.savetxt_kwargs)
            np.savetxt(os.path.join(SaveFolder, 'baseline.dat'), np.append(self.ROIcoords[:,:2], NormData[1].T, axis=1),
                       header=str_hdr_avg, **self.savetxt_kwargs)

    def LoadIavg(self, outFolder, skipcols=2):
        Ir_allexp, Ir, best_exptimes = None, None, None
        Iavg_fpath = os.path.join(outFolder, self.out_names['Iavg'])
        if os.path.isfile(Iavg_fpath):
            Iavg = np.loadtxt(Iavg_fpath, **self.savetxt_kwargs)
            if (Iavg.shape[1] > skipcols):
                Iavg = Iavg[:,skipcols:].T
                logging.debug('Loading average intensity data from {0}: output has shape {1}'.format(Iavg_fpath, Iavg.shape))
            else:
                Iavg = None
                logging.warn('Loading average intensity data from {0} failed (column number {1} not exceeding columns to be skipped, {2}): None returned'.format(Iavg_fpath, Iavg.shape[1], skipcols))
        else:
            Iavg = None
        if Iavg is None:
            Iav_allexp = None
        else:
            Iavraw_fpath = os.path.join(outFolder, self.out_names['Iavg_raw'])
            if os.path.isfile(Iavraw_fpath):
                Iav_allexp = np.loadtxt(Iavraw_fpath, **self.savetxt_kwargs)
                if (Iav_allexp.shape[1] > skipcols):
                    Iav_allexp = Iav_allexp[:,skipcols:].T
                    Iav_allexp = np.expand_dims(Iav_allexp, axis=1)
                    logging.debug('Loading raw average intensity data from {0}: output has shape {1}'.format(Iavraw_fpath, Iav_allexp.shape))
                else:
                    Iav_allexp = None
                    logging.warn('Loading raw average intensity data from {0} failed (column number {1} not exceeding columns to be skipped, {2}): None returned'.format(Iavraw_fpath, Iav_allexp.shape[1], skipcols))
        Iexp_fpath = os.path.join(outFolder, 'exptimes.dat')
        if os.path.isfile(Iexp_fpath):
            best_exptimes = np.loadtxt(Iexp_fpath, **self.savetxt_kwargs)
            if (best_exptimes.shape[1] > skipcols):
                best_exptimes = best_exptimes[:,skipcols:].T
                logging.debug('Loading best exposure times from {0}: output has shape {1}'.format(Iexp_fpath, best_exptimes.shape))
            else:
                best_exptimes = None
                logging.warn('Loading best exposure times from {0} failed (column number {1} not exceeding columns to be skipped, {2}): None returned'.format(Iexp_fpath, best_exptimes.shape[1], skipcols))
        else:
            best_exptimes = None
                
        return Iav_allexp, Iavg, best_exptimes

    def ROIaverageIntensity(self, stack1=None, no_buffer=False, imgs=None, no_boundingbox=False):
        return self.ROIaverageProduct(stack1, stack2=None, no_buffer=no_buffer, imgs=imgs, no_boundingbox=no_boundingbox)
    
    def ROIaverageProduct(self, stack1=None, stack2=None, no_buffer=False, imgs=None, no_boundingbox=False):
        """ ROI average product of images

        Parameters
        ----------
        stack1 : list of indexes. Images will be either read by MIinput or retrieved from img_buffer
                 if None, all images will be included in stack1
        stack2 : None, or list of indexes
                 - if None: function will return averages of single images (in stack1)
                 - if list: length should be the same as stack1
        no_buffer : if True, avoid reading all images to a buffer, but read images one by one
                    (dumping them afterwards)
        imgs : None or 3D array with buffered images. If None, images will be read from MIinput

        Returns
        -------
        AvgRes : 2D array. Element [i,j] is the average of i-th image (or the product of i-th images in the two stacks) on j-th ROI
        """
        if stack1 is None:
            stack1 = np.arange(self.ImageNumber())
        num_ROI = self.CountROIs()
        if no_boundingbox:
            use_bb = None
        else:
            use_bb = self.ROIboundingBoxes
            
        if (self.StackInput() or no_buffer):
            AvgRes = np.nan*np.ones((len(stack1), num_ROI), dtype=float)
            for i in range(AvgRes.shape[0]):
                if stack2 is None:
                    AvgRes[i], NormList = ROIAverage(self.GetImage(stack1[i], buffer=imgs), self.ROI_masks_crop, boolMask=True, 
                                                     norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
                else:
                    if (stack1[i]==stack2[i]):
                        AvgRes[i], NormList = ROIAverage(np.square(self.GetImage(stack1[i], buffer=imgs)), self.ROI_masks_crop, boolMask=True, 
                                                         norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
                    else:
                        AvgRes[i], NormList = ROIAverage(np.multiply(self.GetImage(stack1[i], buffer=imgs), self.GetImage(stack2[i], buffer=imgs)), 
                                                         self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
                    if (self.DebugMode):
                        if (np.any(AvgRes[i]<0)):
                            min_idx = np.argmin(AvgRes[i])
                            logging.warn('Negative cross product value (image1: {0}, image2: {1}, ROI{2} avg: {3})'.format(stack1[i], stack2[i], min_idx, 
                                                                                                                           AvgRes[i][min_idx]))
                            logging.debug('   >>> Debug output for ROIAverage function:\n'
                                          + str(ROIAverage(np.multiply(self.GetImage(stack1[i], buffer=imgs), self.GetImage(stack2[i], buffer=imgs)), 
                                                           self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb, debug=True)))
        else:
            if imgs is None:
                imgs = self.ReadMI()
            if self.DebugMode:
                if len(stack1) < 100:
                    logging.debug('ROIproc.ROIaverageProduct processing buffer with {0} images. First stack ({1} indexes): {2}'.format(len(imgs), len(stack1), stack1))
                else:
                    logging.debug('ROIproc.ROIaverageProduct processing buffer with {0} images. First stack ({1} indexes): [{2},{3},...,{4},{5}]'.format(len(imgs), 
                                                                                                    len(stack1), stack1[0], stack1[1], stack1[-2], stack1[-1]))
            if stack2 is None:
                if self.DebugMode:
                    logging.debug('No second stack: averaging single images')
                cur_stack = imgs[stack1]
            elif np.array_equal(stack1, stack2):
                if self.DebugMode:
                    logging.debug('Equal stacks provided: averaging squared single images')
                cur_stack = np.square(imgs[stack1])
            else:
                if self.DebugMode:
                    if len(stack2) < 100:
                        logging.debug('Second stack ({0} indexes): {1}'.format(len(stack2), stack2))
                    else:
                        logging.debug('Second stack ({0} indexes): [{1},{2},...,{3},{4}]'.format(len(stack2), stack2[0], stack2[1], stack2[-2], stack2[-1]))
                cur_stack = np.multiply(imgs[stack1], imgs[stack2])
            if self.DebugMode:
                logging.debug('ROIproc.ROIaverageProduct averaging stack1=' + str(stack1) + ' and stack2=' + str(stack2) + ' in debug mode...')
                AvgRes, NormList = ROIAverage(cur_stack, self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb, debug=True)
            else:
                AvgRes, NormList = ROIAverage(cur_stack, self.ROI_masks_crop, boolMask=True, norm=self.ROI_maskSizes, BoundingBoxes=use_bb)
            
        return AvgRes
    
    def CalcCorrelation(self, t1, t2, d0corr=True):
        '''
        Calculate sparse correlations between times t1 and t2
        
        Parameters
        ----------
        - t1, t2: int or list of int: image times to be correlated, in image units
                  if t1 and t2 are int, output is 1D
                  if one between t1 and t2 is int or 1-element list, it will be replicated to match the legth of the other list
                  if both t1 and t2 are lists (they should be equal size, or either one should have just one element),
                  correlation will be computed between each t1[i], t2[i] couple.
        - d0corr: bool. True to apply d0 ('contrast') normalization
                  If False, result will be <I1*I2>/(<I1><I2>)
                  If True, result will be further normalized by the average contrast:
                        [(<I1*I1>/(<I1><I1)) + (<I2*I2>/(<I2><I2))] / 2
        
        Returns
        -------
        - corr: 1D or 2D array. 
                  If multiple times are given output is 2D: corr[i,j] is the correlation of i-th couple (t1[i], t2[i]) on j-th ROI
                  If a single t1 and t2 is given, output is 1D: corr[i] is the correlation between the two times computed on i-th ROI
        '''
        if not isinstance(t1, collections.abc.Iterable):
            t1 = [t1]
        if not isinstance(t2, collections.abc.Iterable):
            t2 = [t2]
        if len(t1) != len(t2):
            if len(t1)==1:
                t1 = t1*len(t2)
                logging.info('ROIproc.CalcCorrelation(): single first time ({0}) replicated {1} times to match length of second time ({2}).'.format(t1[0], len(t1), len(t2)))
            elif len(t2)==1:
                t2 = t2*len(t1)
                logging.info('ROIproc.CalcCorrelation(): single second time ({0}) replicated {1} times to match length of second time ({2}).'.format(t2[0], len(t2), len(t1)))
            else:
                logging.warning('ROIproc.CalcCorrelation(): number of first times ({0}) must match that of second times ({1}). Longer list will be cropped'.format(len(t1), len(t2)))
                common_len = min(len(t1), len(t2))
                t1 = t1[:common_len]
                t2 = t2[:common_len]
        avg_t1 = self.ROIaverageIntensity(t1)
        avg_t2 = self.ROIaverageIntensity(t2)
        corr = np.divide(self.ROIaverageProduct(t1, t2), np.multiply(avg_t1, avg_t2)) - 1
        if d0corr:
            contrast_t1 = np.divide(self.ROIaverageProduct(t1, t1), np.square(avg_t1)) - 1
            contrast_t2 = np.divide(self.ROIaverageProduct(t2, t2), np.square(avg_t2)) - 1
            corr = 2 * corr / (contrast_t1 + contrast_t2)
        return np.squeeze(corr)
            
    def FindBestExptimes(self, AverageIntensities, normExptime=True, CorrExptime=True, debug_logfile=None):
        '''
        Find best exposure times based on ROI-averaged intensities
        
        Parameters
        ----------
        - AverageIntensities: 3D array. Element [i,j,k] is the average intensity of j-th ROI measured at k-th exposure time during i-th exposure time ramp
        - normExptime : bool. If True, normalize ROIavgs_best by the best exposure time
                              If False, ROIavgs_best will be the raw (8-bit grayscale) value
        - CorrExptime : bool. If False, disable exposure time correction even if a correcting factor (self.ExpTimeCorrFactor) is specified
        
        Returns
        -------
        - ROIavgs_best: 2D array. Element [i,j] is the intensity of the best exposure time of j-th ROI during i-th time, normalized by the exposure time itself
        - BestExptime_Idx: 2D array, containing the index of the best exposure time selected for ROIavgs_best
        '''
        if normExptime and self.ExpTimeCorrFactor != 0:
            if CorrExptime:
                logging.info('ROIproc.FindBestExptimes: Correcting exposure times with factor dt={0:.4f} ms'.format(self.ExpTimeCorrFactor))
            else:
                logging.info('ROIproc.FindBestExptimes: Disregarding correction time factor dt={0:.4f} ms. Normalizing intensities with nominal exposure times'.format(self.ExpTimeCorrFactor))
        if AverageIntensities.ndim < 3:
            tmp_shape = AverageIntensities.shape
            AverageIntensities = AverageIntensities.reshape((self.NumTimes(), self.NumExpTimes(), -1))
            logging.debug('FindBestExptimes: array with raw intensities reshaped from {0} to {1} ({2} times, {3} exptimes)'.format(tmp_shape, AverageIntensities.shape, self.NumTimes(), self.NumExpTimes()))
        ROIavgs_best = np.zeros((self.NumTimes(), AverageIntensities.shape[-1]), dtype=float)
        BestExptime_Idx = -1 * np.ones_like(ROIavgs_best, dtype=int)
        if debug_logfile is not None:
            debug_logfile.write('\nNow bisecting raw average intensities ({0} times, {1} exptimes, {2} ROIs) with threshold value {3}'.format(*AverageIntensities.shape, self.MaxSafeAvgIntensity))
        
        for idx, val in np.ndenumerate(ROIavgs_best):
            BestExptime_Idx[idx] = min(bisect.bisect(AverageIntensities[idx[0], :, idx[1]], self.MaxSafeAvgIntensity), len(self.expTimes)-1)
            ROIavgs_best[idx] = AverageIntensities[idx[0], BestExptime_Idx[idx], idx[1]]
            if debug_logfile is not None:
                debug_logfile.write('\n{0}: bisect returned {1} (value: {2})'.format(idx, BestExptime_Idx[idx], ROIavgs_best[idx]))
                if BestExptime_Idx[idx]>0:
                    debug_logfile.write('. Smaller exptime ({0}) has raw intensity {1}'.format(BestExptime_Idx[idx]-1, AverageIntensities[idx[0], BestExptime_Idx[idx]-1, idx[1]]))
                if BestExptime_Idx[idx]<AverageIntensities.shape[1]-1:
                    debug_logfile.write('. Larger exptime ({0}) has raw intensity {1}'.format(BestExptime_Idx[idx]+1, AverageIntensities[idx[0], BestExptime_Idx[idx]+1, idx[1]]))
            if normExptime:
                ROIavgs_best[idx] /= self.GetExptimes(corr=CorrExptime)[BestExptime_Idx[idx]]
        return ROIavgs_best, BestExptime_Idx
                             
    def doSLS(self, saveFolder, buf_images=None, no_buffer=False, force_calc=True, norm_PDdata=True, CorrExptime=True, 
              corr_bkg=True, opt_resc_trans=False, export_config=True, export_configparams=None, verbose=0):
        """ Run SLS analysis: compute average intensity, eventually choosing best exposure time for each ROI
        
        Parameters
        ----------
        - saveFolder  : folder path, to save analysis output. 
                        If None, no output will be saved
        - buf_images  : 3D array, buffer with images to be processed. If None, images will be loaded
        - no_buffer   : bool, used only if buf_images is None. If False, the entire image stack will be read in a buffer at once.
                        otherwise, images will be loaded one by one
        - force_calc  : bool. If False, program will search for previously computed SLS results and load those.
        - norm_PDdata : bool. If True, normalize SLS data using PDdata, if available.
                        If False (or if PDdata is None), SLS data will still be normalized by the exposure time
        - corr_bkg    : bool. If False, disable background correction even if it is given
        - opt_resc_trans : bool. If correcting for the optical background, choose if to account for it at 100%
                        or to rescale its contribution based on the transmitted intensity
        - CorrExptime : bool. If False, disable exposure time correction even if a correcting factor (self.ExpTimeCorrFactor) is specified
        
        Returns
        -------
        - ROIavgs_allExp  : 3D array. Element [i,j,k] is the average of j-th exposure time in i-th exposure time sweep, averaged on k-th ROI
        - ROIavgs_best    : 2D array. Element [i,j] is the best average intensity taken from i-th exposure time sweep, averaged on j-th ROI
                            NOTE: values in ROIavgs_best and ROIavgs_allExp are normalized by the exposure time, and eventually by the photodiode count.
        - BestExptime_Idx : 2D array (int). Element [i,j] is the index of the optimum exposure time for j-th ROI
        - buf_images      : 3D array. Buffer of images eventually read during the analysis
        """
        
        if saveFolder is not None:
            sf.CheckCreateFolder(saveFolder)
            self.LastSaveFolder = saveFolder
            
            if export_config:
                analysis_params = {'General' : {'generated_by' : 'ROIproc.doSLS'},
                                   'Analysis': {
                                                'type' : 'SLS',
                                                'out_folder' : os.path.abspath(saveFolder),
                                                'no_buffer'  : no_buffer,
                                                'norm_PDdata': norm_PDdata,
                                                'force_calc' : force_calc,
                                                'CorrExptime': CorrExptime,
                                                'corr_bkg'   : corr_bkg,
                                                'opt_resc_trans':opt_resc_trans
                                                },
                                    }
                analysis_params['General']['generated_by'] = 'ROIproc.doDLS'
                config_fname = 'ROIprocConfig.ini'
                if export_configparams is not None:
                    analysis_params = sf.UpdateDict(analysis_params, export_configparams)
                    if 'SALS' in analysis_params['General']['generated_by']:
                        config_fname = 'SALSconfig.ini'
                self.ExportConfiguration(saveFolder, other_params=analysis_params, out_fname=config_fname)
        
        ROIavgs_allExp, ROIavgs_best, BestExptime_Idx = None, None, None
        if not force_calc:
             ROIavgs_allExp, ROIavgs_best, BestExptime_Idx = self.LoadIavg(saveFolder)
        
        if ROIavgs_allExp is None or ROIavgs_best is None or BestExptime_Idx is None:

            if buf_images is None:
                if no_buffer or self.StackInput()==True:
                    self.MIinput.OpenForReading()
                    buf_images = None
                else:
                    buf_images = self.ReadMI()

            # Compute average intensity for all images
            all_avg = self.ROIaverageIntensity(stack1=list(range(self.ImageNumber())), no_buffer=no_buffer, imgs=buf_images)
            if all_avg.shape[0] % (self.NumTimes() * self.NumExpTimes()) != 0:
                limit_len = self.NumTimes() * self.NumExpTimes()
                all_avg = all_avg[:limit_len]
                logging.warning('Number of images ({0}) is not a multiple of exposure times ({1}). '.format(self.ImageNumber(), self.NumExpTimes()) + 
                                'Average intensity output of shape {0} cannot be reshaped using number of times ({1}) '.format(all_avg.shape, self.NumTimes()) +
                                'and exposure times ({1}). Restricting SLS analysis to first {0} images'.format(self.NumExpTimes(), limit_len))
            ROIavgs_allExp = all_avg.reshape((self.NumTimes(), self.NumExpTimes(), -1))
            ROIavgs_best, BestExptime_Idx = self.FindBestExptimes(ROIavgs_allExp, normExptime=False)
            
            
            # Correct for dark and optical background, normalize by exposure time and photodiode readings
            PDmon = np.ones(ROIavgs_best.shape[0], dtype=float)
            Inorm = np.ones_like(ROIavgs_best, dtype=float)
            Ibase = np.zeros_like(ROIavgs_best, dtype=float)
            if norm_PDdata and self.HasPDdata():
                for tidx in range(ROIavgs_best.shape[0]):
                    cur_mon = self.GetPDdata(chIdx=0, tidx=tidx, tgroup_len=self.NumExpTimes())
                    if cur_mon is None:
                        logging.warning('No monitor PD data available for {0}-th sweep of {1} exposure times each ({2} PD datapoints total)'.format(tidx, self.NumExpTimes(), len(self.PDdata[0])))
                    else:
                        if corr_bkg and self.HasDarkBkgPDdata():
                            cur_mon -= self.GetDarkBkgPDdata(0, calc_avg=True)
                        PDmon[tidx] = cur_mon
                        for ridx in range(ROIavgs_best.shape[1]):
                            Inorm[tidx, ridx] = cur_mon
                            
            if corr_bkg:
                for tidx in range(ROIavgs_best.shape[0]):
                    for ridx in range(ROIavgs_best.shape[1]):
                        cur_dark = self.GetDarkBkgIbest(exp_idx=BestExptime_Idx[tidx,ridx])
                        if cur_dark is not None:
                            Ibase[tidx,ridx] += cur_dark[ridx]
                        cur_opt = self.GetOptBkgIbest(exp_idx=BestExptime_Idx[tidx,ridx])
                        if cur_opt is not None:
                            if cur_dark is not None:
                                cur_opt -= cur_dark
                            cur_opt_normf = 1
                            if norm_PDdata:
                                opt_mon = self.GetOptBkgPDdata(0, calc_avg=True)
                                if opt_mon is not None:
                                    if self.HasDarkBkgPDdata():
                                        opt_mon -= self.GetDarkBkgPDdata(0, calc_avg=True)
                                    if opt_resc_trans:
                                        cur_trans = self.GetPDdata(chIdx=1, tidx=tidx, tgroup_len=self.NumExpTimes())
                                        if cur_trans is None:
                                            logging.warning('No transmitted PD data available for {0}-th sweep of {1} exposure times each ({2} PD datapoints total)'.format(tidx, self.NumExpTimes(), len(self.PDdata[1])))
                                        elif self.HasDarkBkgPDdata():
                                            cur_opt_normf = (cur_trans - self.GetDarkBkgPDdata(1, calc_avg=True)) / (self.GetOptBkgPDdata(1, calc_avg=True) - self.GetDarkBkgPDdata(1, calc_avg=True))
                                        else:
                                            cur_opt_normf = cur_trans / self.GetOptBkgPDdata(1, calc_avg=True)
                                    cur_opt_normf *= PDmon[tidx] / opt_mon
                                    if verbose > 1:
                                        logging.debug('{0}-th time: sample ({1}) and Opt ({2}) PD data used to compute weight of optical background: {3}'.format(tidx, 
                                                        [self.GetPDdata(chIdx=0, tidx=tidx, tgroup_len=self.NumExpTimes()), cur_trans],
                                                        [self.GetOptBkgPDdata(0, calc_avg=True), self.GetOptBkgPDdata(1, calc_avg=True)], cur_opt_normf))
                            if verbose > 1:
                                logging.debug('Now updating baseline ({0}th time, {1}th roi) with optical background correction ({2}) multiplied by correction factor {3}'.format(tidx,ridx,cur_opt[ridx],cur_opt_normf))
                            Ibase[tidx,ridx] += cur_opt[ridx] * cur_opt_normf
                            
            for tidx in range(ROIavgs_best.shape[0]):
                for ridx in range(ROIavgs_best.shape[1]):
                    if verbose > 1:
                        logging.debug('Now updating normalization ({0}th time, {1}th roi) with {2}-th exposure time (best). Current value {3} will be multiplied by {4}'.format(tidx,ridx,BestExptime_Idx[tidx,ridx],Inorm[tidx,ridx],self.GetExptimes(exp_idx=BestExptime_Idx[tidx,ridx], corr=CorrExptime)))
                    Inorm[tidx,ridx] *= self.GetExptimes(exp_idx=BestExptime_Idx[tidx,ridx], corr=CorrExptime)
            ROIavgs_best = (ROIavgs_best - Ibase) / Inorm

            if saveFolder is not None:
                self.SaveIavg(saveFolder, ROIavgs_best, [ROIavgs_allExp, BestExptime_Idx], [Inorm, Ibase], CorrExptime=CorrExptime)
            
            logging.debug('ROIproc.doSLS: output saved')

            # TODO: time average SLS
        
        strlog = 'ROIproc.doSLS analysis returned: raw data (shape: {0})'.format(ROIavgs_allExp.shape) 
        strlog += ', Iavg data (shape: {0}), exptime data (shape: {1})'.format(ROIavgs_best.shape, BestExptime_Idx.shape)
        if buf_images is None:
            strlog += ', no buffer images'
        else:
            strlog += ', buffer images (shape {0})'.format(buf_images.shape)
        logging.debug(strlog)
        
        return ROIavgs_allExp, ROIavgs_best, BestExptime_Idx, buf_images

    def doDLS(self, saveFolder, lagtimes, reftimes='all', no_buffer=False, drift_corr=0, drift_method='paraboloid', 
              force_SLS=True, save_transposed=False, export_configparams=None, include_negative_lags=False,
              skip_avg_g2m1=False, g2m1_averageN=None, g2m1_reterr=False):
        """ Run SLS/DLS analysis

        Parameters
        ----------
        lagtimes :              'all' or list of int. 
                                - If 'all', all available lagtimes will be processed
                                - Otherwise, only specified lagtimes will be processed
        reftimes :              'all' or list of int. 
                                - If 'all', all reference times will be used
                                - Otherwise, specialize the analysis to a subset of reference times
        drift_corr:             int. if > 0, track peak of spatial crosscorrelations to find and correct for speckle drift
                                up to a maximum drift of drift_corr pixels. If 0, do not perform this extra step
                                NOTE: this analysis uses rectangular ROIs without masks.
                                ROI coordinates are set by ROIproc.ROIboundingBoxes
        drift_method :          str. Method for subpixel drift detection and correction. 
                                Available methods: 'paraboloid' or 'overlap'
                                'paraboloid' method is faster and suitable for large speckles
                                'overlap' method is best for small speckles
        no_buffer :             bool. If True, avoid reading full MIfile to RAM
        force_SLS :             bool. If False, program will load previously computed SLS results if available.
        save_transposed:        bool. Format of correlation timetrace output
                                - if False, classic cI output: one line per reference time, one column per time delay
                                - if True, transposed output: one line per time delay, one column per reference time
                                  NOTE: transposed output is incompatible with drift correction
        include_negative_lags:  Used if lagtimes=='all'. if False (default), only process prositive lagtimes. 
                                If reftimes=='all', negative lagtimes are redundant and include_negative_lags will be set to False.
                                If sparse reftimes and all lagtimes are processed, set include_negative_lags==True to include negative lagtimes
        export_configparams:    None or dict with additional configuration parameters to be exported to the output configuration file
        skip_avg_g2m1 :         set to True to skip averaging g2-1 curves at the end
        g2m1_averageN :         int or None. If None, average correlation timetraces on the whole time window to obtain one g2-1 curve per ROI
                                if N>0, average correlation timetraces on windows of N datapoints, 
                                to obtain (self.NumTimes() / N) g2-1 curves per ROI
        g2m1_reterr :           bool. True to return standard deviation of correlation curves averaged to produce g2-1
        """
        
        sf.CheckCreateFolder(saveFolder)
        self.LastSaveFolder = saveFolder
        fout = open(os.path.join(saveFolder, 'analysis_log.txt'), 'w')
            
        if reftimes=='all':
            DLS_reftimes = np.arange(self.NumTimes())
            if include_negative_lags:
                if self.log_verbose > 0:
                    sf.LogWrite('ROIproc.doDLS(): No need of processing negative timelags with reftimes==all: include_negative_lags changed to False', 
                                fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
                include_negative_lags = False
        else:
            DLS_reftimes = np.asarray(reftimes)

        if type(lagtimes) in [np.ndarray, np.array]:
            lagtimes = list(lagtimes)
        if lagtimes=='all':
            if include_negative_lags:
                DLS_lags = np.arange(-self.NumTimes()+1, self.NumTimes())
            else:
                DLS_lags = np.arange(self.NumTimes())
        else:
            DLS_lags = np.asarray(lagtimes)
            if np.min(DLS_lags) < 0 and not include_negative_lags:
                DLS_lags = DLS_lags[DLS_lags>=0]
                if self.log_verbose > 0:
                    sf.LogWrite('ROIproc.doDLS(): Negative lagtimes specified with include_negative_lags==False. Negative lag times will be removed from the list', 
                                fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
        if DLS_lags[0] != 0:
            if self.log_verbose > 0:
                sf.LogWrite('ROIproc.doDLS(): 0 lagtime prepended to DLS_lags', 
                            fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
            DLS_lags = np.insert(DLS_lags, 0, 0)
        DLS_lagnum = len(DLS_lags)
        if self.DebugMode or self.log_verbose > 2:
            sf.LogWrite('Complete list of lagtimes: ' + str(DLS_lags), 
                        fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
            
        if drift_corr>0:
            if self.ROI_masks_crop is not None and self.log_verbose > 0:
                sf.LogWrite('ROIproc.doDLS(): using drift correction with non-rectangular ROIs or with a pixel mask excluding some pixels. ' +\
                            'WARNING: drift correction will take into account all pixels in ROI bounding boxes, disregarding masks', 
                            fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
            ValidROI = np.ones((len(self.ROIboundingBoxes)), dtype=bool)
            search_range = ValidateShiftRange(drift_corr, self.GetCroppedShape())
            search_ROIs = []
            count_ROImodif = 0
            for ridx in range(len(self.ROIboundingBoxes)):
                cur_valid_ROI = ValidateShiftROI(self.ROIboundingBoxes[ridx], search_range, self.GetCroppedShape(), debugMode=self.DebugMode)
                if cur_valid_ROI is None:
                    ValidROI[ridx] = False
                    sf.LogWrite('ROIproc.doDLS() error: ROI {0} (bounding box: {1}) incompatible with search range {2} '.format(ridx, self.ROIboundingBoxes[ridx], search_range) +\
                                'in image of cropped shape {0} (Original shape: {1}, bounding box margin: {2}). Did you forget to call ROIproc.UpdateBBmargin()?'.format(self.GetCroppedShape(), self.MIinput.ImageShape(), self.BoundingBoxMargin), 
                                fLog=fout, logLevel=logging.ERROR, add_prefix='\n'+sf.TimeStr()+' | ERROR: ')
                    search_ROIs.append(self.ROIboundingBoxes[ridx])
                else:
                    if not np.array_equal(cur_valid_ROI, self.ROIboundingBoxes[ridx]):
                        count_ROImodif += 1
                        if self.log_verbose > 0:
                            sf.LogWrite('ROIproc.doDLS() error: ROI {0} (bounding box: {1}) has to be shrunk to {2} to accommodate search range {3} '.format(ridx, self.ROIboundingBoxes[ridx], cur_valid_ROI, search_range) +\
                                        'in image of cropped shape {0} (Original shape: {1}, bounding box margin: {2})'.format(self.GetCroppedShape(), self.MIinput.ImageShape(), self.BoundingBoxMargin), 
                                        fLog=fout, logLevel=logging.WARNING, add_prefix='\n'+sf.TimeStr()+' | WARNING: ')
                    search_ROIs.append(cur_valid_ROI)
            if self.log_verbose > 1:
                sf.LogWrite('ROIproc.doDLS() configured spatial crosscorrelation analysis on {0} ROIs ({1} valid, {2} modified)'.format(len(self.ROIboundingBoxes), np.sum(ValidROI), count_ROImodif), 
                            fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        

        analysis_params = {'General' : {'generated_by' : 'ROIproc.doDLS'},
                           'Analysis': {
                                        'type' : 'DLS',
                                        'out_folder' : os.path.abspath(saveFolder),
                                        'lagtimes' : lagtimes,
                                        'reftimes' : reftimes,
                                        'no_buffer' : no_buffer,
                                        'force_SLS' : force_SLS,
                                        'save_transposed' : save_transposed,
                                        'include_negative_lags' : include_negative_lags,
                                        'drift_corr' : drift_corr,
                                        'drift_method' : drift_method,
                                        'g2m1_averageN' : g2m1_averageN,
                                        'g2m1_reterr' : g2m1_reterr,
                            },}
        if drift_corr>0:
            analysis_params['Analysis']['drift_search_range'] = search_range
        config_fname = 'ROIprocConfig.ini'
        if export_configparams is not None:
            analysis_params = sf.UpdateDict(analysis_params, export_configparams)
            if 'SALS' in analysis_params['General']['generated_by']:
                config_fname = 'SALSconfig.ini'
        self.ExportConfiguration(saveFolder, other_params=analysis_params, out_fname=config_fname)
        
        if self.log_verbose > 1:
            sf.LogWrite('ROIproc.doDLS Analysis started! Input data is {0} images ({1} times, {2} exposure times)'.format(self.ImageNumber(), self.NumTimes(), self.NumExpTimes()), 
                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        if (self.ImageNumber() != (self.NumTimes() * self.NumExpTimes())) and self.log_verbose > 0:
            sf.LogWrite('WARNING: Number of images ({0}) should correspond to the number of times ({1}) times the number of exposure times ({2})'.format(self.ImageNumber(), 
                            self.NumTimes(), self.NumExpTimes()), fLog=fout, logLevel=logging.WARN, add_prefix='\n'+sf.TimeStr()+' | WARN: ')
        if self.log_verbose > 1:
            sf.LogWrite('Analysis will resolve {0} ROIs and DLS will be performed on {1} reference times and {2} lagtimes. Output will be saved in folder {3}'.format(self.CountROIs(), 
                            len(DLS_reftimes), DLS_lagnum, saveFolder), fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
            sf.LogWrite('Now starting with SLS...', fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
                
        if len(DLS_reftimes)<1:
            sf.LogWrite('ERROR: at least 1 reference time needed for DLS (' + str(len(DLS_reftimes)) + ' given). DLS aborted', 
                        fLog=fout, logLevel=logging.ERROR, add_prefix='\n'+sf.TimeStr()+' | ERROR: ')
            fout.close()
            return None
        
            
        ROIavgs_allExp, ROIavgs_best, BestExptime_Idx, buf_images = self.doSLS(saveFolder, buf_images=None, no_buffer=no_buffer, force_calc=force_SLS, export_config=False)
        
        if buf_images is None:
            if no_buffer:
                self.MIinput.OpenForReading()
                buf_images = None
            elif self.StackInput()==False:
                buf_images = self.ReadMI()

        if self.log_verbose > 1:
            sf.LogWrite('SLS analysis completed. Now doing DLS ({0} exposure times, {1} time points, {2} lagtimes)'.format(self.NumExpTimes(), len(DLS_reftimes), DLS_lagnum), 
                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        
        log_period = 1
        for e in range(self.NumExpTimes()):
            readrange = self.MIinput.Validate_zRange([e, -1, self.NumExpTimes()])
            idx_list = np.arange(*readrange, dtype=int)
            if self.log_verbose > 1:
                sf.LogWrite('Now performing DLS on {0}-th exposure time. Using image range {1} ({2} images)'.format(e, readrange, len(idx_list)), 
                            fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
            ISQavg = self.ROIaverageProduct(stack1=idx_list, stack2=idx_list, no_buffer=no_buffer, imgs=buf_images)

            if reftimes=='all' and drift_corr==0:
                cI = np.nan * np.ones((ISQavg.shape[1], ISQavg.shape[0], DLS_lagnum), dtype=float)
                cI[:,:,0] = np.subtract(np.divide(ISQavg, np.square(ROIavgs_allExp[:,e,:])), 1).T
                if self.log_verbose > 1:
                    sf.LogWrite('Contrast (d0) processed', fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
                if (self.LoopMaxInfoN < DLS_lagnum):
                    log_period = int(math.ceil(DLS_lagnum*1.0/self.LoopMaxInfoN))
                    if self.log_verbose > 2:
                        sf.LogWrite('Number of lagtimes ({0}) exceeding {1}: logging every {2}-th lagtime processed'.format(DLS_lagnum, self.LoopMaxInfoN, log_period), 
                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                for lidx in range(1, DLS_lagnum):
                    if (DLS_lags[lidx]<ISQavg.shape[0]):

                        IXavg = self.ROIaverageProduct(stack1=idx_list[:-DLS_lags[lidx]], stack2=idx_list[DLS_lags[lidx]:], 
                                                                 no_buffer=no_buffer, imgs=buf_images)
                        # 'classic' cI formula
                        cI[:,:-DLS_lags[lidx],lidx] = np.subtract(np.divide(IXavg, np.multiply(ROIavgs_allExp[:-DLS_lags[lidx],e,:],
                                                                                                   ROIavgs_allExp[DLS_lags[lidx]:,e,:])), 1).T
                        # d0 normalization
                        cI[:,:-DLS_lags[lidx],lidx] = np.divide(cI[:,:-DLS_lags[lidx],lidx], 0.5 * np.add(cI[:,:-DLS_lags[lidx],0], cI[:,DLS_lags[lidx]:,0]))
                        
                        if (lidx % log_period == 0 or lidx == DLS_lagnum-1) and self.log_verbose > 1:
                            sf.LogWrite('Lagtime {0}/{1} (d{2}) completed'.format(lidx, DLS_lagnum-1, DLS_lags[lidx]), 
                                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')


            else:
                # Shape of output cIs: [num_ROIs, num_reftimes, num_lagtimes]
                cI = np.nan * np.ones((ISQavg.shape[1], len(DLS_reftimes), DLS_lagnum), dtype=float)
                if drift_corr>0:
                    cIcr = np.nan * np.ones_like(cI, dtype=float)
                    dxdy = np.nan * np.ones((cI.shape[0], cI.shape[1], cI.shape[2], 2), dtype=float)
                if self.log_verbose > 1:
                    sf.LogWrite('Computing cI with custom-defined set of reference time and/or lag times: result has shape {0} ({1} ROIs, {2} reference times, {3} lag times)'.format(cI.shape, 
                                    self.CountROIs(), len(DLS_reftimes), DLS_lagnum), fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')

                if (self.LoopMaxInfoN < len(DLS_reftimes)):
                    log_period = int(math.ceil(len(DLS_reftimes)*1.0/self.LoopMaxInfoN))
                    if self.log_verbose > 2:
                        sf.LogWrite('Number of reference times ({0}) exceeding {1}: logging every {2}-th reference time processed'.format(len(DLS_reftimes), self.LoopMaxInfoN, log_period), 
                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')

                # compute all d0s (even if it is not in the list of lagtimes)
                all_d0 = np.subtract(np.divide(ISQavg, np.square(ROIavgs_allExp[:,e,:])), 1).T
                if self.DebugMode or self.log_verbose > 2:
                    sf.LogWrite('SALS.doDLS - cI.shape : ' + str(cI.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                    sf.LogWrite('SALS.doDLS - ISQavg.shape : ' + str(ISQavg.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                    sf.LogWrite('SALS.doDLS - ROIavgs_allExp.shape : ' + str(ROIavgs_allExp.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')

                for ref_tidx in range(len(DLS_reftimes)):

                    cur_stack2 = []
                    cur_lagtimes = []
                    cur_lagidx = []
                    for lidx in range(len(DLS_lags)):
                        if DLS_lags[lidx]+DLS_reftimes[ref_tidx] >= 0 and DLS_lags[lidx]+DLS_reftimes[ref_tidx] < len(idx_list):
                            cur_stack2.append(DLS_lags[lidx]+DLS_reftimes[ref_tidx])
                            cur_lagtimes.append(DLS_lags[lidx])
                            cur_lagidx.append(lidx)
                    if self.DebugMode or self.log_verbose > 2:
                        strLog = 'Images to correlate with reference time #{0} (t={1}): '.format(ref_tidx, DLS_reftimes[ref_tidx]) +\
                                str([str(cur_stack2[i]) + ' (d' + str(cur_lagtimes[i]) + ')' for i in range(min(10, len(cur_lagtimes)))])
                        if len(cur_lagtimes) > 10:
                            strLog += ' (' + str(len(cur_lagtimes)-10) + ' more)'
                        sf.LogWrite(strLog, fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                            
                    # 
                    if drift_corr>0:
                        ref_img = self.GetImage(DLS_reftimes[ref_tidx], buffer=buf_images)
                        if self.DebugMode or self.log_verbose > 2:
                            sf.LogWrite('Reference image shape for drift correction: ' + str(ref_img.shape), 
                                            fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')

                    if len(cur_stack2)>0:

                        IXavg = self.ROIaverageProduct(stack1=[idx_list[DLS_reftimes[ref_tidx]]]*len(cur_stack2), stack2=cur_stack2, 
                                                                 no_buffer=no_buffer, imgs=buf_images)

                        if self.DebugMode or self.log_verbose > 2:
                            sf.LogWrite('IXavg.shape [num_images={0}, num_ROIs={1}] = {2}'.format(len(cur_stack2), cI.shape[0], IXavg.shape), fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                        for lidx in range(len(cur_lagtimes)):

                            cur_tidx2 = DLS_reftimes[ref_tidx]+cur_lagtimes[lidx]
                            if cur_tidx2>=0 and cur_tidx2 < ISQavg.shape[0]:

                                if self.DebugMode or self.log_verbose > 2:
                                    sf.LogWrite('Correlating reftime {0} (t={1}) and lagtime {2} (d={3}, t={4})'.format(ref_tidx, DLS_reftimes[ref_tidx], 
                                                                                                    cur_lagidx[lidx], cur_lagtimes[lidx], cur_stack2[lidx]), 
                                                fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                if cur_lagtimes[lidx]==0:
                                    cI[:,ref_tidx,cur_lagidx[lidx]] = all_d0[:,ref_tidx]
                                else:
                                    # 'classic' cI formula
                                    if self.DebugMode or self.log_verbose > 2:
                                        sf.LogWrite('test: IXavg[0,0] = <I(t={0},ROI0)I(t={1},ROI0)> = {2}'.format(idx_list[DLS_reftimes[ref_tidx]], cur_stack2[0], IXavg[0,0]), 
                                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                        sf.LogWrite('ROIavgs_allExp test: ' + str(ROIavgs_allExp[DLS_reftimes[ref_tidx],e,0]), 
                                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                    cI[:,ref_tidx,cur_lagidx[lidx]] = np.subtract(np.divide(IXavg[lidx,:], 
                                                                                np.multiply(ROIavgs_allExp[DLS_reftimes[ref_tidx],e,:],
                                                                                            ROIavgs_allExp[cur_tidx2,e,:])), 1)
                                # d0 normalization
                                if cur_lagidx[lidx]>0:
                                    cI[:,ref_tidx,cur_lagidx[lidx]] = np.divide(cI[:,ref_tidx,cur_lagidx[lidx]], 0.5 * np.add(all_d0[:,DLS_reftimes[ref_tidx]], all_d0[:,cur_tidx2]))
                                
                                if drift_corr>0:
                                    find_img = self.GetImage(cur_stack2[lidx], buffer=buf_images)
                                    if self.DebugMode or self.log_verbose > 2:
                                        sf.LogWrite('Image shape for drift correction: ' + str(find_img.shape), 
                                                    fLog=fout, logLevel=logging.DEBUG, add_prefix='\n'+sf.TimeStr()+' | DEBUG: ')
                                    for ridx in range(cI.shape[0]):
                                        if ValidROI[ridx]:
                                            xp, yp, corr_peak = FindCrosscorrPeak(find_img, ref_img, SearchRange=search_range, SearchROI=search_ROIs[ridx], 
                                                                                  ValidateInput=False, SubpixelMethod=drift_method, debugMode=self.DebugMode)
                                            cIcr[ridx,ref_tidx,cur_lagidx[lidx]] = corr_peak * 2. / (all_d0[ridx, DLS_reftimes[ref_tidx]] + all_d0[ridx, cur_tidx2])
                                            dxdy[ridx,ref_tidx,cur_lagidx[lidx]] = (xp, yp)
                                        
                        if (ref_tidx % log_period == 0 or ref_tidx == len(DLS_reftimes)-1) and self.log_verbose > 1:
                            sf.LogWrite('Reference time {0}/{1} (tref={2}) completed'.format(ref_tidx+1, len(DLS_reftimes), DLS_reftimes[ref_tidx]), 
                                        fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')

                    else:

                        if self.log_verbose > 0:
                            sf.LogWrite('Reference time {0}/{1} (tref={2}) empty'.format(ref_tidx+1, len(DLS_reftimes), DLS_reftimes[ref_tidx]), 
                                        fLog=fout, logLevel=logging.WARN, add_prefix='\n'+sf.TimeStr()+' | WARN: ')



            # Save data to file
            for ridx in range(cI.shape[0]):
                if self.log_verbose > 1:
                    sf.LogWrite('Now saving ROI {0} to file'.format(ridx), fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
                if save_transposed:
                    np.savetxt(os.path.join(saveFolder, 'cI_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), cI[ridx,:,1:].T, axis=1), 
                               header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)    
                    if drift_corr>0 and ValidROI[ridx]:
                        np.savetxt(os.path.join(saveFolder, 'cIcr_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), cIcr[ridx,:,1:].T, axis=1), 
                                   header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}_cr'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dx_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), dxdy[ridx,:,1:,0].T, axis=1), 
                                   header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}_dx'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dy_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(DLS_lags[1:].reshape((-1, 1)), dxdy[ridx,:,1:,1].T, axis=1), 
                                   header='tau'+self.txt_delim + str(self.txt_delim).join(['t{0}_dy'.format(l) for l in DLS_reftimes]), **self.savetxt_kwargs)                  
                else:
                    first_cols = np.append(idx_list[DLS_reftimes].reshape((-1, 1)), self.imgTimes[idx_list[DLS_reftimes]].reshape((-1, 1)), axis=1)
                    np.savetxt(os.path.join(saveFolder, 'cI_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, cI[ridx], axis=1), 
                               header='idx'+self.txt_delim+'t'+self.txt_delim+'d0_raw'+self.txt_delim + str(self.txt_delim).join(['d{0}'.format(l) for l in DLS_lags[1:]]), **self.savetxt_kwargs)
                    if drift_corr>0 and ValidROI[ridx]:
                        np.savetxt(os.path.join(saveFolder, 'cIcr_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, cIcr[ridx], axis=1), 
                                   header='idx'+self.txt_delim+'t'+self.txt_delim + str(self.txt_delim).join(['d{0}_cr'.format(l) for l in DLS_lags]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dx_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, dxdy[ridx,:,:,0], axis=1), 
                                   header='idx'+self.txt_delim+'t'+self.txt_delim + str(self.txt_delim).join(['d{0}_dx'.format(l) for l in DLS_lags]), **self.savetxt_kwargs)                  
                        np.savetxt(os.path.join(saveFolder, 'dy_ROI' + str(ridx).zfill(3) + '_e' + str(e).zfill(2) + '.dat'), np.append(first_cols, dxdy[ridx,:,:,1], axis=1), 
                                   header='idx'+self.txt_delim+'t'+self.txt_delim + str(self.txt_delim).join(['d{0}_dy'.format(l) for l in DLS_lags]), **self.savetxt_kwargs)                  
                    
        if save_transposed:
            if self.log_verbose > 1:
                sf.LogWrite('DLS analysis completed. Transposed result saved (no g2-1 function will be averaged).', 
                            fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
        else:
            if self.log_verbose > 1:
                sf.LogWrite('DLS analysis completed. Now averaging correlation functions g2-1', 
                            fLog=fout, logLevel=logging.INFO, add_prefix='\n'+sf.TimeStr()+' | INFO: ')
            if not skip_avg_g2m1:
                self.AverageG2M1(saveFolder, avg_interval=g2m1_averageN, save_stderr=g2m1_reterr)
        fout.close()
            
    def AverageG2M1(self, folder_path, avg_interval=None, search_prefix=['cI_','cIcr_','dx_','dy_'], save_prefix=['g2m1','g2m1cr','avgdx','avgdy'], save_stderr=False, sharp_bound=False, lag_tolerance=None, lag_tolerance_isrelative=None):
        """
        Loads cI-like files from folder and average correlations with equal time delays to obtain g2-1 curves
        """

        if lag_tolerance is None:
            lag_tolerance = self.dt_tolerance
        if lag_tolerance_isrelative is None:
            lag_tolerance_isrelative = self.dt_tolerance_isrelative

        
        for i in range(len(search_prefix)):
            if i>len(save_prefix):
                raise ValueError('The size of save_prefix (' + str(len(save_prefix)) + ') must match that of search_prefix (' + str(len(search_prefix)) + ')')
            tres_fnames = sf.FindFileNames(folder_path, Prefix=search_prefix[i], Ext='.dat')
            imtimes_fname = os.path.join(folder_path, self.out_names['imgTimes'])
            if os.path.isfile(imtimes_fname):
                img_times = np.loadtxt(imtimes_fname)
            else:
                img_times = None
            for cur_f in tres_fnames:
                AverageG2M1(os.path.join(folder_path, cur_f), avg_interval=avg_interval, imgTimes=img_times, save_prefix=save_prefix[i], 
                            cut_prefix_len=len(search_prefix[i])-1, delimiter=self.txt_delim, comment=self.txt_comm, save_stderr=save_stderr, sharp_bound=sharp_bound, lag_tolerance=lag_tolerance, lag_tolerance_isrelative=lag_tolerance_isrelative)
            
    def ExportROIs(self, outFolder):
        """
        Exports ROIs:
        - self.out_names['ROIcoords']: text file with ROI coordinates and areas
        - self.out_names['ROI_mask']: integer mask with indices of the ROI every pixel belongs to. NOTE: no ROI overlap supported
        """
        sf.CheckCreateFolder(outFolder)
        ROIhdr_str = self.txt_delim.join(self.ROIcoord_names+[name+'_err' for name in self.ROIcoord_names]+['norm', 'min_row[bb_base:' + str(self.BoundingBox[0]) + ']', 'min_col[bb_base:' + str(self.BoundingBox[1]) + ']', 'max_row', 'max_col'])
        roi_norms = np.expand_dims(self.ROI_maskSizes, axis=1)
        np.savetxt(os.path.join(outFolder, self.out_names['ROIcoords']), np.append(np.append(self.ROIcoords, roi_norms, axis=1), 
                               self.ROIboundingBoxes, axis=1), header=ROIhdr_str, **self.savetxt_kwargs)
        MI.WriteBinary(os.path.join(outFolder, self.out_names['ROI_mask']), self.ROIs, 'i')
                
    def ExportConfiguration(self, outFolder, other_params=None, out_fname=None):

        if out_fname is None:
            out_fname = 'ROIprocConfig.ini'
        sf.CheckCreateFolder(outFolder)
        logging.info('ROIproc configuration exported to folder {0}'.format(outFolder))
        self.ExportROIs(outFolder)
        ini_fpath = os.path.join(outFolder, out_fname)
        np.savetxt(os.path.join(outFolder, self.out_names['imgTimes']), self.imgTimes)
        
        my_params = {'General': {'version' : '2.0',
                                 'generated_by' : 'ROIproc.ExportConfiguration',
                                 'generated_on' : sf.NowToStr(),
                                 'folder' : os.path.abspath(outFolder),
                                },
                       'MIfile' : self.MIinput.GetMetadata(),
                       'ROIs' : {'number' : self.CountROIs(),
                                 'box' : list(self.BoundingBox),
                                 'box_margin' : self.BoundingBoxMargin,
                                 'coord_file' : self.out_names['ROIcoords'],
                                 'mask_file' : self.out_names['ROI_mask'],
                                },
                       'ImgTimes' : {'number' : len(self.imgTimes),
                                   'file' : self.out_names['imgTimes'],
                                   'usecol' : 0,
                                   'skiprow' : 0,
                                },
                       'ExpTimes' : {'values' : self.expTimes,
                                     'corr_factor' : self.ExpTimeCorrFactor,
                                     'corr_values' : self.GetExptimes(),
                                },
                       'Constants' : {'max_avg_intensity' : self.MaxSafeAvgIntensity,
                                      'dt_tolerance' : self.dt_tolerance,
                                      'dt_tolerance_isrelative' : self.dt_tolerance_isrelative,
                                      'txt_delim' : self.txt_delim,
                                      'txt_comm' : self.txt_comm,
                                       },
                       }
        if self.HasPDdata():
            np.savetxt(os.path.join(outFolder, self.out_names['PDdata']), sf.StackArrayList(self.PDdata), **self.savetxt_kwargs)
            my_params['PDdata'] = {'num_channels' : self.CountPDchannels(),
                                   'num_datapoints' : self.CountPDdata(),
                                   'file' : self.out_names['PDdata'],
                                   'usecol' : 0,
                                   'skiprow' : 0}
        for bkg_key in ['Dark', 'Opt']:
            if self.HasBkg(bkg_key):
                my_params[bkg_key+'Bkg'] = {}
                if self.HasBkgIavg(bkg_key):
                    my_params[bkg_key+'Bkg']['Iavg'] = self.GetBkgIavg(bkg_key, texp=1, use_norm=False)
                if 'Iavg_norm' in self.BkgCorr[bkg_key]:
                    my_params[bkg_key+'Bkg']['Iavg_norm'] = self.BkgCorr[bkg_key]['Iavg_norm']
                if self.HasBkgPDdata(bkg_key):
                    my_params[bkg_key+'Bkg']['PDdata'] = self.GetBkgPDdata(bkg_key, calc_avg=True)
                if self.HasBkgIraw(bkg_key):
                    if 'exptimes' in self.BkgCorr[bkg_key]:
                        bkg_exptimes = self.BkgCorr[bkg_key]['exptimes']
                    else:
                        bkg_exptimes = self.expTimes
                    str_hdr = self.txt_delim.join(['texp={0:.3f}'.format(texp) for texp in bkg_exptimes])
                    np.savetxt(os.path.join(outFolder, bkg_key+'Bkg_Iraw.dat'), self.GetBkgIraw(bkg_key).T, header=str_hdr, **self.savetxt_kwargs)
                    my_params[bkg_key+'Bkg']['Iavg_raw_file'] = bkg_key+'Bkg_Iraw.dat'
        if other_params is not None:
            my_params = sf.UpdateDict(my_params, other_params)
        if self.MIinput.IsStack():
            my_params['MIfile']['is_stack'] = True
            my_params['MIfile']['stack_type'] = self.MIinput.StackType
        else:
            my_params['MIfile']['is_stack'] = False
        my_params['MIfile']['filename'] = self.MIinput.GetFilename(absPath=True)
        cf.ExportDict(my_params, ini_fpath)
    
    def RunFromConfig(self, ConfigParams, AnalysisSection='Analysis', OutputSubfolder='reproc', export_configparams=None):
        """Runs an analysis using parameters from a configuration file or a Config object
        
        Parameters
        ----------
        ConfigParams     : full path of the config file to read or dict or Config object
        AnalysisSection  : str, label of the section with analysis parameters
                           keys required:
                           - 'type': type of analysis to perform (supported types: {'DLS'})
                           keys for DLS analysis
                           - 'out_folder': output folder
                           - 'lagtimes': ('all' or list of int. Default: 'all')
                           - 'reftimes': ('all' or list of int. Default: 'all')
                           - 'no_buffer' (bool, default: False)
                           - 'force_SLS' (bool, default: True)
                           - 'drift_corr' (int, default: 0)
                           - 'save_transposed' (bool, default: False)
                           - 'include_negative_lags' (bool, default: False)
        OutputSubfolder  : str, subfolder to use as analysis output. 
                           If None, the out_folder parameter will be used as output folder
                           otherwise, data will be saved in a subfolder of the specified output folder
        export_configparams : None or dict with additional configuration parameters to be exported to the output configuration file
                           
        """
        config = cf.LoadConfig(ConfigParams)
        folder_root = config.Get('General', 'folder', None, str)
        
        if config.HasSection(AnalysisSection):

            an_type = config.Get(AnalysisSection, 'type','UNKNOWN', str)
            logging.info('ROIproc.RunFromConfig running {0} analysis'.format(an_type))

            if an_type=='DLS':
                out_folder = sf.GetAbsolutePath(config.Get(AnalysisSection, 'out_folder', '', str), root_path=folder_root)
                if config.Get(AnalysisSection, 'lagtimes', 'all', str)=='all':
                    lagtimes = 'all'
                else:
                    lagtimes = config.Get(AnalysisSection, 'lagtimes', [0], int)
                if config.Get(AnalysisSection, 'reftimes', 'all', str)=='all':
                    reftimes = 'all'
                else:
                    reftimes = config.Get(AnalysisSection, 'reftimes', [0], int)
                no_buffer = config.Get(AnalysisSection, 'no_buffer', False, bool)
                force_SLS = config.Get(AnalysisSection, 'force_SLS', True, bool)
                drift_corr = config.Get(AnalysisSection, 'drift_corr', 0, int)
                g2m1_averageN = config.Get(AnalysisSection, 'g2m1_averageN', 0, int)
                g2m1_reterr = config.Get(AnalysisSection, 'g2m1_reterr', False, bool)
                save_transposed = config.Get(AnalysisSection, 'save_transposed', False, bool)
                include_negative_lags = config.Get(AnalysisSection, 'include_negative_lags', False, bool)
                if OutputSubfolder is None:
                    new_out_folder = out_folder
                else:
                    new_out_folder = os.path.join(out_folder, OutputSubfolder)
                export_configparams = sf.UpdateDict(export_configparams, {'Analysis': {'out_folder' : new_out_folder}})
                if 'General' not in export_configparams:
                    export_configparams['General'] = {'generated_by' : 'ROIproc.RunFromConfig(DLS)'}
                self.doDLS(new_out_folder, lagtimes=lagtimes, reftimes=reftimes, drift_corr=drift_corr, no_buffer=no_buffer, 
                               force_SLS=force_SLS, save_transposed=save_transposed, include_negative_lags=include_negative_lags, 
                               export_configparams=export_configparams, g2m1_averageN=g2m1_averageN, g2m1_reterr=g2m1_reterr)
                logging.info('DLS analysis run and saved to folder {0}'.format(new_out_folder))

            if an_type=='SLS':
                out_folder = sf.GetAbsolutePath(config.Get(AnalysisSection, 'out_folder', '', str), root_path=folder_root)
                no_buffer = config.Get(AnalysisSection, 'no_buffer', False, bool)
                if OutputSubfolder is None:
                    new_out_folder = out_folder
                else:
                    new_out_folder = os.path.join(out_folder, OutputSubfolder)
                export_configparams = sf.UpdateDict(export_configparams, {'Analysis': {'out_folder' : new_out_folder}})
                if 'General' not in export_configparams:
                    export_configparams['General'] = {'generated_by' : 'ROIproc.RunFromConfig(SLS)'}
                self.doSLS(new_out_folder, no_buffer=no_buffer, force_calc=True)
                logging.info('SLS analysis run and saved to folder {0}'.format(new_out_folder))            
            else:
                logging.warn('ROIproc.RunFromConfig ERROR: unknown analysis type {0}'.format(an_type))
        else:
            logging.warn('ROIproc.RunFromConfig ERROR: analysis section {0} not found'.format(AnalysisSection))
